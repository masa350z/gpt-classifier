url,tags,text
https://qiita.com/ymiya14/items/17159773cf3bb54d2179,['画像'],"この記事はNuco Advent Calendar 2023の19日目の記事です。


弊社Nucoでは、他にも様々なお役立ち記事を公開しています。よかったら、Organizationのページも覗いてみてください。
また、Nucoでは一緒に働く仲間も募集しています！興味をお持ちいただける方は、こちらまで。






はじめに
エンジニアであればプロジェクトで画像ファイルに触れる機会は多々あると思いますが、
あなたは画像ファイルの中身について詳しく説明できますか？
本記事では、画像ファイルの形式ごとにデータがどのように格納されているかを詳しく解説します。「ピクセルごとにRGBの値が設定されていて...」くらいの理解度であれば、ぜひこの記事を読んで画像ファイルに関する知見を深めましょう。

ファイル構造の理解
画像ファイルはそれぞれ異なる構造を持っています。
一般的な構造は、ヘッダー、メタデータ、画像データから構成されます。
ヘッダーにはファイルの種類やバージョン情報が含まれ、メタデータは画像に関する追加情報を提供しています。画像データはピクセルの色情報が格納される領域で、形式によってその構造が異なります。
これらについて理解を深めることで、画像処理アルゴリズムやデータの解析において有益な手がかりとなり得るのではないでしょうか。

PNGファイル

PNGファイルの概要
PNGはPortable Network Graphicsの略称です。
PNGファイルは1600万色を表現可能であり、これをを無劣化で圧縮可能であるので、図やイラストなどの保存に向いています。
また、背景の透明・半透明のグラフィック処理が可能なため、Webデザイナーがよく用いるファイル形式でもあります。

PNGファイルの構造
PNGファイルは、シグネチャとチャンクという要素で構成されています。
シグネチャは必ずファイルの先頭にあり、その次にチャンクが来ます。
チャンクは様々な種類があり、必須となるチャンクが IHDR, IDAT, IEND の3つです。
チャンクのかたまりの中で、IHDRチャンクは必ず先頭にあり、IENDチャンクは必ず末尾に来ます。
ファイル全体としては以下のようになっています。

シグネチャ
チャンク(IHDR)
チャンク
...
...
チャンク(IEND)


シグネチャ
シグネチャは必ずファイルの先頭に位置していて、このファイルがPNGファイルであることを示しています。中身は以下のような8byte長のデータであり、2~4byte目をデコードすると
""PNG""となりファイル形式を表していることがわかります。
 address  00 01 02 03 04 05 06 07
 00000000 89 50 4e 47 0d 0a 1a 0a


チャンク
チャンクの基本的な構造は以下のようになっています。



フィールド名
サイズ
内容




Length
4byte
このチャンクの Chank Data の大きさを表す


Chank Type
4byte
このチャンクの種類を表すアルファベット4文字


Chank Data
0byte以上
チャンクの種類に関連したデータ本体


CRC
4byte
データのエラーを検出するための情報




必須チャンク
先述の通りPNGファイルには4つの必須チャンクが存在します。
ChankTypeごとにChankDataに何が格納されているか詳しく見て行きましょう。

IHDR
このチャンクにはこの画像ファイルに関する情報が格納されています。



フィールド名
サイズ
内容




Width
4byte
画像の横幅


Height
4byte
画像の縦幅


Bit Depth
1byte
ビット深度


Color Type
1byte
パレット、カラー、αチャンネルの情報


Compression Method
1byte
画像データの圧縮方式


Filter Method
1byte
圧縮前の画像データに対して行った事前処理


Interlace Method
1byte
画像データのピクセルの出現規則




IDAT
IDATチャンクには画像のメインデータである各ピクセルの色の情報が格納されています。
ピクセル毎に、RGBの値の表現に8bitずつ必要となるので、512×512のPNG形式の画像だとしたら786432 bytesが画像のデータとなります。
PNG形式ではこの画像データは、zlibと呼ばれるライブラリを使用して圧縮されています。

IEND
チャンクのかたまりの最後に出現するチャンクであり、ファイルの終点であることを示しています。IENDチャンクはChankDataを持たず、Lengthは常に0となります。



フィールド名
サイズ
データ




Length
4byte
0


Chank Type
4byte
IEND


CRC
4byte
(省略)




PNGの利点と制限

利点

可逆圧縮: 元の画像データを正確に再現可能。
透過サポート： PNGは透過チャンネルをサポートしてるので、画像内の一部分を透明にすることが可能。


制限

圧縮率： JPEGに比べて圧縮率が劣る場合があり、特に写真などの複雑な画像においてファイルサイズが大きくなることがあります。
色空間の制約： CMYKなどの印刷向けの色空間を直接サポートしていないため、プリント用途では注意が必要です。


使い道
Webデザイン、ロゴやイラストの保存するときに使用されます。

JPEGファイル

JPEGファイルの概要
JPEGは、Joint Photographic Experts Groupの略称です。
静止画像を圧縮して保存するファイル形式として現在標準的に使われている形式です。
可逆圧縮形式もサポートしていますが、可逆圧縮は特許などの関係でほとんど普及していません。1992年9月18日にリリースされた比較的古いフォーマットです。
JPEGの欠点を克服すべく数々の後継規格が提案されてきましたが、いずれも主流になるには至らず、JPEGが現在も静止画像規格の主流となっています。

JPEGファイルの構造
JPEGファイルは、先頭にSOIマーカー、末尾にEOIマーカーというもので挟まれていて、その間の領域に複数のセグメントとイメージデータが存在する、というような構造になっています。


SOIマーカー、EOIマーカー
SOIマーカーはJPEGファイルの先頭にあり、JPEGファイルの起点を表すマーカーです。
多くの画像ビュアーはこのSOIマーカーを読み取りJPEGファイルの判別を行っているため、SOIマーカーが無ければ画像は表示不可。中身は0xFFD8という2バイトのコードです。
EOIマーカーはJPEGファイルの終わりを表すマーカです。
多くのJPEG画像ビュアーでは画像データの途中からデータが破損している場合を考慮し、EOIが無くても画像を表示可能となっています。中身は0xFFD9という2バイトのコードです。

必須セグメント
JPEG形式にはファイルを構成する上で必須となるセグメントが4種存在します。



フィールド名
内容




DQT (Define Quantization Table)
量子化テーブル定義


DHT (Define Huffman Table)
ハフマンテーブルデータ


SOF (Start Of Frame)
JPEGファイルの種類や画像サイズなどの情報


SOS (Start Of Frame)
圧縮方式などの情報




DQT (Define Quantization Table)
このセグメントには、量子化という工程に必要な量子化テーブルの定義が格納されています。量子化とは一般に連続値を離散値で表現することであり、画像ファイルに関して言えば、見た目をそれほど変化させることなく圧縮率を上げる処理のことを指します。
JPEG画像は8×8ピクセルのブロックに分割され、各ブロック内の輝度や色差などは座標軸上で波形として表現されます。この波形は離散コサイン変換によって細部が簡略化されますが、その後のデータの端数は切り捨てられ、さらにデータを圧縮するために量子化テーブルが使用されます。
量子化テーブルでは画質などを調整でき、端数が多く切り捨てられるほど、JPEG変換後の画質が劣化します。JPEGが非可逆圧縮である理由は、この段階で数値の端数が切り捨てられるためです。

DHT (Define Huffman Table)
このセグメントには、ハフマンテーブルというデータが格納されています。
ハフマンテーブルは、データをさらに圧縮するためのハフマン符号化を行うための領域です。
ハフマン符号は、データを出現頻度に基づいて分類し、出現頻度の低いデータを0と1で分岐するバイナリツリーの深層に配置します。そして、バイナリツリーの先頭から符号化を行うことでデータを圧縮するアルゴリズムです。
JPEGでは、このハフマン符号化が量子化とともに2段階の圧縮過程の一環として利用されます。

SOF (Start Of Frame)
SOFはJPEGファイルの種類や画像の横幅、縦幅など最も重要なパラメータを記録しており、
1つのJPEGファイルに必ず1つ存在します。
SOFにはマーカコードの違いにより、ffc0〜ffcfのうちffc4、ffc8、ffccの３つを除く13種類存在し、そのうちffc0とffc2の2種類が一般的に利用されます。

SOS (Start Of Scan)
SOSは、実際に画像として描画されるのに利用されるイメージデータの先頭にあるセグメントで、圧縮方式などの情報が格納されています。
このセグメント以降に存在するデータを、各セグメントで得られた値を用いてデコードすることで、実際に描画で用いるRGBの情報が取得可能となります。

イメージデータ
量子化されたブロックとハフマン符号でエンコードされた画像データが続きます。これが実際の画像のピクセルデータです。JPEGファイルでは画像をデータにする時に、一辺が8ピクセルの倍数の小さな画像に分割し、それぞれの画像をデータに変換します。

JPEGの利点と制限

利点

汎用性: ほとんどのブラウザー、ソフトウェア、アプリケーションで利用可能。
データが小さい: ファイルサイズが小さいため、短時間で転送でき、オンラインで素早くアクセスし表示可能。GIFなどの可逆圧縮方式と比べて、JPEGは桁違いにサイズが小さい。


制限

非可逆圧縮: 圧縮後は元の品質には戻すことができなくる。
画質の低下: 大幅な圧縮をした場合は画質が低下する。エッジやラインがはっきりした画像は圧縮によってシャープさが失われる。
ポスタリゼーション: 圧縮によってデータが多く失われると、画像がブロック状になる現象が起こる。


使い道
ファイルサイズが小さいため、Webでの画像の公開に適しています。画像や漫画をJPEGでWebに公開することで、読者がコンテンツに素早くアクセスでき、クラウドストレージの容量も圧迫しません。

GIFファイル

GIFファイルの概要
GIFは、比較的単純な画像向けに設計されたラスターファイル形式で、主にインターネットでの表示に使用されます。
各ファイルでは、ピクセルあたり最大8ビットがサポートされており、256色のインデックスカラーが使用可能です。
また、GIFファイルでは、画像またはフレームを結合して基本的なアニメーションを作成可能です。GIFは、CompuServe社が開発し、LZW圧縮の特許権を米UNISYS社が取得していたため、日本国内では2004年6月20日の特許失効まで利用料を払う必要がありました。

GIFファイルの構成要素
GIFのバージョンは、GIF87 GIF87a GIF89a の3つが存在しますが、
アニメーションの表現が可能で最も一般的な GIF89a について解説していきます。
GIFファイルは、データを種類ごとに分けてブロックという区画で管理します。
以下がGIFファイルを構成する主なブロックです。

ファイルヘッダ (Header)
拡張画像ブロック (Graphic Control Extension)
アプリケーション拡張ブロック (Application Extension)
画像ブロック (Image Block)
トレーラー (Trailer)


ファイルヘッダ (Header)
ファイルヘッダには画像全体の情報が格納されています。



フィールド名
内容




Signature
GIFフォーマットであることを示す識別子


Version
GIFのバージョンを示す識別子


Logical Screen Width
画像の横幅


Logical Screen Height
画像の縦幅


Global Color Table Flag
共通パレット存在フラグ


Color Resolution
1画素のビット数


Sort Flag
共通パレットのソートフラグ


Size of Global Color Table
共通パレットに定義した色の数


Background Color Index
背景色とする色の指定


Pixel Aspect Ratio
ピクセルの縦横長さの比率


Global Color Table
共通パレットデータ(1色3バイトのデータが色の数だけ並ぶ)




拡張画像ブロック (Graphic Control Extension)
拡張画像ブロックは、このブロックの直後に続くImage Dataブロックに影響し透過色の指定やアニメGIFの設定などをする拡張ブロックです。



フィールド名
内容




Extension Introducer
ブロックの識別子


Graphic Control Label
拡張ブロックの識別子


Block Size
データの大きさ


Reserved
(未使用)


Disposal Method
次のイメージを重ねるときの処理


User Input Flag
ユーザー入力を受けるか


Transparent Color Flag
透過処理をするか


Delay Time
イメージ描画までの待ち時間


Transparent Color Index
透過色とする色の指定


Block Terminator
ブロックの終了を示す識別子




画像ブロック (Image Block)



フィールド名
内容




Image Separator
ブロックの識別子


Image Left Position
このイメージの描画開始位置(左端からのずれ量)


Image Top Position
このイメージの描画開始位置(上端からのずれ量)


Image Width
画像の横幅


Image Height
画像の縦幅


Local Color Table Flag
固有パレットがあるか


Interlace Flag
インターレースしているか


Sort Flag
固有パレットがソートされているか


Reserved
(未使用)


Size of Local Color Table
固有パレットに定義した色の数


Local Color Table
固有パレットデータ


LZW Minimum Code Size
圧縮データに関する情報


Block Size
Image Data の大きさ


Image Data
LZWで圧縮した画像データ


Block Terminator
ブロックの終了を示す識別子




アプリケーション拡張ブロック (Application Extension)
アプリケーション(ソフトウェア)が独自の情報を書き込み可能



フィールド名
内容




Extension Introducer
ブロックの識別子


Extension Label
拡張ブロックの識別子


Block Size
データの大きさ


Application Identifier
アプリケーションの識別名


Application Authentication Code
アプリケーションの識別コード


Block Size
Application Data の大きさ


Application Data
アプリケーションデータ  アニメGIFの場合は繰り返し回数


Block Terminator
ブロックの終了を示す識別子




トレーラー
ファイルの最後に1つだけ配置され、ファイルの終了を示しています。



フィールド名
内容




Trailer
GIFファイルの終端を示す識別子




GIFの利点と制限

利点

アニメーション: GIFは簡単にアニメーションを表示可能。ループすることができ、簡単に作成・共有可能。
汎用性: ほとんどのブラウザー、ソフトウェア、アプリケーションで利用可能。
サイズが小さい: 制限された色のパレットやLZW圧縮などの技術によりファイルサイズが小さい。


制限

色の制限: GIFは通常256色までの色パレットしかサポートしておらず、写真などの多彩な色を持つ画像には向いていません。


使い道
ウェブ上でのアニメーション広告、バナー、アイコンに使用されます。

TIFFファイル

TIFFファイルの概要
TIFFは、Microsoft社と現在は存在しないAldus社によって開発されたフォーマットです。
TIFFはモノクローム1ピクセル1バイトデータのみならず、1ピクセル2バイトデータ、1ピクセル4バイトデータ、さらに、複数の画像を1つのファイルに収録できる等、非常に汎用的なフォーマットです。
また、ジオリファレンス(デジタル画像ファイルを物理空間内の位置に関連付けること)に必要な位置情報も扱うことができるため、各種衛星データのフォーマットとして用いられます。

TIFFファイルの構造

ファイルヘッダ
ファイルヘッダは、TIFFファイルか否かの識別に使用されます。
データがモトローラ型なのか、インテル型なのかをここから読み取ります。
ファイルヘッダの最後は、IFDの場所を示すオフセット値が格納されています。
IFDはファイルヘッダの後のファイル内の任意の場所にあるため、ここに位置の情報を格納する必要があります。したがって、次に読み取る情報（IFD)は、このオフセット値の場所となります。

IFD (Image File Directory)
イメージファイルディレクトリは、画像の各種パラメ-ター、画像関連データ・カラーマップ・イメージデータへのポインタを格納した12バイトのデータエントリの集まりで、ファイルに対するディレクトリと同じような役目をしています。
IFDの各エントリの先頭には、データの型を示すタグと呼ばれる識別コードが保存されていて、IFDエントリのことをタグ付きポインタと呼ぶこともあります。
1つのIFDは1つのイメージデータに対応し、IFDはファイル中にいくつでも存在可能なので、TIFFはGIFのように1つのファイル中に複数のイメージデータを保存することも可能です。
しかし、通常は1つのIFDと1つのイメージデータが格納されています。
以下がIFDの構造となります。



フィールド名
size
備考




エントリカウント
2 byte
続くIFDエントリの数


IFDエントリ
12 byte
エントリカウント分IFDエントリが続く


...
...
...


...
...
...


IFDポインタ
4 byte
次のIFDのオフセット値  存在しなければ0




IFDエントリ
TIFFに記録されているエントリー（プロパティ）情報を表示する。



名前
size
内容




タグ
2 byte
データの識別コード


データの型
2 byte
1~12


カウントフィールド
4 byte
データフィールドに含まれる値の数


データフィールド  or データポインタ
4 byte
各種データまたはバイト単位のオフセット値




タグ
IFDエントリのデータフィールドに含まれるデータ、またはデータポインタが指し示すデータの種類を表す識別コード

データの型
データの型を表すコードで、以下のような種類があります。

1…BYTE型(1バイト整数)
2…ASCII型(1バイトのASCII文字)
3…SHORT型(2バイト短整数)
4…LONG型(4バイト長整数)
5…RATIONAL型(8バイト分数、4バイトの分子とそれに続く4バイトの分母)
6…SBYTE型(1バイト符号付き整数)
7…UNDEFINED型(あらゆる1バイトデータ)
8…SSHORT型(2バイト符号付き短整数)
9…SLONG型(4バイト符号付き長整数)
10…SRATIONAL型(8バイト符号付き分数、4バイトの分子とそれに続く4バイトの分母)
11…FLOAT型(4バイト実数、IEEE浮動小数点形式)
12…DOUBLE型(8バイト倍精度実数、IEEE倍精度浮動小数点形式)


カウントフィールド
データフィールドに含まれる値の数を表しています。
この値にデータの型からわかるデータのバイト数をかけたものが、データフィールドのバイト数となります。 ASCII型のデータの場合、ASCII文字列は1バイト以上のnull文字(00H)で終了している必要があり、そのnull文字もカウント数に含まれます。

データフィールドまたはデータポインタ
データ型とカウントフィールドの値から求められるデータフィールドのサイズが4バイト以内ならば、データそのものが含まれ、4バイトよりも大きければ、データフィールドに対するポインタ(ファイルの先頭を基点としたオフセット値)が含まれます。
データポインタは4バイトのLONG型整数によるオフセット値なので、4バイト全てが有効だが、データフィールドの場合はデータが左詰めで保存され、必要な上位バイトだけが有効になり、下位バイトには0がパディングされます。

イメージデータ
イメージデータには、実際の画像データが格納されています。画像データは通常、ピクセルの並び（ラスタ形式）で表現されます。各ピクセルには、色情報や輝度情報など、画像の特性に応じたデータが含まれます。

TIFFの利点と制限

利点

可逆圧縮: 元の画像データを正確に再現可能であり画質の低下や劣化が起こりません。
マルチファイル化： 複数の画像を1つのファイルにまとめることが可能。


制限

ファイルサイズ： データ容量が、他の画像ファイルフォーマットに較べて格段に大きくなってしまいます。
Web上での制限： Web上で使用する際は、他の画像ファイルフォーマットへ変換が必要です。


使い道
高画質写真、高解像度スキャン、コンテナファイルなどに使用されます。

PSDファイル

PSDファイルの概要
「PSD」は、アメリカのAdobe Systemsという会社によって提供されているAdobe Photoshopというグラフィックデザインの作成や編集で使われるアプリケーションで主に利用されるファイル形式であり、「Photoshop Data（又はDocument）」を略したものです。

PSDファイルの構造
PSDファイルは5つのセクションに分かれていてそれぞれ

ファイルヘッダセクション
カラーモードデータセクション
イメージリソースセクション
レイヤーとマスク情報セクション
イメージデータセクション

という名称が付けられています。各セクションの先頭アドレスは管理されていないため、先頭から順に各セクションのサイズを解析することで、それぞれのセクションの先頭アドレスが割り出すことができます。

ファイルヘッダセクション
このセクションには以下の要素が含まれます。



名前
備考




ファイル識別子
ファイルがPSD形式であることを示すファイル識別子


バージョン番号
常に 1 である事が期待される


予約領域
予約領域でデータは存在しない


チャンネル数
範囲は 1~56 まで


キャンバスの縦幅
(範囲は 1~30,000 まで)


キャンバスの横幅
(範囲は 1~30,000 まで)


データビット数
ビット深度(1, 8, 16, 32)


色モード
※詳細は後述




色モード
色空間の管理方法を示す。色空間番号と対応する色空間は下記の通り



色モード
色空間




0
Bitmap


1
Grayscale


2
Indexed


3
RGB


4
CMYK


7
Multichannel


8
Duotone


9
Lab




カラーモードデータセクション
このセクションでは色に関する様々な情報を格納しています。
カラーモードが Indexed, Duotoneである場合に使用されるが、Indexed, Duotone以外はこのセクションのサイズは0となります。

イメージリソースセクション
このセクションではPSDファイルを編集するソフトウェアが使用する設定値などの情報を示します。

レイヤーとマスク情報セクション
レイヤー数、レイヤー内のチャンネル、ブレンド範囲、調整レイヤー キー、エフェクト レイヤー、マスクパラメータなど、レイヤーとマスクに関する情報が含まれています。
レイヤーまたはマスクがない場合、このセクションはゼロで埋められた 4byteフィールドで表されます。

イメージデータセクション
このセクションには全てのレイヤーを統合した画像情報が格納されています。
ピクセルの色情報は、Red, Green, Blueの順番で格納されています。

PSDの利点と制限

利点

Adobe製品との相性: PSDはAdobe Photoshopのネイティブファイル形式なので、Adobeが開発するIllustratorやPremiere ProなどのAdobe製品に簡単に読み込み可能で連携がしやすい。
編集状態を保持可能: レイヤーやパスごとに編集や修正が可能で、Photoshopで作業中の状態をそのまま保存することが可能


制限

ファイルサイズ: 場合によってはサイズが膨大(最大2GB)になる可能性がある
Adobe製品を持っていないと開くことすら不可能


使い道
Webサイトのデザイン制作、写真の補正などに使用されます。

BMPファイル

BMPファイルの概要
BMP形式は、1990年代にMicrosoftがWindowsデバイスに依存しない画像データの表示方法として開発されました。
BMPはビットマップの略で、一般的に、「バンプ」と発音します。BMPは、様々な画面とデバイス間でデジタル画像の解像度を維持できるように、MicrosoftがWindows用に開発したファイル形式です。カラーまたは白黒の2D画像を保存可能です。
現在では、BMPファイルはWindowsデバイス専用ではなく、多くのMacやAndroidデバイスが対応するようになりました。

BMPファイルの構造
BMPファイルは以下のようにファイル構造がシンプルなのが特徴です。



名称
内容




ファイルヘッダ
ファイル情報(画像の色数・縦幅・横幅など)


イメージデータ
画像データの情報が格納されている




ファイルヘッダ



offset
size
概要
詳細




0
2byte
フォーマット種類
BMPなら0x42、0x4d


2
4byte
ファイルサイズ
単位[byte]


6
4byte
予約領域
将来の拡張用領域


10
4byte
ヘッダサイズ
この値からデータ部の先頭位置がわかる


14
4byte
情報ヘッダのサイズ
40[byte]


18
4byte
画像の横幅
単位[px]


22
4byte
画像の縦幅
単位[px]


26
2byte
プレーン数
常に1


28
2byte
1画素の色数
単位[bit]でRGBカラーの場合: 24グレースケールの場合: 8


30
4byte
圧縮形式



34
4byte
圧縮サイズ
単位[byte]


38
4byte
水平解像度
単位[ppm]=[px/m]


42
4byte
垂直解像度
単位[ppm]=[px/m]


46
4byte
色数
0なら全色使用


50
4byte
重要色数
0なら全色使用




イメージデータ
色数が24[bit]（RGBカラー）の場合は次のようになります。



offset
size
説明
補足




54
1
(0, 0)の画素値
Blueの値(0~255)


55
1
(0, 0)の画素値
Greenの値(0~255)


56
1
(0, 0)の画素値
Redの値(0~255)


57
1
(1, 0)の画素値
Blueの値(0~255)


58
1
(1, 0)の画素値
Greenの値(0~255)


59
1
(1, 0)の画素値
Redの値(0~255)


60
1
(2, 0)の画素値
Blueの値(0~255)


61
1
(2, 0)の画素値
Greenの値(0~255)


62
1
(2, 0)の画素値
Redの値(0~255)


︙
︙
︙
︙




BMPの利点と制限

利点

多くのWebブラウザーとソフトウェアに対応してます。
デバイスに依存せず、様々なデバイスに保存し異なる画面で表示しても品質は劣化しません。
非常に汎用性が高く、複数の色深度、プロファイル、アルファチャンネルを扱うことが可能。
データの圧縮にも対応しています。


制限

非圧縮のBMPは、JPEGやPNGと比べてはるかにファイルサイズが大きく、共有しにくくなることがあります。故に、Webサイトでの使用にも不向きで、空き容量の少ないハードディスクでは保存できないこともあります。
BMPに格納されるのはRGB画像のみである点も、主にCMYKで作業する場合には注意が必要です。


使い道
高画質デジタル画像の保管と表示等に使用されます。

SVGファイル

SVGファイルの概要
SVGは、Scalable Vector Graphics (拡大縮小可能なベクター画像) の略称です。
SVGは線をつないだ図形、テキスト、ビットマップイメージを組み合わせた画像を表すデータ形式です。XMLベースの為、ウェブブラウザでソースを閲覧でき、テキストエディタ等で編集できます。
また、HTMLとの親和性により、ハイパーリンクを埋め込んだり、JavaScript 等と連携させることも可能です。

SVGファイルの構造
SVGはXML形式で記述されるため、以下のソースを例に説明します。
<svg xmlns=""http://www.w3.org/2000/svg"" width=""512"" height=""512"" viewBox=""0 0 512 512"">
  (画像データ)
</svg>




フィールド名
内容




xmlns
XMLの名前空間のURIを指定する属性  XML名前空間とはXMLの要素を一意に識別するための仕組み


width
画像の横幅


height
画像の縦幅


viewBox
width, heightの値で決まるビューポートに対し座標を定める




図形の描画
SVG形式で描画できる基本的な図形をいくつか紹介します。
このコードを上記の(画像データ内)に記述することで図形を描画可能。

直線
<line x1=""10"" y1=""30"" x2=""190"" y2=""30"" stroke=""black"" stroke-width=""2"" />




フィールド名
内容




x1
始点のx座標


y1
始点のy座標


x2
始点のx座標


y2
始点のy座標


stroke
直線の色


stroke-width
直線の太さ




長方形
<rect x=""5"" y=""5"" width=""30"" height=""30"" stroke=""black"" fill=""#fff"" stroke-width=""2"" />




フィールド名
内容




x1
始点(左上)のx座標


y1
始点(左上)のy座標


x2
始点(右下)のx座標


y2
始点(右下)のy座標


stroke
直線の色


fill
塗りつぶす色


stroke-width
直線の太さ




円
<circle cx=""20"" cy=""20"" r=""15"" stroke=""black"" fill=""#fff"" stroke-width=""2"" />




フィールド名
内容




cx
中心のx座標


cy
中心のy座標


r
半径


stroke
直線の色


fill
塗りつぶす色


stroke-width
直線の太さ




SVGの利点と制限

利点

拡大や縮小表示の際に画質が荒くならず解像度を一定に保つことができます。
CSSによるアニメーション表現が可能。


制限

対応していないブラウザがあります。


使い道
2次元のグラフィック、グラフ、イラストをWebサイトで表示する場合に使用されます。

RAWファイル

RAWファイルの概要
RAWデータとは、デジタルカメラ、デジタル一眼レフカメラ、などに搭載されている写真編集の記録形式です。RAWデータ形式では、カメラ内部で画像に処理を行なわずデータを非圧縮または低圧縮率で保存します。
デジタルカメラには、レンズから入射した光をデジタルデータに変換するイメージセンサーという部品が搭載されています。内部のイメージセンサーが読み取った光の情報を、そのまま記録したものがRAWデータです。
通常、RAWデータはJPEGに変換してカメラ内のメモリーに保存されます。JPEGは見た目を大きく損なうことなくデータを圧縮することが可能ですが、不可逆圧縮なので、圧縮前に比べて画像の精度は劣化してしまいます。RAWデータは処理前のデータが記録されているため、JPEGに比べて高精度な画像の生成が可能です。
RAWデータは光の強弱についての情報のため、そのままでは画像として見ることはできません。そのため、RAWデータは一度パソコンにデータを取り込み、画像ファイルへの変換処理を行なう必要があります。この工程を「RAW現像処理」と呼びます。

RAWファイルの構造



フィールド名
形式
備考




RAWヘッダ部
text
カメラ設定情報


サムネイル画像部
jpeg
160×120程度


再生用JPEG画像部
jpeg
フルサイズjpegの1/2程度


RAWデータ部
raw
イメージセンサーが捉えた光の情報




RAWの利点と制限

利点

カメラのセンサーが捉えた細部がすべて記録されるため高品質のファイルを得ることが可能。
ホワイトバランスが撮影後に自由に変更可能。


制限

データのサイズが大きくなってしまいます。
RAWはカメラメーカーごとに独自の規格となっているので、現像には専用のソフトが必要です。


使い道
一眼レフカメラ、デジタルカメラ内の写真のデータなどに使用されます。

参考にしたサイト
PNG イメージを自力でパースしてみる
https://darkcrowcorvus.hatenablog.jp/entry/2017/02/12/235044
JPEGファイルの構造
https://hp.vector.co.jp/authors/VA032610/JPEGFormat/StructureOfJPEG.htm
JPEGファイルについて
https://www.adobe.com/jp/creativecloud/file-types/image/raster/jpeg-file.html
GIF画像ファイル フォーマット - 略して仮。
https://menyukko.ifdef.jp/cauldron/dtgifformat.html
ファイルフォーマット(CEOS, TIFF, GeoTIFF, BMP, RAW)
http://rs.aoyaman.com/seminar/about3.html
CGファイル概説 第5章 第2節 その1
http://www.snap-tck.com/room03/c02/cg/cg05_02.html
【解説】PSDファイルの仕様（基本構造編）
https://qiita.com/vram/items/2632cb9ed2dec9430f59
PSD - Photoshop 画像ファイル形式 - File Format Docs
https://docs.fileformat.com/ja/image/psd/
BMPファイルについて - Adobe
https://www.adobe.com/jp/creativecloud/file-types/image/raster/bmp-file.html
BMP画像のファイル構造（ヘッダ部・データ部）
https://algorithm.joho.info/image-processing/bmp-file-data-header/
SVGファイルについて - Adobe
https://www.adobe.com/jp/creativecloud/file-types/image/vector/svg-file.html
SVG入門
https://www.tohoho-web.com/ex/svg.html
RAWデータ - ICT用語集
https://www.ntt-west.co.jp/business/glossary/words-00247.html
RAWデータとは | RAW現像の基本
https://lrlab.exblog.jp/14195828/


弊社Nucoでは、他にも様々なお役立ち記事を公開しています。よかったら、Organizationのページも覗いてみてください。
また、Nucoでは一緒に働く仲間も募集しています！興味をお持ちいただける方は、こちらまで。





"
https://qiita.com/Muji___rushi/items/7d107c9561a461118f61,"['Python', 'データ分析', 'データサイエンス', 'Kaggle']","Kaggleアドベントカレンダー2023の19日目の記事です．

TL;DR

データ分析未経験からkaggleでどんなことを学んだか
想像していたデータ分析と実業務とのGap
kaggleやっていて良かったこと、kaggleでは学ばなかったこと


はじめに
趣味でkaggleを始めたことをきっかけに、現在はデータ分析の仕事をしています。
Muj!rush!というアカウントでKaggleをしています。Kaggle expertです。
kaggleを始めてから3年程度経過したので(この3年間は、地球の公転が早まってんのかってくらい時間が経つのが早かったです)、これまでを振り返ることで、今後kaggleを始めてデータサイエンティストを目指すような方への参考になれば幸いです。
Kaggleと出会ったことで仕事への向き合い方や、今後のキャリアの考え方が変わったので、
僭越ながら一言だけ言わせてもらうと、""kaggleは人生の役に立つ""。

※記事にある画像は全てchatgpt4で生成されています

バックグランド
2023年現在、社会人8年目で、去年まではインフラエンジニアでした。
インフラエンジニア時代は、基本的には上流工程の要件定義がメイン業務で、エンジニアとはいえ高度なインフラ技術を持ち合わせているわけではなく、それでいて肩書きは「インフラエンジニア」。
本当にエンジニアと名乗っていいのか？自分は何者なんだ？の答えのない問いが日々ありました。


Kagglerへの道のり

1年目（2021年) ... Kaggleに出会う
2021年の元旦1/1に、何者かになるために、自分の興味があることを「1年間で1000時間以上学ぶルール」を作り、実行しました。1000時間あれば何かしらのプロフェッショナルに近けるんじゃないかという、かなり浅い理由です。
元々データ分析は未経験でしたが、AIは基礎教養として知っといた方がいいのかなと思い、「独学プログラマー Python言語の基本から仕事のやり方まで」や、米国データサイエンティストがやさしく教えるデータサイエンスのためのPython講座など見てpythonは少し学習してました。
その頃にkaggleの存在を知り、データ分析ってなんか賢そうということで、kaggleスタートブックやkaggle本などkaggle関連の本を数冊買って、kaggleを始めました。
それでも最初の数ヶ月は、コンペでの具体的な議論や公開コードを見ても、全く理解できませんでした。
このため、まずは基礎を身につけるために、kaggleで毎月開催されていたTabular Playground Seriesに参加して、テーブルデータの読み込みから予測までの実装を一通り写経することを繰り返していました。
そんな中、CommonLit Readability PrizeというNLPコンペで、わけもわからず公開コードを適当にコピペしてアンサンブルの重みを変えたら、初めて銅メダルが取れました。
当時は、「なんか銅色の丸が付いたけど、何コレ」みたいな感覚で、kaggleの楽しみ方すらわかっていない状況でした。


2年目（2022年） ... Kaggleにハマる
2022年の1月に参加してたJigsaw Rate Severity of Toxic Commentsという、文章が有害かどうかを分類するNLPコンペでたまたま1submitで銀メダルがとれました。その時の模様は昨年のKaggle Advent Calendar 2022のこちらの記事に書いています。
それ以降、NLP関連のコンペに参加して、Kaggle Discussionみて手法を学び、それを実践したらスコアが少しずつ上がっていきました。kaggleにおける'活用'と'探索'を繰り返している時期で、大袈裟ですが、当時は1日単位で新しい知識が身についているような感覚があり、純粋に知らないことを学ぶのがとても楽しかったです。
当時は、仕事終わった直後に体力回復のため2,3時間寝て、夜に起きてkaggleやって深夜の3,4時に寝る、という生活を数ヶ月続けてました。コンペ終盤は学習が終わる時間帯に夜中に起きて、別の学習を回してまた寝る、というモデルの学習時間に生活リズムがコントロールされている状況でした(既にAIに人間の生活が支配されてる、、)
当時の仕事はデータ分析やプログラミングとは一切関係なかったので、まだまだ実力不足と思いながらも次第に仕事でデータ分析できたらいいなと思い始めました。このため、社内制度を使って異動希望を出したところ、無事に翌年から社内異動することが決定しました。
ちなみに、2年目もインプット1000時間ルールは継続実行していました(とはいえkaggleのdiscussion見る時間とかも含む)


3年目（2023年） ... データサイエンティストになる
2023年2月からデータサイエンティストになりました。
とにかく、世の中のKagglerに恥じないように頑張る、というモチベーションでした。

業務では、レコメンドなどを用いてユーザの行動変容を促す機械学習プロダクトなど、複数の分析チームの一員になりました。
それまで、1人でPC画面の向かってデータについてあれこれ考えていたので、配属当初は生身の人間とデータ分析に関する会話を出来てるだけでとても楽しかった記憶があります。
また、プロダクトからどんな価値を出していくか、そのためにAPIのインターフェイスをどう設計するか、コールドスタートなどの機械学習の普遍的な課題対処というようなことを日々検討しており(データサイエンティスト、アナリスト、機械学習エンジニア全部をやるイメージ)
商用化前のプロダクトについては、モデルの学習にはどんなデータが必要か、どれくらいデータがあればいいか、どうやって効果検証するか、なども考えます。
データ分析コンペへの参加も継続していて、KDDCUP2023(データマイニング国際会議であるKDDのワークショップ)に会社として参加して、幸運にも9位入賞してアメリカ現地発表(ポスター)と論文投稿を行うことができました。また、位置情報系の国際会議であるSIGSPATIAL2023のワークショップ形式コンペ(HuMob Challenge 2023)に会社の同僚とプライベートで参加して、3位入賞してドイツ現地にて発表する経験が出来ました(優秀な会社の後輩のおかげ)。
その他、kaggleでもOTTO, 火山コンペなどにも参加してNLPコンペ以外でも多く学ぶことができました。
ちなみに、3年目はインプットルールは撤廃しました（既に学ぶ習慣がついていたため）


想像していたデータ分析と実業務とのGap


データが生み出す価値を本気で考える。現場の課題に対してデータを使ってどんな価値を創れるを本気で考える経験もできました。


どんなデータを使うかは自分次第。社内データウェアハウスにあるデータを探すところから行う必要があります。もちろん、データ活用時には適切な社内決済をとります。


モデリングはデータサイエンスの一部。こちらは、kaggle始めた時からよく耳にしていたので、実際の業務でもモデリング検討時間は限られていますが、想定内でした。


Kaggleの外にも優秀なデータサイエンティストはたくさんいる。当たり前ですが、Kaggleの外にも優秀なデータサイエンティストがいるので、その方達と分析議論できるのは、とても大事な経験になっています


総じてポジティブなGapしかなく、思い切って社内異動して本当に良かったと感じています。

Kaggleやっていて良かったこと


知らないことも学べばいいだけマインド。多くのkagglerが、当たり前のように高度なモデルや手法を使いこなすしているので、自分もこれまで知らないモデルや手法であっても、特にハードルを感じずに実装を試すようになりました。結果的に自然とできることの幅が広がったように思います。

常にこれでいいかを自分に問い続ける。明日から強いKagglerが同じチームにきても恥ずかしくない精度か？を問うようになります。

課題やデータセットごとのベースラインや手法をアクセスできるようになる。上位解法を覚えてなくても参照先はわかる状態。レコメンドならこのコンペ、ベーシックなNLPならあのコンペ見ればきっと有益な情報あるはず、など。


Kaggleでは学ばなかったこと


AWSなどのクラウド知識。クラウド知識は割と必須の知識になるかと思うので、業務の前に個人アプリ開発などを行い基礎知識は身につけておけば良かったと思いました（今からでも遅くないことは内緒です）

Gitなどチーム開発の知識。ソロ参加が多く必要性も感じなかったので仕事を始めてから学ぶようになりました（Kaggleでもチーム参加などで学ぶ機会はあると思います）

精度向上に対する費用対効果。業務を始めて、kaggleでもコンペ初期ベースラインと最終1位のスコア差や実装量などが気になるようになりました。

でも、大丈夫です、「知らないことも学べばいいだけ」なので。

最後に
Kaggleを始めたことで、自分の趣味を仕事にすることができ、キャリアについても自ら切り開いていくものだと気づき、実行できました(それまでは、自発的に手を挙げて社内異動するような性格ではなかったです)
Kaggleを始める前に望んでいた、何者かになれたかはわかりませんが、データ分析を続けていくことで、きっと何者かになれるんじゃないかという希望があります。
Kaggleも業務もまだまだ発展途上なので、これからも成長して行きます！
(2024年はKaggle Master目指して頑張ります)

"
https://qiita.com/phen/items/94bd9e5d5dfd665ad044,"['Python', 'JavaScript', '初心者', 'エンジニア', '英語コンテンツ']","この記事はNuco Advent Calendar 2023の20日目の記事です。


弊社Nucoでは、他にも様々なお役立ち記事を公開しています。よかったら、Organizationのページも覗いてみてください。
また、Nucoでは一緒に働く仲間も募集しています！興味をお持ちいただける方は、こちらまで。



はじめに
エンジニアリングの分野では、技術の進化が日々進んでおり、エンジニアにとって継続的な学習が不可欠です。新しい技術の習得やスキルの向上に有効な学習手段が重要です。
日本では日本語のコンテンツはもちろんありますが、最先端の技術を取得するには、英語圏のコンテンツは切りたくても切り離せないことです。本記事は、日本人エンジニアが自身のスキルを向上させ、最新の技術に追いつくための優れた英語コンテンツをご紹介します。

エンジニア向けのおすすめの英語コンテンツ一覧
エンジニアにおすすめ17の英語コンテンツをご紹介します。

1.Codecademy (Website):



難易度：★★☆☆☆
言語：主にPython、JavaScript、HTML、CSS
無料と有料のバージョンがあり、対話的な学習を提供し、多様なプログラミング言語や技術に焦点を当てたコースを提供しています。プログラミングにおいて実践的なスキルを身につけるための理想的な環境であり、利用者はコーディングを実際に試しながら学ぶことができます。特にエンジニアが新しい技術やスキルを効果的に習得するための手段として重宝されています。
Codecademyはインタラクティブな学習アプローチで知られ、演習やプロジェクトを通じて実践的なコーディング体験を提供します。Codecademyの特長は使いやすいインターフェースとステップバイステップのガイダンスにあり、構造的な学習パスを求める初学者に最適です。対話型のコーディング演習の重点は、学習者がアプリケーションを通じて概念を理解し、理論的な理解と実践的なスキルの両方を向上させるのに役立ちます。コースの推奨平均学習時間は約20〜30時間であり、基本的なプログラミング知識を迅速に習得したい人にアクセスしやすいです。

2.freeCodeCamp (Website):



難易度：★★★☆☆
言語：主にHTML、CSS、JavaScript、およびさまざまなフレームワークを中心にWeb開発言語
無料でプログラミングを学ぶプラットフォームであり、YouTubeチャンネルも有益なチュートリアルを提供しています。特にプロジェクトベースの学習が重視されており、利用者は実際のアプリケーションを開発するプロジェクトを通じてスキルを向上させます。その活気あるコミュニティと共に、エンジニアが実用的なスキルを身につけるのに役立ちます。
freeCodeCampは実践的な成果に重点を置いており、コーディングの課題やプロジェクトを通じて実践的なコーディングを奨励しています。プラットフォームのユニークな特徴は、ソーシャルインパクトに重点を置いており、コーディングスキルを意義のあるプロジェクトに適用したい学習者に最適です。平均学習時間は学習者の探索の深さとプロジェクトへの参加度によって幅広く変動し、300から500時間の範囲があります。

3.The Net Ninja (YouTube):



難易度：★★☆☆☆
言語：Web開発とJavaScriptに焦点
ウェブ開発やJavaScriptに焦点を当てたわかりやすいチュートリアルを提供しています。エンジニアが実際の開発プロセスで遭遇するであろう課題に対処する方法が分かりやすく解説されており、初学者から中級者までに適しています。
The Net Ninjaはインプットを中心に学習スタイルを展開しています。各ビデオが特定のトピックに焦点を当てるように構造化されており、複雑な概念を簡素化する点がThe Net Ninjaの強みであり、初級者や中級者にアクセスしやすくしています。特定のトピックの平均学習時間は約10時間であり、Web開発スキルを拡張したい学習者にとって迅速で効果的なリソースです。

4.Traversy Media (YouTube):



難易度：★★☆☆☆
言語：様々（Web開発およびバックエンド）
初心者向けに分かりやすいウェブ開発やバックエンド技術のチュートリアルを提供しています。動画の解説は非常にわかりやすく、エンジニアがプログラミングスキルを向上させるのに役立つでしょう。
Traversy Mediaの特徴は、わかりやすく実践的なアプローチであり、実践的な学習経験を好む学習者に適し、Web開発コンセプトの迅速で明確な説明を求める人々にお勧めです。チュートリアルは初心者向けに設計されており、実践的な例の強調は学習者が知識を即座に適用できるようにします。特定のトピックの平均学習時間は約20時間であり、Web開発で実践的なスキルを得たい学習者にとって便利なリソースです。

5.HackerRank (Website):



難易度：★★★★☆
言語：アルゴリズムと問題解決に重点を置き、さまざまなプログラミング言語をカバー
コーディングのチャレンジと競技プログラミングに焦点を当て、YouTubeチャンネルでは解説とヒントを提供しています。企業がこのプラットフォームを利用して採用プロセスを行うことがあり、エンジニアが実際の業務に近い形でスキルを磨くのに貢献しています。
HackerRankの特徴は、問題解決スキルを向上させるコーディングの課題の実践的な学習に力を入れていることです。
中級から上級の学習者向けであり、技術面接の準備やアルゴリズムの問題解決能力を向上させたい人にお勧めです。プラットフォームのユニークな特徴は競技プログラミングに焦点を当てており、コーディングコンテストに興味がある学習者に最適です。平均学習時間はプロジェクトに依存し、一貫した練習がコーディングスキルをマスターするために必要です。

6.Khan Academy (Website):



難易度：★★☆☆☆
言語：主にJavaScriptとSQL
初心者向けのプログラミングコースを提供し、YouTubeでは補足的なビデオも提供しています。カーンアカデミーの特長は、各自の進度に合わせて学ぶことができる自己学習の柔軟性にあります。プログラミング初学者にとって、基礎から着実にスキルを築くのに適しています。
Khan Academyの特徴は、インプットがメインで、ステップバイステップの学習パスを求める初学者にお勧めです。プラットフォームの強みは明確な説明と対話型の演習にあり、学習者はブラウザでコーディングを実践できます。推奨される平均学習時間は約20〜30時間であり、プログラミングの概念をしっかりとした基盤で提供します。

7.Corey Schafer (YouTube):



難易度：★★☆☆☆
言語：Python
Pythonに特化した豊富な知識と経験を持つプログラマーであり、その専門的な洞察力を視聴者に伝えるためにYouTubeプラットフォームを活用しています。彼の動画は、初心者から中級者まで、幅広いスキルレベルの視聴者に向けて構築されています。その分かりやすい解説と実践的なアプローチにより、多くの人がプログラミングの基礎から応用まで学ぶことができます。
Corey　Schaferの特徴は明確な説明と実践的な例を提供することであり、実践的な学習アプローチを好む学習者に適しています。Pythonの概念の詳細な説明を好む視覚的および聴覚的な学習者にお勧めです。学習時間はトピックによって異なりますが、Coreyの教育スタイルは初心者にもアクセスしやすいです。ビデオはWeb開発や自動化などのトピックをカバーしており、学習者にPythonプログラミングの包括的な理解を提供します。

8.W3Schools (Website):



難易度：★★☆☆☆
言語：HTML、CSS、JavaScript
初学者から経験豊富な開発者まで、あらゆるスキルレベルの人々を対象にした教育プラットフォームとして知られています。ウェブ開発において重要な基礎知識を網羅しており、そのために必要なHTML、CSS、JavaScriptの基本から発展的なトピックまでを包括的に提供しています。これにより、ユーザーはウェブサイトのデザインから動的なコンテンツの実装まで、あらゆる側面でスキルを向上させることができます。
W3Schoolsは理論的なインプットとアウトプットにバランスが良く、その広範なドキュメンテーションにあり、Web開発の概念について詳細な説明を提供しています。プラットフォームは初心者やWeb開発テクノロジーに頼りにする人々に適しています。学習時間は柔軟であり、学習者は自分のペースで進むことができます。

9.Programming with Mosh (YouTube):



難易度：★★☆☆☆
言語：様々な言語（Java,HTML,CSS,Pythonなど幅広く）
Programming with Moshはプログラミング言語と開発ツールに関する実践的なアドバイスを提供して、実際のアプリケーションに焦点を当てており、実践的でプロジェクトベースの学習を好む学習者に適しています。チュートリアルは実際のシナリオとアプリケーションに焦点を当てています。Moshの簡潔で分かりやすいビデオは、シンプルな教育スタイルを好む視覚的な学習者に適しています。学習時間は対象の複雑さによって異なり、各ビデオは焦点を絞った理解を提供します。

10.MIT OpenCourseWare (Website):



難易度：★★★★☆
言語：様々な言語（コースの中で希望の言語が選べます）
MITのコンピュータサイエンスコースに無料でアクセスできるプラットフォームです。講義、課題、試験など、完全なコースコンテンツが提供されており、高度な情報技術を学びたいエンジニアにとって価値があります。アルゴリズム、データ構造、人工知能などを含む広範なコンピュータサイエンスのトピックをカバーしています。
MIT OpenCourseWareは中級から上級の学習者向けにお勧めであり、挑戦的で学問的に厳格なカリキュラムを求めている人々に適しています。プラットフォームのユニークな機能は、講義、課題、試験を含む完全なコースコンテンツの利用可能性です。平均学習時間はコースによって異なりますが、学習者はコンピュータサイエンスの理論に深く理解することができます。

11.Mozilla Developer Network (MDN) (Website):



難易度：★★★☆☆
言語：HTML、CSS、JavaScript
ウェブ開発者向けの包括的なリソースで、HTML、CSS、JavaScriptなどに関するドキュメンテーションが提供されています。エンジニアは正確で実践的な情報を得ることができ、ウェブ開発のスキル向上に寄与します。
特長は詳細なドキュメンテーションとチュートリアルにあり、主に理論的なインプットを求める学習者に最適です。Web開発技術の複雑な側面に深く入りたい学習者にお勧めです。プラットフォームの強みは権威あるドキュメンテーションであり、ブラウザの技術と標準に関する詳細な説明を提供しています。学習時間は探索されるトピックの複雑さに基づいて異なります。

12.LeetCode (Website): 　



難易度：★★★★★
言語：様々な言語（問題による）
コーディングのチャレンジに特化したプラットフォームで、特にコーディング面接での問題解決スキルを向上させることを目的としています。エンジニアは幅広い難易度の問題に取り組むことで、アルゴリズムとデータ構造に対する自信を築くことができます。
LeetCodeは技術面接や競技プログラミングに備える上級学習者にお勧めです。プラットフォームの特長は創造的で効率的な解決策が必要な広範なアルゴリズムの問題集です。平均学習時間は学習者と課題の難易度により異なります。

13.GeeksforGeeks (Website):



難易度：★★★☆☆
言語：C,C++,Java,Pythonなど多数
データ構造、アルゴリズム、プログラミング言語のチュートリアルを含む膨大なコレクションを提供するインプットとアウトプットのバランスがあるプラットフォームです。エンジニアはさまざまな難易度の問題に取り組みながら、実践的なスキルを磨くことができます。特にアルゴリズムとデータ構造に焦点を当てた学習に適しています。
GeeksforGeeksは中級学習者にお勧めであり、探求されるトピックの深さに学習時間が変わります。プラットフォームの強みは広範なコンテンツにあり、システマティックにさまざまなコンピュータサイエンスのトピックを探求したい人々に適しています。

14.CS Dojo (YouTube):



難易度：★★☆☆☆
言語：Python
ソフトウェア開発、アルゴリズム、データ構造に関する実践的なアドバイスを提供するYouTubeチャンネルです。特にアルゴリズムとデータ構造に焦点を当てており、これらの基本的な概念から始まり、応用例や実践的なコーディングの手法までを丁寧に解説しています。例えば、ソートアルゴリズム、グラフ理論、動的計画法など、プログラミングにおいて重要なトピックが広くカバーされています。
学習時間は比較的短く、ビデオは通常数分間です。これにより、学習者は小刻みに情報を消化することができます。CS Dojoの実践的なアプリケーションと実世界の例に焦点を当てたアプローチは、コーディングの概念を迅速に理解して適用したい人々におすすめです。

15.Simplilearn(Youtube):



難易度：★★★☆☆
言語：Python,Java,C++などの様々な言語をカバー（コース内容から選べます）
データサイエンス、クラウド、AI、機械学習などの豊富なコンテンツを提供しています。YouTubeチャンネルも活用され、視聴者は無料で利用でき、自分のペースで学習できる柔軟性があります。エンジニアは新しい技術やトレンドに迅速に対応するための知識を得るのに役立ちます。
Simplilearnはオンデマンドのコースで柔軟性を提供し、学習者は自分のペースで進むことができます。平均学習時間は選択したコースによって異なり、実践的なアプリケーションと業界で求められるスキルに焦点を当てています。多様なコンテンツと業界志向のアプローチにあり、新しいテクノロジードメインにスキルを向上させたり移行させたりする専門家向けに適しています。

16.Sololearn(Website):



難易度：★★☆☆☆
言語：C++,Python,HTML,CSS,SQL
インプットがメインの無料の学習コンテンツで、パソコンおよびモバイル端末で学習できるプログラミングコースを提供するプラットフォームです。エンジニアはスマートフォンを活用してプログラミングスキルを向上させ、簡単なクイズやコースを通じて楽しく学ぶことができます。
Sololearnはわかりやすいレッスンとソーシャルラーニングコミュニティを提供し、学習者は相互作用し知識を共有できます。平均学習時間は約20〜30時間であり、忙しいスケジュールを持つ学習者に隙間の時間でも学習が可能です。

17.Coursera (Website):



難易度：★★★☆☆
言語：コースにより様々
世界最高水準の大学や企業が提供するコース、プロフェッショナル認定証及び学位を無料で利用できるオンラインコースです。エンジニアは高品質な教育リソースにアクセスし、専門的なスキルを向上させることができます。
Courseraはより構造化された形式の学習体験を求める人々にお勧めです。平均学習時間はコースとその複雑さに基づいて異なります。柔軟な学習スケジュールを組むことができ、社会人や学生にも自分のペースで学習を進めます。プラットフォームは学問的な厳格性に特化し、認識される認定や学位を取得でき、キャリアの向上や学術的な認定を目指す学習者にとっておすすめです。

英語圏のコンテンツの学習の利点
これらのウェブサイトを利用することで、日本語でカバーしていない内容の学習や、実践的な応用とリアルワールドプロジェクトを通じてスキルを向上させることができます。ITの最先端である英語圏の国々（アメリカ、欧州）と同じ視点になり、海外へのプロジェクトの参加や、世界との競争力を高めることが出来ます。

最後に


弊社Nucoでは、他にも様々なお役立ち記事を公開しています。よかったら、Organizationのページも覗いてみてください。
また、Nucoでは一緒に働く仲間も募集しています！興味をお持ちいただける方は、こちらまで。


"
https://qiita.com/technuma/items/df14245d75244b8f437d,"['仕事術', 'ソフトスキル']","
はじめに
仕事に追われる日々から解放され快適に楽しく働くことができる環境を実現するためには、自己管理が重要です。ここでいう「仕事に追われず快適に楽しく働ける状態」とは、自分自身で意思決定を行い、仕事の進行を自らコントロールする能力を身につけることを意味します。
多くのエンジニアは仕事の量や複雑さに圧倒され、自分のペースで仕事を進めることができないという状況に直面しています。しかし、自己管理スキルを身につけることでこれらの課題を乗り越え、より自分起点な働き方が可能になります。
この記事では、よく起きがちな問題とあわせて自己管理を強化するための具体的な方法を示します。

1. 他の人から見て何をやっているかわからない問題

主要なポイント

「あれってどうなってます？」って聞かれていませんか？
これを頻繁に聞かれる場合、確実に何やっているかわからない人だと思われています
タスクの状態は、必ず聞かれる前にこちらから状況共有するのが鉄則です


具体的な例やエピソード

何をやっているのかわからないことがなぜ問題なのか

何をやっているのかわからない場合、以下の問題が潜んでいる場合があります

進捗が悪いままタスクを抱え込んでしまっているかもしれない
タスクのゴールを勘違いしてしまっているかもしれない
他の人と同じタスクを重複して取ってしまっているかもしれない




なにかボトルネックがある場合は早期に気づいて、チームとしては解消に動きたいです

抱え込んでしまうと、チームとしての課題発見・対応が遅れてしまい問題が大きくなってしまうことがある

NOTE: なぜ報告が遅れてしまうのか
「タスクが遅れそうです」と報告することで、「遅れそうなことが確定する」から、チームに迷惑をかけたくないがために遅れを確定させたくないという気持ちがはたらきます
「今は少しうまくいっていないけど、もう少し頑張れば取り戻せるはず！」と考えれば損失（遅れ）から目を背けることができるので、報告よりもタスクを優先してしまうことが多くあります
あなただけではなく全人類にこの損失確定回避バイアス（今名付けました）がかかっているので気にすることはありません
（僕も今損失確定を回避するために、夜遅くにこの記事を頑張って書いています）
遅れることが確定するギリギリまでタスクを持ち続けて「やっぱり間に合いません」と言うと、上司やチームメンバーとしてはリカバリーできる手段がかなり限られてしまいます
信頼を大きく損ねてしまうので注意しましょう




タスクの進行状況が不明瞭なのは、バッチ処理の進行状況を確認できない状況に似ています

例

良くないバッチ
僕: このバッチは30分くらいで終わるやろ（バッチを叩く）
バッチログ: ...
僕: 様子はわからんけどまぁ動いてるやろ
バッチログ(2時間後) : ...
僕: あれ、なんか問題あった？！ちゃんとログ吐いてたらもっとはやく気づけたのに・・・
良いバッチ
僕: このバッチは30分くらいで終わるやろ（バッチを叩く）
バッチログ(10分後): 120件中10件処理おわったで！
僕: あれ、30分で終わる想定だったけどこのままいくと2時間くらいかかりそうだなぁ...それなら処理を高速化してみるか
僕: 30分かけてバッチ高速化完了！いやー最初の10分で重いことがわかったから、時間を無駄にせずに対策できたわ〜


解説
良くないバッチの例では問題を早期発見できていないので、何も対応ができませんでした
一方良いバッチは、10分で想定と実態のズレを検知できたので、高速化してみるという手段でリカバリーできました
報告を早期にすることはまさにこのためで、締切間際に「間に合いません」と言われても打つ手がないのです
まだ時間が多く残っている状態であれば、多くのリカバリープランが残されていることが多いです
これが周りからみた「助けやすい状態」です
「このタスクって方向性ややり方ってあってるのかな」と不安があるときほど共有を増やして、助けてもらいやすい状態を維持しましょう




対策（初歩）

分報を徹底する

分報とは
分報とは、日報の分単位バージョンです
日報は1日に1回ですが、1日に1回だと共有のタイミングが遅すぎることから、分単位で共有していこう！という考え方で生まれたものが分報です
ただし、僕は1分単位でつぶやかないといけないとは思っていません
最大1時間までの幅で、作業内容やこれからやること、困っていることなどをTwitter(X)に書くように吐き出していきましょう
慣れてきたら思ったことをどんどん吐き出していくのがおすすめです
参考: 入社一ヶ月の分報戦記（Ubie）

どうやるのか

30分に1回、今までやったこと、次にやることを書いてslackのtimesチャンネルまたはチームのチャンネルに投稿しましょう
同じ作業内容が3回つづいたら、想定よりタスクが遅れていることをチームに報告するなど、できるだけ機械的に判断できるようにしておきましょう
慣れてきたらWorking Out Loud （常にアウトプットしながら物事を考える思考様式）をやっていくのもおすすめです
例: 分報の書き方
やったこと
xxチケットにて、migrationファイルを生成してテーブル作成のPullRequestを作った (PullRequestのリンク)
次にやること
xxチケットにて、xx modelに処理を追加する
コツ
できる限り具体的に書くとより助けてもらいやすくなりますし、自分のタスクがなにかに中断されても復帰しやすくなります
commitの粒度を細かくして、できるだけGitHubのリンクで共有できるようにしておくとより良いと思います


なぜやるのか

分報は自分の進捗をチームに可視化し、必要なサポートを受けやすくするために不可欠です
全人類に損失確定回避バイアスがかかっているので、損失確定回避バイアスから逃れる仕組みづくりが重要であるためです

もし""同じ作業内容が3回つづいたらチームに報告する"" ができなくても、分報を書き続けていればチームの誰かが「大丈夫ですか？」と声をかけてくれるかもしれません


NOTE: 分報はタスクのゴールを勘違いしている場合も早期発見に繋がる可能性を高めます
例えば、今からチームでサンドウィッチをつくりましょう！となりました
チームメンバーはハムや食パンなどを買っています
ところが自分だけ餅米を買っていたらどうでしょうか？
チームメンバーから「え、なんでサンドウィッチ作るのに餅米買ってるの！？」とツッコまれますよね
タスクが見えている場合は買うタイミングで気付けます
しかしタスクが見えにくい状態だと、下手したら餅をつき始めて気づくなんてこともあります
分報を徹底することで、「今から餅米買うぞ！」を大声で言ったらツッコミが入って軌道修正してもらえる可能性を高めます
なので、分報はチームメンバーからみてわかりやすく書くことを心がけましょう


Tips: 途中でも共有しましょう

タスクが100%終わってなくてもガンガン共有しましょう
20%しか終わっていなかったとしても、「現状20%くらいの完成度ですが確認してもらえますか」と前置きした状態での共有であれば全く問題ありません






対策（発展）

タスクを引き受けた瞬間に、これからの作業イメージや完了後の状態を共有する

どうやるのか

タスクを引き受けたときにやっておきたい確認例

なぜこの仕事をやるべきなのか

「改めて確認ですが、この仕事をやる理由はxxであってますか？」


タスク完了後にどういう状態になっていてほしいか

「この仕事が完了したらxxという状態になっている想定ですがあってますか？」


進め方の確認

「xxの箇所にデータ不整合があるかもしれないという調査タスクを進めるにあたって、進め方の確認させてください
まず半日程度かけて〇〇と△△という調査をする予定で、それでも解消しなかったらXXをしようと思っています
半日の調査結果次第で動き方が変わる可能性が高いので、半日調査時点で状況共有させてください
この進め方・共有のタイミングで問題ないでしょうか？」












2. ポジションを取らないことで、他の人に意思決定を委ねてしまう問題

主要なポイント

エンジニアの仕事で最も価値が高いものは、ポジションを取り「意思決定する」ことです


具体的な例やエピソード

「ポジションを取る」とは？

選択肢にA,Bがあるときに、「僕はxxという理由でAが良いと思います」と主張を明確にすることです


なぜポジションを取る必要があるのか

ポジションを取って初めて、チームメンバーや上司から「なんでそれを選んだんですか？」「xxの観点って考えられてますか？」などフィードバックをもらうことができます
このフィードバックに対して1つ1つ説明をしていくことで、自分がわかっていること・わかっていないことが明確になります


NOTE: ポジションを取ることで初めて脳が動く

僕のポジションを取るときと取らないときの頭の中はこんな感じです

ポジションを取るとき
僕: （AかBか決めたいな）
僕: （Aがいいかな）
脳: （本当に？どれくらいAがいいの？）
僕: （A:60, B:40くらいでAがいいなぁ）
脳: （なんでAってBより20多いの？どういう要因？）
僕: （20多い理由はxxだからかな・・）
僕: xxという理由でAが良いと思います
ポジションを取らないとき
僕: （AでもBでもどっちでもいいな）
脳: ...（思考停止）


人はなにかを決めるときに損をしたくないというバイアスが働くので、その瞬間に脳が動きます
Amazonでポチる瞬間、Uberで食べたいものを選ぶ瞬間、松屋で券売機を押す瞬間に実はみんなポジションを取っています（理由を聞かれることはありませんが）




対策（初歩）

「xxという前提だったら、yyという理由でAがいいと思いますがどう思いますか？」という形で、余白を残しつつ理由や根拠を明確にして主張することで、他のメンバーからのフィードバックを受け入れやすい形で提示しましょう


対策（発展）

何も意見を持たない相談ではなく自分の意見を含めた提案にすることで、組織としての意思決定スピードを大幅に上げることができます
（もちろん全くわからない場合は無理に提案する必要はないです）

上級

「自分はxxという理由でAにしようと思っていますが問題ないですか？」→承認者「問題ないです！」


中級

「自分はxxという理由でAが良いと思いますがどう思いますか？」→承認者「この観点が足りないので考慮をお願いします」


初級

「どうすればいいですか？」








3. 遠慮して質問しないことで、ドメイン知識（業務知識）が身につかない問題

主要なポイント

初期に自分が納得するまで質問しないと、後で議論についていけなくなります


具体的な例やエピソード

なぜ気をつけるべきか

質問を遠慮してしまうことで重要な業務知識の習得が遅れ、結果として開発効率が低下します
新しくプロダクトに関わるタイミング、新しいプロジェクトに関わるタイミングの一番最初こそがなんでも聞けるタイミング
「N年もいるのにこんなことも知らないんですか？」と思われるとつらいですね


プロダクトに関わる初期や、プロジェクト初期ほど納得するまでしつこく質問しましょう

なぜやったほうがいいか

議論はいくつもの前提の上に成り立っている

前提となるドメイン知識を持っていないと、濃い議論ができない








対策

特にわからないものがないと感じても、「xxという理解で合ってますか？」と自分の理解があっているかを確認しましょう
頭の中でわかった気になるのと、声に出して確認するのとは大きな違いがある
NOTE: 「チームの時間を自分の理解のために使ってもらうなんて気が引けるなぁ」という人へ
自分ばかりが質問しつづけていると、チームの時間を奪っているような気持ちになりますよね
「早くチームに貢献したい」と思う気持ちもわかりますが、1,2年後にしっかり返せばいいんです
僕は遠慮し続けて何も質問しない人よりも、不器用でもいいからとにかく理解しようとする人のほうが応援したくなります
NOTE: 初期に質問できてなかったなと後悔している人へ
最も質問しやすいのは初期ではありますが、もし現時点で議論についていけていないのであれば今から少しずつでも質問・確認しましょう
ドメイン知識を頑張って得る行動で支払ったコストは中期的に、確実に回収できます




おわりに
まとめると以下のアクションはやって損はないものなので、もしやっていないものがあれば試してみてください

作業を見える化するために分報を徹底する、作業イメージを予め確認する
ポジションをとって意思決定の主体を自分にし、「自分はこう思いますがどう思いますか？」という言い方にする
遠慮せずに質問し、理解の確認をすることでドメイン知識を獲得する


付録
書ききれなかったけど大事な考え方






"
https://qiita.com/DeepTama/items/09b5f57654aa7a8af6a7,"['Python', 'pandas', 'データサイエンス', '統計検定']","
 はじめに 

Pandasで大量データを扱って処理時間にイライラしたことはないでしょうか？
なんと、Pandasを従来のコードを変えずに高速化するライブラリィが出たみたいです。
NEC研究所が出したFireDucks 🔥🐦  というライブラリィで、ベータ版が無償公開されています。
しかも CPU環境でも高速化されるみたいです。詳細は下記のサイトを参照してください。



 
ベーター版ですが無償とは素晴らしいですね！　　
 
早速検証してみましょう。

 環境 

FireDucksの利用方法には、「インポートフック」、「明示的なインポート」の２種類があります。
「インポートフック」の場合は、pythonの起動時にオプションを指定することでコードの書き換えなしに動かすことができます。一方、「明示的なインポート」の場合は、import文にfireducksを明示的に指定します。

インポートフック
python3 -m fireducks.imhook your_script.py



明示的なインポート
# import pandas as pd
import fireducks.pandas as pd


今回、Googleコラボで検証するため、「明示的なインポート」を使用します。

 簡単な検証 

定番の tatanicデータ を使って検証してみます。
groupby()メソッドでの効果を測定(してみますが、データ量が少ないので1万回ループして計測して見ます。
動作環境はGoogleコラボ上で、効果の違いが出そうなcpuリソースを使います。

通常のPandas環境 

import pandas as pd
import time

# Githubに公開されているデータを直接読み込む
url = 'https://raw.githubusercontent.com/pandas-dev/pandas/master/doc/data/titanic.csv'
titanic = pd.read_csv(url)

start = time.time()

for i in range(10000):
    titanic.groupby([""Sex"", ""Pclass""])[""Fare""].mean()

elaps = time.time() - start
print(elaps)


おおよそ9.5秒でした。

FireDucks環境 



fireducks　のインストールを行います

!pip install fireducks


FireDucksでの処理時間の検証

import fireducks.pandas as pd2
url = 'https://raw.githubusercontent.com/pandas-dev/pandas/master/doc/data/titanic.csv'
titanic2 = pd2.read_csv(url)

import time

start = time.time()

for i in range(10000):
    titanic2.groupby([""Sex"", ""Pclass""])[""Fare""].mean()

elaps = time.time() - start
print(elaps)


約3.4秒で おおよそ３倍 もの高速化がされています。
これはすごい！。
大量データを扱う場合にはかなり効果が発揮されそうです。

 おわりに 

無償かつコードの変更なしに手軽るに使えるとなれば、使わない手はないと思います。
公式ホームページによると、全てのpandasのメソッドが高速化されている訳ではなく、順次対応メソッドを増やしていくとのことみたいです（FireDucksに対応されていないメソッドは、内部で通常のpandasのメソッドを呼び出しているので、使えないわけではなく高速化されないだけ見たいです）。
 
今後とも応援していきたいですね（今後とも無償での継続提供を期待しています!）。
 

[補足] 

Pandasをはじめ、Pythonやmatplotlib等のチュートリアルをオンラインで自己学習できるコンテンツを公開しています。よろしかったらこちらもご参照ください。



"
https://qiita.com/satoshihiraishi/items/3cbc0c0323066029c5a5,"['Java', 'Quarkus', 'googlecloud', 'CloudRun']","
はじめに
Retail AI Adventurers Advent Calendar 2023の21日目の投稿です。
昨日は@t-hiroyukiさんの「M1チップ搭載のMac & RancherDesktop環境下におけるTestcontainersを使用したテスト失敗の原因の考察とその解決策」でした。
彼と話していると何だか癒されます。

自己紹介
Retail AIの基盤チーム(Infrastructure Systems Development Group)に所属しています。
興味がある方は、以下を参照下さい。
会社HP：https://www.retail-ai.jp/
採用ページ：https://recruit.jobcan.jp/retail-ai/
今年に入って自作キーボードを5つ(ErgoArrows、CHARYBDIS、keyball61、keyball39、killer whale)購入し、現在2個故障中、1つ未作成です。

目次

背景
Quarkusについて
Kotlinでの実装
アプリケーションの作成
DockerImageの作成
CloudRunにデプロイ
まとめ


背景
4〜5ヶ月前だったか社内プロジェクトにおいて、アサインされて初めて知りました。
「かーかす？何ですか、それ？」
PHP歴10数年の私が、それ以来簡単なアプリを作るときに「Quarkusで作るにはどのJavaのライブラリ・・・」なんて考えるようになるとは。
直近では自分用にバーコード(JANコード）やQRコードを生成するものを作りました。

Quarkusについて
Quarkusは、Java用のフルスタックKubernetesネイティブフレームワークで、Javaのマイクロサービスアプリケーションの開発と運用に特化しています。その主な特徴や他のフレームワークとの違いについて以下の点が挙げられます。
1. コンパイル時最適化: Quarkusはビルド時に多くの処理を行うことで、ランタイムのオーバーヘッドを減少させます。これは、特にコンテナ化されたマイクロサービスやサーバレス環境でのパフォーマンスを向上させるために重要です。
2. メモリ使用量の削減: Quarkusアプリケーションは、従来のJavaアプリケーションに比べてメモリ使用量が少なくなる傾向があります。これは、特にリソースが限られている環境で有利です。
3. 起動時間の短縮: Quarkusは起動時間が非常に短いことで知られており、これもサーバレス環境やマイクロサービスアーキテクチャに適しています。
4. 拡張性と開発者フレンドリー: Quarkusは、多数のライブラリやフレームワークと統合することが可能で、開発者が既存の知識やコードベースを活用できるように設計されています。
5. Kubernetesネイティブ: QuarkusはKubernetesとの統合を重視しており、Kubernetes環境でのデプロイや管理が容易です。
他のフレームワークと比較して、Quarkusは特にコンテナ化された環境やマイクロサービスアーキテクチャに最適化されている点が大きな優位性です。従来のJava EEやSpring Bootなどのフレームワークに比べて、より効率的なリソース利用と高速なスタートアップを実現しています。これにより、クラウドネイティブアプリケーションの開発において重要な要素であるスケーラビリティとパフォーマンスが強化されています。


Kotlinでの実装
KotlinはJava Virtual Machine（JVM）上で動作するモダンなプログラミング言語であり、Javaとの互換性を持っているため、Quarkusのフレームワーク内で効果的に利用することができます。
QuarkusでKotlinを使用する利点は以下の通りです
互換性: KotlinはJavaとの高い互換性を持っており、既存のJavaライブラリやフレームワークをそのまま使用できます。これにより、Quarkusの機能とKotlinの利点を組み合わせて使用することが可能です
簡潔な構文: Kotlinは簡潔で読みやすい構文を持っており、開発者の生産性を高めます。Quarkusと組み合わせることで、開発プロセスがさらに効率的になります。
安全なコード: Kotlinはnull安全な言語設計をしており、NullPointerExceptionのような一般的な問題を回避するのに役立ちます。
コルーチンによる非同期処理: Kotlinのコルーチンは非同期プログラミングを簡単に実現します。これは、リアクティブプログラミングやマイクロサービスアーキテクチャにおいて特に有用です。
QuarkusはこれらのKotlinの特徴を活用し、クラウドネイティブJavaアプリケーションの開発をさらに強化します。したがって、Quarkusフレームワークを使用してKotlinでアプリケーションを作成することは、非常に有効な選択肢です。

SpringBoot3との比較
Javaのフレームワークで有名なSpringBoot3と何が違うのか、調べた範囲で比較します。

Dockerfileの自動生成
SpringBootでDockerfileは手動で生成するか、ビルドプラグインを用いる方法のようです。
OCI イメージのパッケージ化
Quarkusではビルドを実行するとDockerfileが4種類作成されます。
このあたりがクラウドファーストな感じがしますね。

legacy-jar
jvm
native-micro
native


コンテナイメージの軽量化
先ほどの４つのDockerファイルの中身を確認して、使用しているイメージをそれぞれ調べました。

legacy-jar、jvm

docker pull registry.access.redhat.com/ubi8/openjdk-17:1.16  # 401.94MB


native

docker pull registry.access.redhat.com/ubi8/ubi-minimal  # 93.17MB


native-micro

docker pull quay.io/quarkus/quarkus-micro-image:2.0  # 28.4MB

Java17のイメージで他に例を出すと
gcr.io/distroless/java17-debian12:latest  #  225.17MB
eclipse-temurin:17-jdk-alpine             #  315.08MB
openjdk:17-jdk-slim                       #  407.74MB

Quarkusのnative-microのイメージ容量があまりにも軽量すぎます。

検証
terraformのコードはこんな感じで
# .tfenvの定数をインポート
variable ""credentials"" {}
variable ""project_id"" {}
variable ""region"" {}
variable ""project_name"" {}
variable ""vpc_name"" {}
variable ""firewall_name"" {}
variable ""service_name"" {}
variable ""container_image"" {}

# プロバイダの設定
provider ""google"" {
  credentials = var.credentials
  project     = var.project_id
  region      = var.region
}

# VPCの作成
resource ""google_compute_network"" ""my_network"" {
  name                    = var.vpc_name
  auto_create_subnetworks = false
}

# ファイアウォールの設定
resource ""google_compute_firewall"" ""my_firewall"" {
  name        = var.firewall_name
  network     = google_compute_network.my_network.self_link
  source_tags = [""web""]

  allow {
    protocol = ""tcp""
    ports    = [""80"", ""443""]
  }
}

# Cloud Runの設定
resource ""google_cloud_run_v2_service"" ""my_service"" {
  name     = var.service_name
  location = var.region
  ingress  = ""INGRESS_TRAFFIC_ALL""

  template {
    containers {
      image = var.container_image
    }
    timeout = ""1s""
  }
}

data ""google_iam_policy"" ""admin"" {
  binding {
    role = ""roles/run.invoker""
    members = [
      ""allUsers"",
    ]
  }
}

resource ""google_cloud_run_v2_service_iam_policy"" ""noauth"" {
  name        = google_cloud_run_v2_service.my_service.name
  location    = google_cloud_run_v2_service.my_service.location
  project     = google_cloud_run_v2_service.my_service.project
  policy_data = data.google_iam_policy.admin.policy_data
}


Qurakusのコード
https://github.com/satoshihiraishi/quarkus-helloworld
nativeでイメージを作成し、作成されたイメージは87.9MBでした。
SpringBootのコード
https://github.com/satoshihiraishi/springboot-helloworld
openjdk:17-jdk-slimを使用し、作成されたイメージは457.23MBでした。

シンプルに「Hello World!」を表示するだけです。SprngBootはCLIで作成するとこの表示になりますが、Quarkusは異なったhtmlが出力されるので同じように合わせました。
それぞれのイメージを作成し、CloudRunにデプロイしてcurlコマンドで１０秒毎に20回アクセスしました。

結果
Cloud Loggingから算出しています。
SpringBoot：平均 5.9 ms

Quarkus：平均 5.2 ms

比較対象が文字の出力だけなので、もうちょっと違いが出やすい題材にすればとか、
SpringBootのイメージもdistrolessを使えばもう少し早くなってQuarkus超えてたかも、
などの反省は次に活かしたいと思います。

まとめ
Quarkusいいよ。

次回
私が会社に自作キーボードを使ってるのを見て、凄く興味を持ってる@k-yoshigaiさんがGo言語について書く予定です。
"
https://qiita.com/Yametaro/items/cd64df92660f63b61a6e,"['コミュニケーション', 'ウエストランド', 'あるなしクイズ']","
ある日、非エンジニアの同僚から学んだこと
今年の5月に、Qiitaの中の人たちと直接会う機会がありました。
（ポッドキャストの収録）
初対面ということもあり、私はなかなか打ち解けられずにモジモジしていました。
しかし、私の同僚の福太郎（本名）1という男は、超ハイテンションで───

福太郎「初めましてギュン！」

───と挨拶し、とにかく相手を褒めて、場を温めてくれました。
そのおかげで、いい感じの雰囲気でポッドキャストの収録が進みました。
（しかも、なんとその日の福太郎は謎の頭痛でメチャクチャ体調が悪かったといいます。）

コミュニケーション能力の大切さに気づいた
ちょっとした振る舞いの違いで、仕事の進み方がこんなに違うんだなぁ、と感じました。
なんというか「感情の因果応報」のようなものを見せつけられた気持ちになりました。
そして「自分も少しは見習わないとなぁ」と思いました。
でも、ワイのような陰キャがコミュ力を磨くと言っても、何をしていいか分かりません。

そこでAIを活用
そこで、エンジニアの味方「ChatGPT」を使って、他職種の人のいい所を学んでみようと考えました。
どうせなら面白く学びたいので───

ウエストランド風の「あるなしクイズ」形式で出力してもらおう！

───と考えました。
そして、ChatGPTに以下のプロンプトを入力しました。

「営業マンにあって、プログラマーにないもの」をセリフ形式で列挙してください。

（その他もろもろ、ウエストランドっぽくなるように調整しました）
さて、うまいこと他職種の人の良い所を学べるのでしょうか・・・！？
以下が、ChatGPTの出力結果です。

営業マンにあって、プログラマーにないもの①


偏見が強い
ウエストランド風になるように調整しすぎたせいか、私の心をえぐるようなセリフが出力されました。

学び
日光を浴びると、幸せホルモン「セロトニン」が出るらしいです。
私も外に出ようと思います。

営業マンにあって、プログラマーにないもの②


無茶を言うな
プログラミングのコードは専門的だから、説明は難しいんじゃ・・・！

学び
でも確かに「良い仕事をすること」と同じくらい「それを相手に伝えること」も大事だと思います。
意識しようと思いました。

営業マンにあって、プログラマーにないもの③


コードや技術書ばかり読んでいた
確かに、井口の言う通りかもしれません。

学び
コードやエラーメッセージだけでなく、感情の因果応報も読めるようになりたい。
そう思いました。

営業マンにあって、プログラマーにないもの④


みんな痔ではない
私がそうなるように指示したというのもありますが、それにしてもAIってこんなに偏見まみれの意見を生成できるんですね・・・！

学び
適度な運動も、幸せホルモン「セロトニン」の分泌を促すそうです。
取り入れようと思いました。

営業マンにあって、プログラマーにないもの⑤


確かに

「もっと自分自身と戦えよ！」

その通りだと思いました。

学び
でもまぁ、衣食住が確保されている状況で、スマホやPCを持っていて、SNSで揉めていられる・・・それは平和な証拠ですからね！
今のままでヨシ！

営業マンにあって、プログラマーにないもの⑥


言い過ぎ
感情のないロボットではないだろ。

学び
「感謝の気持ちを表現する」とか、そう言う行動は、愛情ホルモン「オキシトシン」の分泌を促すそうです。
私もどちらかというと無表情な方なので、気をつけようと思いました。

営業マンにあって、プログラマーにないもの⑦


営業マンだって不健康だろ
あいつらだって、飲み会ばっかしてて不健康だろ！

学び
健康的な食生活も、幸福ホルモン「セロトニン」の分泌を促すらしいです。
野菜とか食べようって思いました。

まとめ
なんか、AIのオススメしてくることを習慣化したら凄く脳が健康になりそう

宣伝
プログラミングの記事も書いてるから読んでね。

8歳娘「パパ、関数を合成して？」




福太郎さんは営業マンではなく広報の人です ↩



"
https://qiita.com/sachaos/items/e3a613b018febb898fde,"['JavaScript', 'Go', 'WebAssembly', 'wasm']","
はじめに
WebAssembly (略して Wasm) では WASI や WIT、 Component Model など様々な仕様があります。
それぞれが登場した背景、モチベーションなどを理解することでなんとなく概要を掴んでいくことができるのではないかと考えたため、過去・現在・未来と時間軸で整理してみました。
まず Wasm とその特徴に関して簡単に紹介した後、Wasm の過去として生まれた背景やモチベーションを紹介します。
そして現在の　Wasm　がなぜ注目を集めているのか、そして現在策定中の仕様と目指している未来について紹介します。

WebAssembly とはなにか

WebAssembly はスタックベースの仮想マシン用バイナリ命令フォーマットの仕様です。Wasm と略されます。
Wasm ファイル(Wasm モジュール)は一般に .wasm という拡張子で表されるバイナリファイルで、Wasm ランタイムと呼ばれるスタックベースの仮想マシンによって実行されます。
Wasm は W3C によってその仕様が標準化されています。
Wasm は以下のような代表的な特徴を持っています。以下それぞれを紹介します。

Language-independent (言語非依存)
Portable/Platform-independent (ポータブル/プラットフォーム非依存)
Safe (安全)
Fast (高速)


Language-Independent
Wasm には Language-Independent (言語非依存) であるという特徴があります。
これは、Rust、Go、C/C++、Zig など、様々なプログラミング言語から Wasm モジュール .wasm にコンパイルできることを意味しています。
特に、多くの言語のコンパイラ基盤として使用されている LLVM が Wasm をサポートしているため、Wasm にコンパイルできる言語の範囲が広がっています。


Portable/Platform-independent
Wasm のもう一つの大きな特徴は、Portable (ポータブル) であり、Platform-independent (プラットフォーム非依存) であることです。
Wasm モジュールは、特定のプラットフォームに依存していません。
例えば、サーバ上で Wasmtime という Wasm ランタイムを使用して動作させたり、Google Chrome の V8 エンジンなどのブラウザで実装されている Wasm ランタイムを利用してブラウザ上で動かすことが可能です。
また、Wasm は OS、 CPU アーキテクチャに依存していません。
OS (Windows、Mac、Linux) や CPU アーキテクチャ (x86-64、ARM) が異なる環境でも動作します。
Wasm ランタイムが OS や CPU アーキテクチャの差異を吸収しているため Wasm アプリケーションでは意識する必要がありません。


Safe
Wasm モジュールはサンドボックス内で動作するため、安全であるという特徴を持っています。
ここでいうサンドボックスとは以下のことを指しています。

与えられたホスト関数しか実行できない
与えられたメモリにしかアクセスできない


与えられたホスト関数しか実行できない
ホスト関数(Host functions) 1 とは Wasm ランタイムから Wasm モジュールに渡されるものです。
Wasm モジュールが入出力など外部システムへ影響を与える必要がある場合はこのホスト関数を経由する必要があります。
Wasm モジュールは利用するホスト関数を明示的に import として記述する必要があります。
これによって、そもそも危険なホスト関数を、　Wasm　モジュールに渡さないようにしたり、明らかに過剰なホスト関数を要求している Wasm モジュールは怪しいと判断することができます。
例えば Wasm ランタイムが Wasm モジュールに対して getrandom 関数のみを提供する場合、この　Wasm　モジュールは　getrandom　関数のみを利用でき、　　
open　や　write　など他の関数を実行することはできない、といったような具合です。


与えられたメモリにしかアクセスできない
Wasm モジュールはプロセスの全メモリにアクセスできるわけではなく、割り当てられたメモリ領域内のみで操作を行うことができます。
このため、他のメモリ領域に秘密情報が存在していたとしても、Wasm モジュールはこれにアクセスすることができず、安全性が保たれます。
もちろん Wasm ランタイムにバグがあった場合はこの限りではありません。


Fast
Wasm は、ネイティブコードに近い速度を目指しています。
ブラウザでは、 JavaScript よりも高速に実行することが可能です。
ブラウザ外での使われ方ではよくコンテナと比較されていますが、コンテナよりも起動速度が速いとされています。
なぜ起動速度が速いかというと Wasm は　Wasm　ランタイムさえあれば動作するため依存がすくなく、 また　Wasm　モジュールは一般的にコンテナイメージよりもサイズが小さいため、ダウンロードやロードに時間がかからないこと、さらに Wasm はすべての読み込みを待たずに起動することができるためです。

JVM との違い
Wasm はしばしば JVM（Java Virtual Machine）と比較されます。JVM と似ている点は「Write once, run anywhere」という点です。
しかし、 Wasm は対応している言語が多い、一つの企業に独占されていない、 サンドボックス環境である、起動速度が速い、メモリフットプリントが小さい、言語仕様が小さい、という点で異なります。

実際に Wasm Module を作ってみる
ここでは、実際に簡単な Wasm モジュールを作成してみます。
Go言語でもバージョン1.21から後述する WASI をサポートしているので、Wasmtime などの Wasm ランタイムで動作する Wasm モジュールを作成することができます。
以下は Go 言語で記述されたシンプルな Hello World と出力するプログラムです。
package main

import ""fmt""

func main() {
    fmt.Println(""Hello World"")
}

Go で Wasm モジュールをビルドする際には、GOOS=wasip1 という環境変数を設定します。これは WASI の preview 1 というスナップショットを意味しています。GOARCH には wasm を指定します。
ビルドされたWasmモジュールは、wasmtime や wasmer などのWasmランタイムを使用して実行することができます。以下はそのビルドと実行のコマンド例です。
# コンパイルして hello.wasm を出力
$ GOOS=wasip1 \
  GOARCH=wasm \
  go build -o hello.wasm \
  main.go

# wasmtime(Wasm ランタイム)で hello.wasm を実行
$ wasmtime hello.wasm
Hello World

# hello.wasm はバイナリになっている
$ xxd hello.wasm | head
00000000: 0061 736d 0100 0000 00f2 8080 8000 0a67  .asm...........g
00000010: 6f3a 6275 696c 6469 64ff 2047 6f20 6275  o:buildid. Go bu
00000020: 696c 6420 4944 3a20 2243 4a48 3844 6c4c  ild ID: ""CJH8DlL
00000030: 3061 4f64 5578 3033 7679 7647 482f 4e36  0aOdUx03vyvGH/N6
00000040: 3732 4f46 7853 654c 7465 534a 5274 4945  72OFxSeLteSJRtIE
00000050: 7464 2f73 4c4b 7730 6b7a 544f 4b67 534c  td/sLKw0kzTOKgSL
00000060: 4239 6768 6849 472f 5279 6644 4c44 5672  B9ghhIG/RyfDLDVr
00000070: 7941 2d70 3136 4f50 3255 6c50 220a 20ff  yA-p16OP2UlP"". .
00000080: 01d0 8080 8000 0e60 017f 017f 6000 017f  .......`....`...
00000090: 6001 7f00 6002 7f7f 017f 6003 7f7e 7f01  `...`.....`..~..


Wasm の過去
Wasm はブラウザ上での Javascript よりも高速な実行環境を求めた結果として誕生しました。
2008年に JavaScript の JIT コンパイラが開発され、ブラウザ上での実行環境の高速化の歴史が始まりました。 しかし JavaScript は動的な言語であり、既にその高速化には限界があることが明らかになっていました。
そこで、異なるアプローチでブラウザでの高速な実行環境を開発する試みが 2 つ始まりました。一つは Google が主導する NaCl（Native Client）プロジェクトで、もう一つは Emscripten と asm.js の組み合わせです。

NaCl
NaClは、プロセスのサンドボックス下でネイティブコードを実行し、高速化と安全性を実現することを目的としていました。
しかし、このプロジェクトは 2 つの問題に直面しました。
一つは、当時既にレガシーとされていたプラグインAPIに依存しているという問題で、結果的にコミュニティにうまく受け入れられませんでした。
もう一つはポータビリティの問題で、 x86 や ARM など CPU アーキテクチャごとに異なるマシンコードが必要であったことです。

Emscripten + asm.js
NaCl と同時期に、 Mozilla の Alon Zakai によって始められたのが Emscripten です。
Emscripten は Alon Zakai が趣味で開始したものだったのですが、後に雇用主の Mozilla のプロジェクトとして行うようになりました。
Emscripten は C++ コードを JavaScript コードにコンパイルするツールです。
asm.js は JavaScriptのサブセットで、型を強制することにより最適化を可能にするものです。しかし、asm.js もまた、拡張性に関する問題や、 JavaScript のシンタックスに依存しているためパースが遅いという問題がありました。
asm.js は以下のようなイメージです。
function add(x, y){
    x = x | 0;         // `| 0` (or 0)を付けることで値を変えずに
    y = y | 0;         // 符号付き32bit整数型であることを型証明&強制している。
    return x + y | 0;  // 戻り値も同様
}

asm.js はこのように型アノテーション(| 0) をつけて、値を変えずに、符号付き32bit整数型であることを型宣言&強制しています。
これによって型が定まるので最適化が可能となるため高速に動作させることができるのが asm.js です。
JavaScript のシンタックスの範囲内で型アノテーションをつけるというのが個人的に面白いなと思いました。

Google と Mozilla が協力
2013年に Google と Mozilla で協力を開始し、2015年には共同で WebAssembly プロジェクトを立ち上げました。
なので Wasm は NaCl と Emscripten + asm.js の良い点を取り入れています。
Wasm は asm.js のように JavaScript ベースとせず、 NaCl のようにバイナリコードベースとしています。
asm.js の良い点であるDOMとして同じプロセスで動作すること、そしてJavaScriptを通じてWeb APIを直接呼び出せる点を取り入れています。

過去の WebAssembly 利用例
Wasm はブラウザの高速化のために生まれた技術でした。
この当時の WebAssembly の利用例はブラウザにとどまります。
代表的な利用例としては Unity などのゲームエンジンや、 Google Earth が上げられます。
これらのアプリケーションは Wasm の高速性を活用してブラウザ上でよりリッチな体験を提供する事ができました。

Wasm の現在
Wasmは現在、ブラウザ外での実行環境として注目を集めています。
それはどういう経緯なのかを見ていきます。

WASI (WebAssembly System Interface) の登場

2019年には WebAssembly System Interface、略して WASI がアナウンスされました。WASI はWasm がシステムと会話するためのインターフェースを標準化するもので、現在は preview 1という仕様が広く利用されています。

WASI が出来た背景
WASI が開発された背景には、Wasm がブラウザを超えて使用され始めたことがあります。Fastly 2 や Node.js などが実際に Wasm をブラウザ外で利用し始めていたため、ブラウザ外のシステムと会話する方法が必要とされ、システムインターフェースの標準化が進められました。
WASI の具体例としては、fd_write（FileDescriptorに書き込む）、random_get（ランダムな値をバッファに入れる）、environ_get（環境変数を取得する）などがあります。
詳細はWebAssembly/WASI の仕様を御覧ください。

WASI によるポータビリティ
WASI により、Wasm のポータビリティの特徴を損なうことなくシステムとの会話が可能になります。
WASI はシステムコールの抽象化のようなものです。
Wasm のレイヤから OS ごとのシステムコール、 Windows だったら Windows API などと呼び分ける必要はありません。
Wasm モジュールは WASI を呼び出せばよく、 Wasm ランタイムが OS ごとの差異を吸収してくれています。


WASI による安全性
Wasm のサンドボックスの特徴で説明したように、Wasm モジュールは利用する依存関係を明示的にインポートする必要があります。WASI の関数に関しても同様で、インポートされていないものは使用できません。
そのため Wasm のサンドボックスの特徴によるセキュリティも WASI にそのまま適用できます。

Wasm ユースケースの変化
WASIのアナウンスにより、Wasmのユースケースはブラウザ外へと広がりました。
Docker の創始者は、2008年に WASM と WASI があれば Docker を作る必要がなかったと述べています。

If WASM+WASI existed in 2008, we wouldn't have needed to created Docker. That's how important it is. Webassembly on the server is the future of computing. A standardized system interface was the missing link. Let's hope WASI is up to the task! https://t.co/wnXQg4kwa4— Solomon Hykes / @shykes@hachyderm.io (@solomonstre) March 27, 2019
 
具体的には、アプリケーションの実行環境としては Azure では AzureKubernetesService が Wasm の実行をサポートしていたり、Fastly では Edge コンピューティング環境での Wasm の実行をサポートしていたり、 Fermyon という Wasm を動かす PaaS もあります。
また、プラグインシステムとしての利用例として Envoy が対応していたり、 kube-scheduler では採用を検討していたりします。
こちらに関しては プラグイン実行エンジンとしてのWasm も参考になるかと思います。

Wasm の未来
Wasm は将来、ソフトウェア開発のエコシステムに革命をもたらす可能性を秘めています。
特に注目されているのが、現在策定中の Component Model という仕様です。

Component Model
Component Model は、Wasm モジュール間の連携を可能にするための追加仕様です。
Component Model は2つの要素から構成されています。
一つ目は WIT（WebAssembly Interface Type）で、もう一つはそのバイナリ表現である Canonical ABIです。

Component Model の背景
Component Model は、Wasm モジュールの再利用性を高めるために仕様の策定が進められています。
実は Wasm Core の仕様は型が少なくinteger, float それぞれ 32bit, 64bit の 4 つと、 SIMD 用の 128bit の vector 型のみです。
これではモジュール間で文字列や構造体などの複雑なデータ型の受け渡しをするのが困難というのが課題でした。
これを解決するために、WebAssembly Interface Type (WIT) が開発されました。

WIT (WebAssembly Interface Type)
WIT は、Wasm モジュールが何をインポートし、何をエクスポートしているのかを記述できる IDL (Interface Definition Language) です。 ProtocolBuffer のようなものを想像すればわかりやすいのではないかなと思います。
これにより、String、Struct、List などのリッチなデータ型を Wasm Core の基本型以外で使用できるようになります。
例えば、次の WIT の例では my-world というコンポーネントが log 関数（stringをパラメータとして受け取る）をインポートし、 run 関数をエクスポートしています。これにより、外部から log 関数を提供することを期待し、外部から run 関数を呼び出せることを表現しています。
package local:demo

world my-world {
  import log: func(message: string)

  export run: func()
}


Component を組み合わせる
各 Component が WIT で import, export を定義しているので、それを満たすように Component を組み合わせてあげることが必要になります。
下の例では、右側の app というコンポーネントが add, log という関数を import しているので、
add 関数を export している calc というコンポーネント、
log 関数を export している logger というコンポーネントを渡すことで
app の依存を満たしています。


Component Model と Bytecode Alliance
Component Model が解決しようとしている問題については、Bytecode Alliance のブログポストが参考になります。
Bytecode Alliance は 2019年に設立された団体です。
初期メンバーは Mozilla, Fastly, Intel, Red Hat です。
Bytecode Alliance の目的は WebAssembly, WASI の標準の実装や提案を行って、ブラウザ外の WebAssembly の未来を作ることです。
また、 Wasmtime など主要な Wasm ランタイムなどのメンテナンスを行っております。

Bytecode Alliance の考える課題
Bytecode Alliance は現在のソフトウェア開発では、開発したソフトウェアの 80%が他人のコード(ライブラリ)で構成されており、これによりセキュリティ上のリスクが生じると主張しています。
現在のソフトウェアの実行の仕組みではソフトウェアに与えられる権限は、依存しているライブラリにも同様に等しく渡されるため、これらのライブラリが悪意を持っている、もしくは脆弱性がある可能性があり危険であるということを主張しています。
確かに、ライブラリに悪意あるコードが紛れ込ませられていたという問題は見聞きしたことがありますし、ライブラリの脆弱性が報告されたためアップデートする必要が発生するというのはよくあることのように思います。


Component Model が解決する問題
Component Model は、Wasm モジュール間の相互運用性を高めることによって、この問題を解決します。
Wasm モジュールの相互運用性が向上することによって Wasm モジュールをライブラリとして使いやすくなります。
Wasm モジュールをライブラリとして利用できるようにすると、 Wasm の特徴である安全性によってこの問題を解決できます。
Wasm の特徴で話したように Wasm モジュールのメモリ空間は他と別れています。
なので重要な情報をアプリケーションが持っていても、それを渡さなければ悪意のあるライブラリはそこにアクセスができません。
また、Wasm モジュールはそれぞれ独立したサンドボックスで動作し、インポートされた外部依存しか利用できません。
なので重要な外部依存(e.g. ファイルアクセス、ネットワークなど)をリスクの高いコンポーネントに渡さないということも将来的に可能になるそうです。

また、Component Modelにより、プログラミング言語のサイロ化を解決することができます。
いままでは基本的には様々な言語で同様なライブラリが作られていました。
しかし Component Model により、異なるプログラミング言語で作成された Wasm モジュールを組み合わせて使用することが可能になります。
これにより、例えば Go で作成されたモジュールを C++ やRustから使用することが可能になります。

まとめ
本記事では、WebAssembly (Wasm) の概要、生まれた背景、現在注目されている理由、WASI、Component Model とその未来について見ていきました。
Wasm は多言語対応、プラットフォーム非依存、安全性、高速性など多くの特徴を持ち、これらの特徴が組み合わさって独自の技術エコシステムを築いていっています。
特に、WASIの導入によりブラウザ外での使用が可能になり、Wasm の用途はさらに拡大していますし、 最近は WASI の一部として進められている wasi-nn を利用して LLM の実行環境として利用する例なども流行っている印象です。
また Component Model などの新しい仕様は、Wasmモジュール間の相互運用性を高め、サードパーティライブラリのセキュリティ問題を緩和し、プログラミング言語のサイロ化を解決して開発のエコシステムを変える可能性があるかもしれません。
これからの Wasm の発展に期待したいですね。

参考

https://webassembly.org/
https://wasi.dev/
https://bytecodealliance.org/
https://zenn.dev/hodagi/articles/4925afbeb3c4dc
https://www.youtube.com/watch?v=XuZt1OCCQTg
https://www.youtube.com/watch?v=6_BRLqxiZPU
https://bytecodealliance.org/articles/announcing-the-bytecode-alliance
https://component-model.bytecodealliance.org/
https://github.com/WebAssembly/spec
https://github.com/WebAssembly/WASI
https://github.com/WebAssembly/component-model
https://logmi.jp/tech/articles/324956

"
https://qiita.com/nukipei/items/ee14f83a436231d3a0e5,"['Python', '最適化', 'pulp', '数理最適化', 'Streamlit']","
はじめに
こんにちは！
私は業務で、数理最適化を活用したシステム開発、および導入支援に従事しています。
本記事は、私の妻が担当した 雑務（部署の懇親会のグループ分け） を題材として、それを数理最適化問題に落とし込み、条件を満たすようなグループ分けを求める簡易的なアプリをPythonで実装してみました。
「とりあえず動くものを作る」という意識で取り組んだため、もし特に数理最適化周りについて、より効率的なアプローチをご存じの方、また思いついた方は、ぜひご指摘いただけると幸いです。

具体的に・・・
「部署の懇親会のグループ分け」の背景や概要は以下の通りとのことでした。

部署全体の人数は 100人程度、またその部署内にいくつかのチームが存在する。
この部署には、特に若手について、別チームの社員との親交が薄いという課題があった。
そこで、別チームの若手同士や、若手と年次が上の社員の親睦を深めるために懇親会が開催されることとなった（以下、年次が上の社員を「ベテラン」と呼びます）。
懇親会は全員参加の予定であるため、当日はいくつかのグループに分かれる。（このグループ分けを妻が担当）
グループごとに、若手の人数はなるべく均等にしたい。
各グループ内の社員の所属チームはなるべくばらけるようにしたい。


このようなグループ分けを、妻は手動で実施していたとのことなのですが、 「できるだけ、チーム・年齢層がばらばらになるように、100人のグループ分けをすること」 は、人力では至難の業であり、実際に妻もだいぶ苦戦していました。

一方、この手のグループ分けは数理最適化の得意とするところであり、同じ条件ではないですが、 「数理最適化でグループ分け」 に取り組んだ記事1や書籍2もいくつか存在します。
そこで、「じゃあ、やってみよう」と思い、実際に取り組んでみたというのが、この記事の前振りとなります。

実装した簡易アプリ
実装した簡易アプリは以下の通りです。（雑なUIで恐縮ですが。。）

利用した言語・ライブラリのバージョン・依存関係は以下の通りです。
python = "">=3.11,<3.12""
numpy = ""^1.24.3""
pulp = ""^2.7.0""
streamlit = ""^1.29.0""
pandas = ""^2.1.3""


Pythonは3.11.4を利用
画面はstreamlitを利用
最適化計算はモデリングツールをPuLP、ソルバーはデフォルトのCBCを利用
データ操作にPandasやNumPyを利用

実装したプログラムは以下のリポジトリに格納しています。



具体的なインプット（入力データ）とアウトプット（計算結果）は以下の通りです。

インプット

社員数
チーム数
各社員ごとの情報（年齢層, 所属チーム）
1グループあたりの人数


アウトプット

グループ分けの結果（CSVダウンロードも可）
グループごとの年齢層・チームの内訳を示したグラフ
チーム被り状況を示した表



具体的な、インプット・アウトプットの詳細イメージは本記事の最後に説明していますので、興味のある方は是非ご覧ください。


本記事では、実装の詳細や、具体的な定式化の説明は行いません。興味のある方は、↑のGitHubを見ていただければと思います。



最適化のポイント
「部署の懇親会のグループ分け」の要件について、最適化計算に関係のある部分を再掲すると、以下の通りです。

部署全体の人数は 100人程度、またその部署内にいくつかのチーム（今回は7チームほど）が存在する。

当日はいくつかのグループ（今回は7 or 8人）に分かれる。
グループごとに、若手の人数はなるべく均等にしたい。
各グループ内の社員の所属チームはなるべくばらけるようにしたい。

細かい定式化の説明は長くなってしまうので割愛し、その代わりにグループ分けを最適化問題に落とし込んだ際のポイントをいくつか紹介します。

ポイント①：「ばらばら」「均等」を数式で表現する。
今回扱う懇親会グループ分けでは、「所属チームはなるべくばらけるようにしたい」「若手の人数はなるべく均等にしたい」とあるように、社員の属性を分散させたいというのが主旨ということがわかると思います。しかし、素直に「分散をそのまま最小化する問題」としてしまうと、分散が非線形であるため、解きづらくなってしまいます。
そこで、今回は他の記事に倣い3、 「各グループの社員の属性被りの数の、最大と最小の差を最小にする問題」 として解いています。
例えば、簡単のために部署内に2チームしか存在しないとして、各グループのチーム被りの数を均等にしたいというケースを考えます。この時、以下を最小にすることで、均等を表現します。
max(各グループのチーム1の人数)-min(各グループのチーム1の人数)

以下の図にて、直観的な理解を示します。チーム1の人数の最大値と最小値の差が小さい、グループ分け案②のほうが、各グループの所属チームが均等になっていることがわかると思います。


ポイント②：条件の優先度を考慮して、計算する。
ポイント①の方法で、チームと年齢層ができるだけ分散するようなグループ分けを求め、妻に見せたところ・・・

このように「うーん」という顔をされました。どうやら、チーム数の都合上、どうしてもグループ内でチーム被りが発生してしまうのですが、その際 「若手のチーム被りを優先して、発生させないようにしたい」 とのことでした。この時の実装は、若手もベテランも平等にチーム被りが発生する状況となっており、そこが微妙ということらしいです（厳しい・・）。
そこで、最後の要件である 「各グループ内の社員の所属チームはなるべくばらけるようにしたい。」 について、より詳細に整理したところ、以下のように、同じばらばらでも優先度が存在することがわかりました。

同じグループの人はすべて別チームがベスト（ほとんど無いケースなので、今回は考えません。）
もしチーム被りが発生する場合は、以下の優先度順でばらけさせたいとのこと。

第一優先：各グループ内で、若手同士のチーム被りができるだけ発生しないようにする。
第二優先：各グループ内で、若手全員がベテランとできるだけチームが被らないようにする。
第三優先：各グループ内でベテラン同士のチーム被りができるだけ発生しないようにする。




そこで、東京海洋大学の久保先生がX4で紹介した、優先順位付きの多目的最適化に対するアプローチを適用してみました。
本記事での具体的なアプローチは以下の通りです。

まず最初に、各グループ内の「若手のチーム被り」ができるだけ発生しないように、目的関数を設定し、それを最小にするようなグループ分けを求める。
次に、「各若手と同じグループのベテランのチーム被り」ができるだけ発生しないように、目的関数を設定し、それを最小にするようなグループ分けを求める。この時、「1の目的関数 ≦ 1で求めた最適値」 という制約を加える。
最後に、各グループ内の「ベテランのチーム被り」ができるだけ発生しないように、目的関数を設定し、それを最小にするようなグループ分けを求める。この時、「1の目的関数 ≦ 1で求めた最適値」 と、「2の目的関数 ≦ 2で求めた最適値」 という制約を加える。

注目すべきは、より優先順位が高いものから考慮に入れて最適化計算を行い、その解を制約として後続の計算に追加している点です。これにより、優先順位通りのグループ分けを実現することができます。
このアプローチを取って、再度グループ分けを計算し、見せたところ、「いいね」と納得してもらえました。

あっさり説明で恐縮ですが、本記事はここまでにしたいと思います。（より細かい定式化の話は、どこかのタイミングで別記事でしたいと思います。）

まとめ
妻が担当した雑務である「部署の懇親会のグループ分け」の簡易アプリをPythonで実装し、その概要やポイントを紹介しました。
何か、コメント、指摘がある方は是非お願いします！特に最適化計算については、個人的にほかに良いアプローチがある気もしているので、より効率的なアプローチをご存じの方、また思いついた方は、ぜひご指摘いただけると嬉しいです。

（最後に）画面のイメージ
最後に、作成したグループ分けアプリのインプット・アウトプットのイメージをいくつか紹介します。無駄に様々な情報を可視化していますので、ここで供養させてください。

インプットデータの設定

社員数の設定
部署全体の社員数を数値で設定します。


チーム数の設定
部署内のチーム数を数値で設定します。


各社員がどのチーム、どの年齢層なのかを設定
csvファイルで各社員がどのチーム、どの年齢層なのかを設定します。
現状は、ランダムに生成したデータと、サーバーに直接配置した手動設定データのみを設定できます。（そのうち、アップロードできるようにするかもです。。）

各社員のチーム、年齢層は「データを表示」を押下することで、確認できます。


計算の実行
「グループ分け実行」を押下すると、計算を実行します。計算中は以下のように、「計算中」と表示されます。


アウトプットの確認

グループごとの社員一覧の確認
グループごとの社員一覧を確認できます。また、社員のチームや年齢層ごとに色を分けて表示することも可能です（図はチームごとに色分けをした例。）

ちなみに、★マークは若手の社員であることを示しています。

グループごとの年齢層・チームの内訳
記事冒頭のGIF画像で紹介できなかった機能ですが、グループごとの社員の年齢層とチームの内訳をグラフで確認できます。図は年齢層の内訳を示していますが、年齢層が均等になっていることがわかるかと思います。


チーム被り状況の確認
こちらもGIF画像にはない機能で、グループごと、チームごとのチーム被りの状況を確認できます。セレクトボックスで選択したチーム（図ではチームA）の年齢層ごとの被り数や、各グループの分散具合を確認できます。


結果をCSV形式でダウンロード
おまけ機能ですが、以下の画像のように、ボタンを押下することで、グループ分けの結果をCSV形式でダウンロードできます。




https://qiita.com/matsulib/items/898873b73d584c7dcb8b 等 ↩


https://www.ohmsha.co.jp/book/9784274227356/ ↩


https://qiita.com/SaitoTsutomu/items/f4478dfbc3c1cf6425e3 ↩


https://x.com/MickeyKubo/status/1700297528376279055 ↩



"
https://qiita.com/ussy_k/items/cfee1b94a10830d7ad86,"['editor', 'ツール', 'VSCode', 'cursor', 'GPT-4']","
はじめに
Systemi（株式会社システムアイ） Advent Calendar 2023　21日目の記事です
↑より毎日、弊社社員が記事を投稿しています！
本記事と合わせて他記事も是非読んでみてください！！
個人としては、記念すべき初投稿になります！！

本編
私は、VSCodeが最強のエディタだと思ってました。
ですが、最近「もうVScodeは使いません」だの「R.I.P VScode」だの申し上げている方々を見つけ、
気になって深掘りしていった結果、今回ご紹介するCursor Editorに辿りつきました。
認知度もだいぶ上がってきていることを日々感じながらも書いていきます(汗)

AIファーストのコードエディター　「Cursor」
CursorはAnysphare社が製作したAI統合ソフトウェア開発プロジェクトです
凄くシンプルに表現すると「VSCode」 × 「ChatGPT 4」です
プランによってChatGptの利用制限などもあります



価格は以下の通りです。
触ってみて個人的に感じたのは無料だと満足しなくて、Proにした方が良いなとは感じました
※2023年12月時点で2130円/月なので少々お高く感じるかもしれませんが、
Gpt-4が今入れないという点と特徴の一つとして、エディタと一体な点を踏まえるとアリです

公式HPのForumをクリックするとCurosor Editorのコミュニティサイト繋がっています


まずは使ってみよう
公式HPからインストールする


インストールをするとショートカットや言語、コマンドラインの設定が可能
デフォルトでVSCodeと記載されているあたり、意識しているのが伝わる

Cursorの特徴の一つとして、VSCodeユーザなら、既存で使用しているVSCodeの拡張機能,キーバインド,他設定を引き継ぐ(VSCodeからImportする)ことができる！！

ぱっと見VSCodeだなと思いつつ、右の方にAI Chatスペースがありフォーカスすると...

チャット欄があり、gpt-4とgpt3.5を選択できたり、imageを挿しこんだりできます

試しにヘッダーを作ってもらうことにします

(すみません、色々触ってたらgpt-4の回数制限超えました。それに関して少し下の方に記載しています)

簡単な実装をあっさり出してくれました
ここらはChatGPTと何ら変わりのない状態ですね


メンション機能
因みにさらっと@Next.jsとメンション付けて書いていましたが、
@を押すと、キャプチャの様にファイルやドキュメント、gitコミットなどを読み込ませることができます
Docsから公式ドキュメントを読み込ませておくと、最新のバージョンに対応できるように育てられるから、この辺りが自分色に染めることが可能となり、個人的なCursorの良いポイントの一つだと思っています


Next.jsのリンクを入れると、勝手に名前やプレフィックスなどの情報が入る

学習して.....

読了。
1分ちょっとで公式ドキュメントをインプットしてくれました


【因みに】
無料版のgpt-4の利用制限を超えると、こんな感じにupdateするかgpt-3.5を使うか聞かれます

全然関係ない、最近推しチームが弱いので答えてくれないだろうと思いつつ聞いてみたら...

ざっくり回答だけど、的を得過ぎてました( )

おわりに
ソースコードレビューやエラー解消など、個人開発向けで利用するのであれば快適に利用できると思います
プロジェクトでの利用はChatGPTはじめ、多くのAIツールとどう向き合って,どこで使っていくかを考え、共有していく必要があると改めて感じました
初投稿で、至らない点があるかと思いましたが、お読み頂きありがとうございました！！
"
https://qiita.com/Takashi_Masumori/items/f0e7142c57762821e3e0,"['転職', 'キャリア']","
はじめに
日系の IT 企業から外資系 IT 企業に転職されるというエンジニアの方も結構いると思います。
私自身は、2017 年に日系 SIer から日本マイクロソフトに転職し、約 6 年半勤め、今後は独立して市民開発者育成支援をしていく予定です。
在籍中は、ありがたいことに常に高い評価をいただき、幸いにも社内での大きな表彰を受け、信じられないほど素晴らしい報奨旅行に行く機会をいただきました。そのため、それなりには会社への貢献ができたと考えています。
今回、自身の過去 6 年半の経験を整理する意味で、あくまで私個人の経験に基づく見解ですが、マイクロソフトで成功し、活躍する人の特徴を整理してみました。
外資系 IT 企業で成功、活躍するにあたって共通する部分も少なからずあると思いますので、現在日系 IT 企業に勤めており、外資系 IT 企業への転職を考えいる方の参考になれば幸いです。


成功の定義は人それぞれかと思いますが、例えば、入社後、自身が思い描いた、場合によってはそれ以上の成長をしたり、キャリアを歩んだり、成果を出したりしている。一生続くような素晴らしい人間関係を構築できている。社会、業界、お客様、同僚に対して大きく貢献をしている。自分軸で生き、幸福度の高い人生を送れているといった感じでしょうか。あくまで結果的にですが、大きな Award を受賞したり、大きな報酬を得たりすることもあると思います。



アピール、自己主張が上手い、バランス感覚に優れている
日系の企業から転職してきた人の中には、アピール、自己主張が弱い人がいる印象があります。
少し厳しいですが、私の紹介で入社した友人の多くもそのような感じというか、「もっとアピール、自己主張した方がいいのでは？」と感じることも少なからずありました。
あくまで主観ですが、恐らく、年功序列色が強く、先輩に過剰に気を遣い、成果をアピールすることや、やりたいことを主張することがあまり肯定的に捉えられない、場合によってはわがままに捉えられてしまう環境で長年働いてきたためと考えます。
いい意味では、従順、献身的とも捉えられるかもしれませんが、結果的に自分のやりたいことを犠牲にしてしまい、キャリアの選択肢も狭めてしまっている印象があります。
また、アピールが弱いため、例え同じようなことをしていても、アピールが上手な人と比較すると評価を受けにくい印象もあります。
そのため、メンタリングする際は、本人が気づいていない強み、本人が思っている以上にインパクトが出ている成果に気づいてもらうよう問いかけるだけでなく、自分がやりたいことをもっと主張してもよいことや、自身の成果をもっとアピールしても良いことを伝えたりもしていました。
逆に、マイクロソフトで成功、活躍する人は、アピール、主張が強すぎて周囲との摩擦が生まれないよう配慮しつつ、自身の成果はしっかりとアピールし、やりたいことはしっかりとマネージャー、リーダー等に上手く主張している印象があります。その結果、チャンスを上手く掴んでいるというか、引き込んでいる気がします。
例えば、私の場合、入社して 3 年弱ほどは Exchange (メール)、Office 365 のセキュリティ周りを担当しており、2020 年頃に Power Platform というローコード、ノーコードプラットフォームのサービスに興味を持ち、当時異動してそれほど期間が経過していませんでしたが、そちらのサービスを担当したいという思いが強くなりました。
そのため、既存の業務をしつつ、自分自身や周囲の人の業務を効率化するアプリを Power Platform で作成し、そのアプリを広報するために説明会を自分自身で企画して何度も実施しました。その結果、Power Platform に関する仕事の相談が来るようになり、最終的に、私が Power Platform を担当することがある意味既成事実化するというか、その当時の組織のターゲットと Power Platform の相性が良かったこともあり、その方が組織のとっても良いといった状態になり、希望通り Power Platform の担当エンジニアになることが出来ました。
それ以外にも、やりたいことが思い浮かんだらマネージャーにすぐに確認し、この程度であれば確認すらせずにやっていいという感覚を掴んだら、事後報告をするなど、やりたいことはどんどん主張、そして成果をアピールするようにしていました。
もちろん、部署異動についても自分主体となります。私の知る限り、日系の企業のように、本人の希望関係なく、年度初めに部署異動が決まる、部署、プロジェクトの都合等で突然辞令が出て部署異動が決まる、場合によっては物理的に住む場所が変わるということはありません。
部署異動する際は、必要に応じて他部門の方に 1on1 を申し込んで情報収集して、自分でオープンポジションを見つけ、社外の方と同じように申し込みをして、経歴書を用意して面談をします。社内転職と呼ぶ人もいます。もちろん、確定した際、現部署に拒否権はありません。
「まだ現在の部署で●年しか経っていないから」、「現在の部署の体制上、人が足りておらず、今異動するのは申し訳ない」ということで異動を躊躇してしまう人もいると思います。
もちろん、義理や人情も大事ですが、マイクロソフトで成功、活躍している人は、自分のやりたいことを踏まえ、周囲と摩擦が生まれないよう上手くバランスを取りつつ、積極的に社内または社外転職をしております。

コネクションづくり、他者を巻き込むのが上手い
マイクロソフトでは、いつ、誰に対しても 1on1 の依頼してもいいですし、誰に対してもメンターになってほしいとお願いしてもいいです。
例えば、インターン生の中には、2 カ月の間に、40 人以上の社員 (US で働いている人や社外的にも有名な方含む) と、自ら積極的に動き、1on1 をした人もいます。
また、必要に応じてマネージャーに確認することもありますが、基本的には、勝手に何か企画をして実行してもいいですし、例えば、他部門の人を巻き込んだ V-Team を立ち上げてもいいです。
マイクロソフトで成功、活躍している人は、呼吸するようにそのようなことをしている印象があり、もちろん、入社してすぐにそのようなことを実践し、どんどん人脈を広げ、他者を巻き込み、短期間で大きな成果を上げている人もいます。
私も、入社後、1 週間も経過していない中で勉強会を企画して実施するだけではなく、他部門のマネージャーの方に 1on1 の依頼をしたり、他部門の方に対してプレゼンを実施したり、新卒の方向けに、ビジネスの基礎、マインドセット、お客様の仕事についてなど、勝手にプレゼンを企画して実施したりしていました。
その後も、多様な V-Team に積極的に関わってきました。以下は、今年参加した V-Team の活動をまとめた記事です。私がこれらのチームに参加するきっかけとなったのは、後に述べるアウトプットです。私のアウトプットを通じて、イベント運営用アプリの開発やチームへの参加を依頼され、喜んで参画しました。結果として、皆が私の独立を凄く応援してくれるなど、素晴らしい出会いとなりました。



個人的に、こちらも、日系の企業から転職してきた人の中には、苦手な人がいる印象があります。例えば、あくまで私の知る限りですが、入社後のオンボーディング期間中、部署外や US で働いている人はもちろん、部署内の人に対してですら、みずから積極的に 1on1 のリクエストをしている人をほどんど見た記憶がありません (マネージャーの指示でオンボーディング中にチームメンバーに対して 1on1 リクエストをするよう指示された場合は除く)。
あくまで主観ですが、組織が縦割りになっており、他部門のマネージャーと 1on1 をすると怪訝な顔をされたり、他部門の方に対してプレゼンをしようとしたり、V-Team を立ち上げようとしたりすると「工数はどうするんだ」と言われたりするといったカルチャーの中で長年働いてきたことが理由と考えます。
その結果、「転職してすぐにそんなことしていい」という発想すら出てこないというか、「恐れ多くてそんなことできない」といったような心理的ブロッカーが働いてしまうものと考えます。
もちろん、外資系の IT 企業といっても、会社、部門によってカルチャーは違うため、最初のうちは念のため確認しながらでもいいですが、少なくとも、マイクロソフトでは、そのようなことを気軽に行っていいということを認識いただいて問題ないというか、むしろ推奨されると個人的に思います。

出し惜しみすることなくアウトプットをしている
マイクロソフトで成功、活躍している人は、社内はもちろん、社外、コミュニティ等に対して、自身のナレッジ、考えなど、アウトプットを積極的にというか、人によっては呼吸するくらい自然に行っている印象があります。
例えば、エンジニアであれば、チーム、部署、本部、組織全体に対して勉強会を開く、外部ブログに記事を書いてナレッジを共有する、作成したアプリをパブリックに公開する、社外、コミュニティのイベントに登壇するような感じです。
アウトプットすることで、自身のナレッジを一般化して、つまり、誰でもできるようにして、新しいチャレンジをしやすくなるというメリットや、結果的に有益な仕事や情報が集まり成長にもつながる、自身の影響範囲が広がるというメリットがあります。
私自身も、直近の 1 年間で 90 以上の Power Platform 関連のブログ記事を書いており、主に市民開発者の方向けに情報発信しております。
結果的に予想以上に沢山の人に見てもらい、相談が沢山来るのはもちろん、自身の知名度も少なからず上がり、会社から評価され、結果的に独立するという選択をすることにもつながりました。
「有償のエンジニアだから」といったような理由で、自身の既存の仕事、テリトリーを守ることに固執し、出し惜しみをし、アウトプットをしていなかったら、間違いなく、独立してやっていける自信は出てこなかったというか、そういった選択肢は生まれてこなかったと思います。

他者貢献のマインドセット
個人的にこちらが一番重要と思います。別の表現を用いると、圧倒的に Giver な人、利他の心、他喜力の強い人でしょうか。
※私は､利他の心という表現が好きなため、本記事ではなるべくそちらの表現を使わせていただきます
有名な稲盛和夫さんが提唱する人生の方程式で言うと、考え方の部分でしょうか。
人生・仕事の結果＝考え方×熱意×能力
当たり前と思う人もいると思いますし､言葉で言うのは簡単ですが、どんなに大変なときも決してぶれずに、見返りを求めず、非常に深い愛情を持ち、他者 (同僚、お客様はもちろん、世の中すべての人)  の人生に貢献することを最優先にして生きていくことは簡単なことではないと私は考えます。
そう思う理由は、ビジネスでは、達成が容易ではない､正論を振りかざすだけではどうにもならない、時には達成不可能で理不尽にすら思えるような非常にハードルの高いターゲットの達成が求められ、その達成のためには、非常に多様な考えを持つ、かつ超多忙な人たちを巻き込み、協力し合う必要があるためです。
ターゲットの達成が困難で、日々上司や先輩からプレッシャーをかけられ、押しつぶされそうになるようなケースもあると思いますし、外資系の企業のため、場合によっては職を失うという不安を感じるケースもあるかもしれません。
そのようなプレッシャーや不安から、利他の心が負けてしまい、お客様に対して少なからず不誠実な言動をとってしまう、自分の思うように動いてくれない、意見が合わない同僚に対して不誠実な言動をとってしまう、場合によっては厳しい言葉を浴びせてしまう、最悪、人格含め人を裁いてしまうようなこともあるかもしれません。
そのような言動を取った際においても、一時的にターゲットを達成し、表面上は成功したように見えることもあるかもしれませんが、周囲の人からは表面的な賞賛しか浴びず、場合によっては嫉妬されるなど、心の底から人とつながることはできず、人間関係も希薄なものになると思います。
先ほどの方程式ですと、一見ターゲットを達成したためプラスに感じるかもしれませんが、考え方がマイナスであるため、特に、同僚、お客様、世の中全体という範囲で考えると、実はマイナスの結果になってしまう場合もあると考えます。
言うは易しですが、そのようなマイナスの結果にならないよう、利他の心が強い人は、常にその心を磨き、自分と深く､時には厳しく向き合い、例えば、「自らの成果に固執するあまり、お客様に不誠実な対応を取っていないか」、「過去の成功体験にとらわれていないか」、「単に正論を振りかざすだけになっていないか」、「同僚に対して不誠実な対応を取っていないか、裁いてしまっていないか」、「自分の利益ではなく、相手の利益を優先した言動を取れていたか、決断が出来ていたか」など、日々の言動を省みるような習慣を少なからず持っていると感じます。
そして、そのような習慣を持っていることもあり、厳しいターゲットが課せられておりプレッシャーや不安を少なからず感じている中でも、あくまでも、お客様の成功や他者に貢献することをファーストプライオリティにおき、常に他者に対して誠実で、倫理的で、また、エネルギーに満ちあふれており、情熱的で､探究心が深く､向上心が高く、それでいて謙虚で人への感謝を惜しまず、良い意味で他者を巻き込み、魅了し、結果的にターゲットの達成にも大きく貢献すると思います。
もちろん、達成した素晴らしい成果に対して､周囲の人から心の底からの賞賛を浴び、人と広く、そして深くつながっていくと思います。結果的に、マイクロソフトで働きつづけるとしても、卒業して新しいチャレンジをするとしても、一生続くような人脈を形成していくと考えます。
先ほどの方程式ですと、個人の範囲はもちろん、同僚、お客様世の中全体という範囲で考えてもプラスの結果になると考えます。
あくまで私の個人的な考えですが、そのような人がマイクロソフトで成功、活躍し、またその後も成功、活躍している印象があります。

まとめ
他にも、チャレンジ精神、成長意欲、テクノロジーに対する探究心、ダイバーシティ＆インクルージョンなど挙色々ありますが、以上が、私が 6 年半働いてきた中で、マイクロソフトで成功、活躍していると感じた人の特徴です。
あくまで個人的な経験に基づく主観ですが、今後、日系 IT 企業から外資系 IT 企業への転職を考えている方、転職したばかりの方、転職後、思ったような活躍が出来ずに悩んでいる方の参考になれば幸いです。
"
https://qiita.com/tanvir-hasan/items/b5cd5903d23f0c10ad55,"['Go', 'tech']","
Golang： 並行処理で何百万ものデータを簡単にフェッチする

紹介
ソフトウェア・エンジニアとして、私たちはより少ない労力とメンテナンスで非常に強力なシステムを提供することを求められている。バックエンドの皆さんは、docやCSV、APIから何百万ものデータをフェッチするタスクがあるかもしれません。
巨大なデータをフェッチする際の問題は以下の通りです：

パフォーマンス
リソースの利用
データのフィルタリングとソート


ケーススタディ
例えば、商品データを取得するためにパートナーのAPIと統合する必要があり、10分ごとに新鮮なデータを取得し、データが変更された場合はDBにアップサートする必要があるとします。
データのフェッチにいくつかの同時実行を使用することをお勧めします。データの異なるバッチで5つの同時実行があるとします。そうすると

スレッド1は1-100000のデータを取得するために実行されます。
スレッド2が100001-200000のデータをフェッチする。
別のスレッドで続行


並行処理とは
並行処理とは、ハードウェアとソフトウェアに応じて、コンピュータが複数のタスクを同時に実行する能力のことである。

実装
Golangでは、sync.WaitGroupを使って複数のプロセス（goroutine）を実行し、データの取得を処理することができます。
sync.WaitGroupはgoroutineの待機に使われる。実装はとても簡単で、Add()を使って実行したいゴルーチンの数を設定できる。
package main

import ""sync""
import ""runtime""
import ""fmt""

var LIST_PRODUCT_TYPE = [3]string{""food"", ""electronics"", ""clothing""}

type GetListProductResponse struct {
    Data []ProductListResponse `json:""data""`
}

type ProductListResponse struct {
    Code         string `json:""code""`
    Name         string `json:""name""`
    Price        string `json:""price""`
    Status       bool   `json:""status""`
}

func getProducts(ctx context.Context, req *GetProductListRequest) (*GetListProductResponse, error) {
    // calling endpoint 3rd party
    // parse to response
    // and return the data
    return &productList, nil
}

func main() {
    wg := sync.WaitGroup{}
    doneChan := make(chan interface{}, 1) //シグナル待ち用
    productsChan := make(chan *GetListProductResponse) //実行するすべてのゴルーチンからのプロダクト・データを共有するためのチャンネルです。
    errChan := make(chan error)  //各ゴルーチンのデータ取得時のエラーを共有するためのチャンネル

//このループは商品データを同時に取得する
    for key := range LIST_PRODUCT_TYPE {
        wg.Add(1)
        req := &GetProductListRequest{
            ProductType: LIST_PRODUCT_TYPE[key],
        }
        go func() {
            defer wg.Done()
            products, err := getProductList(ctx, req)
            if err != nil {
                errChan <- err
                return
            }
            productsChan <- products
        }()
     }

     
//このコードは、sync.WaitGroup (wg)が完了するのを待ち、
//nil値をチャネル(doneChan)に送信してタスクが完了した
//ことを知らせる新しいgoroutineを開始している。

     go func() {
         wg.Wait()

         doneChan <- nil
     }()

     var (
         catalogues GetListProductResponse
         data       []ProductListResponse
     )

 // このforループは無限に実行され、その中にselect文があり、これは複数のチャンネルを同時に処理するために使われる。
     for {
         select {
         case <-ctx.Done():
             return nil, ctx.Err()
         case err := <-errChan:
             return nil, err
         case products := <-productsChan:
             data = append(data, products.Data...)
             catalogues.Data = data
         case <-doneChan:
         return &catalogues, nil
     }
   }
}


これがGolangでデータを同時に取得する例です。
"
https://qiita.com/triple321jhango/items/55ba631ea28463827cd3,"['AWS', 'IaC', 'Terraform', 'エンジニアのキャリア', 'AWS認定試験']","
はじめに
結論から申し上げますと、2024年1月よりAWS設計構築案件にアサインが決まりました。
簡単に私の自己紹介をしますと、妻子持ちで30代実務未経験からSES企業へ転職し現在5年目。
過去ローなスキルでずっとさまよっており、インフラのオンプレやクラウドの設計構築案件は未経験。
ですが、やっとの思いでAWSの設計構築案件にアサインすることができたので、2023年にやってきたことについてまとめていきたいと思います。
この記事はAWS設計構築案件にアサインしたいと思っている人に向けた記事です。
必ずしもこの記事を読んだからAWS設計構築案件にアサインできるかどうかと言われると、それは違うと思っています。
が、参考までにどんなことをやってきたのか？ということを共有することはできるかと思いますので、今回記事を書かせていただきました。

目次
1.はじめに
2.資格の取得
3.技術記事執筆
4.AWSハンズオン本の執筆
5.インフラのIaC化
6.最後に

資格の取得

現在の資格



認定日
認定資格




2022.04.30
AWS Certified Solutions Architect - Associate (SAA)


2023.05.13
LinuC-1


2023.10.21
AWS Certified Solutions Architect - Professional (SAP)



2023年4月までの私の資格はAWS Certified Solutions Architect - Associate (SAA 2022年4月取得)のみでした。
ただ、これだけでは弱いと感じX (旧Twitter)やYoutubeなどから情報収集し、どうしたら実務未経験でも企業にアピールできるか？を考え、インフラの分野はポートフォリオよりも資格を評価する企業が多いことを聞き、まずは見た目でもわかりやすい資格でアピールする方針としました。

LinuCについて
AWSを学習する上で必須となる知識としてサーバの知識は必須になってきます。
Linuxの基本的なコマンドは一通り触れます、とアピールできるようにLinuC Lv1を取得して網羅的にLinuxを学ぶことにしました。

学習コンテンツ

Ping-t
Linux教科書 LinuC レベル1 スピードマスター問題集 Version10.0対応

Ping-tに関しては有名なコンテンツかと思いますが、LinuCは101および102の二つの試験をパスして初めてLinuC Lv1の資格として認められますが、LinuC101の問題についてはPing-tに登録することで無料で最強Web問題集600問以上の問題を解くことができます。


102については別途有料会員として登録する必要があります。
単月ごとに利用期間を選べるので、例えば1ヶ月で受かる自身がある、という方は1ヶ月料金を支払って有料会員になるなどするとよいかと思います。
【ご利用期間と料金】
1ヶ月・・・2,640円（税込み）
2ヶ月・・・3,300円（税込み）
3ヶ月・・・3,960円（税込み）
6ヶ月・・・4,950円（税込み）
12ヶ月・・・6,930円（税込み）
24ヶ月・・・9,900円（税込み）
36ヶ月・・・12,870円（税込み）


私の場合はLinuCの取得までに半年程度費やしました。
1日に学習できる時間も限られているというもあり、長期スパンで考え日々コツコツ学習しました。
2022年12月の中旬から学習し、101を3月に取得、102を5月に取得しました。

AWS SAPについて
次にSAPです。
某転職エージェントさんがしきりに話していたことなのですが、実務未経験でAWS設計構築案件にアサインしたいと思うなら、Associateレベルの資格では評価されないので、Professionalの資格を取得して差別化しないと評価に値しないと聞き、そこからSAP取得に向け学習することにしました。
SAP取得までに費やした学習時間はおおよそ2ヶ月半です。

学習コンテンツ









問題集についてはCloudTech一択でした。
私はCloudTechの有料会員 (永久ライセンス)となっていますので、300問あるSAPの資格演習と模擬試験を繰り返しこなす、という勉強を続けました。
CloudTechの資格演習の良いところは、300問すべてに構成図付きの解答解説が付いています。
そのため、文字列だけでは理解が難しいところを視覚化して理解し易いように解説されていますので、非常に勉強効率が上がります。
SAP以外のAWS資格問題集も充実していますので、一度リンクをたどって確認してみると良いかと思います。
資格演習以外にもハンズオン動画が豊富にありますので、問題集だけでは理解できない内容をハンズオンで補足するといったことも可能です。
AWS資格学習やハンズオン学習でどの教材を使用したら良いか悩んでいる方がいらっしゃいましたら、CloudTechを検討してみると良いかと思います。
割引クーポンありますのでご利用ください。


割引クーポン: kyrieee2304


その他有名な技術記事としてDevelopsersIoやAWS Black Beltオンラインセミナーなどはスキマ時間を活用し、理解が浅いサービスについてのインプットとして活用しました。

技術記事執筆
技術記事執筆についてはアピール材料とする目的や自分自身がAWSの各サービスを深く理解するための手段して自身でブログを立ち上げ執筆していきました。



こちらも某転職エージェントさんから聞いた話ですが、自身で技術記事などを書いてアピールしている人は実はほとんどいないので、それだけで差別化できます、と言われました。
技術記事は以前から書いていましたが、案件面談などでアピールできるポイントは資格だけじゃないですよ、ちゃんと手も動かせる人材ですよ、をわかってもらうためにハンズオンを中心とした技術記事を少しずつ執筆していきました。

AWSハンズオン本の執筆
こちらはCloudTechコミュニティ内で発足したプロジェクトで、AWSのハンズオンに特化したKindle本を出版しようということで、コミュニティ内でメンバーを募集しハンズオンの本を作っていくプロジェクトに参加しました。
出版されたKindle本で部分的な執筆や図解の作成などに携わったKindle本は以下です。


CodeシリーズでハッピーなCI/CDを構築しよう【AWSハンズオン虎の巻シリーズ】: 知識がなくても手を動かしながらCI/CDをAWS上に実装できる一冊！


AWSで実践するEC2の脆弱性対応: 脆弱性ってなんですか？でも大丈夫！EC2の脆弱性対応を知識0からEC2脆弱性をハンズオン対応


書籍の執筆はGoogleドキュメントやMarkdownを使用してドキュメント作成をします。
Markdownで執筆する場合にはGitHubで管理し、PullRequestをすることでメイン執筆者からレビューをもらい、指摘修正、最終的にマージされるという実際のGitHubフローで行われたため、かなり勉強になりました。
このKindle本の出版に部分的に携わったとは言え、案件面談の際には100%「この本の執筆経験について教えていただけますか？」と質問されました。
実務経験はないですが、こういった業務外活動についてちゃんと目を通してくれているということについてはかなりプラスに働いたのではないかと思っております。

インフラのIaC化
次にTerraformを使ったインフラのIaC化を学習しました。
Webの一般的な構成ですが、以下のような
CloudFront+ALB+EC2の構成で独自ドメインで冗長化されたEC2インスタンスにアクセスする構成をTerraformを使用してコード化しました。

GitHubにコードをアップしています。



まだまだ初歩的な内容しか理解できておらず、コードのリファクタリングなどはできていませんが、Terraformでコード化してみると、いろいろな学びがあり、またapplyに成功したときは非常にうれしい気持ちになれました。

最後に
ここまで読んでいただきありがとうございます。
実務未経験でAWS設計構築の案件にアサインするためにはどうするべきか？を色々考えて行動してきました。
やり方はそれぞれあるかと思いますが、最終的に一番強いと思っているのはどれだけ毎日継続できるか？かと思っています。
1日1歩ずつでも前に進めるように日々学習を継続することで目標に近づける可能性が上がるのではないでしょうか。
"
https://qiita.com/k2-kanda/items/42e7df9a578495a5a215,"['TypeScript', 'SPA', 'Vue.js', 'フロントエンド', 'Vue3']","
はじめに
僕は仕事でもプライベートでもVueを一番触っています。フロントエンドの経験が無いときに「何かWebアプリ作ってみるか」と思い立って最初に手をつけたのがVueで、それ以来気に入って使っています。
Vueに限らず、フロントエンド（特にSPA界隈）の技術の発展は凄まじく、便利なツール・ライブラリで溢れています。今回は、「もしこれからVueで何かを作ろう」と思っている人向けに、 とりあえずこれを入れておけばいいよ 、というツールチェインを紹介したいと思います。


あくまで筆者が触ってみたことがあるものから選択しています。フロントエンドは技術の陳腐化も早いのでその点ご承知ください。



前提

Vueのバージョンは3系
さすがに今は3系の時代です。
Vueでわからないことをググると未だに2系の情報が出てきてしまうのがVueの辛いところですが、Composition APIの書き味が素晴らしいので3系一択です。（Options API？そんな書き方ありましたね）

TypeScript
JavaScriptに慣れた状態でTypeScriptを書き始めると「一生TSC（コンパイラ）に怒られるしやってらんねー！」となりますが、動かしてからコケるよりも書いているときにコケる方がマシです。ちゃんと<script setup lang=""ts"">で書きましょう。

開発環境系

Vite



言わずとしれた超高速ビルドツールです。Vueに限らず様々なフレームワークに対応しています。


読み方は「ヴィート」のようです（「バイト」ではない）


開発時はHMRに対応しているため、ソースコードを編集すると即座に反映されるのが気持ちいいです。もうやめられません。
従来はVueのプロジェクトを作成するときはVue CLIというツールがメジャーでしたが、Viteが登場してからはこっちが普及したため、Vue CLIはメンテナンスモードになってしまいました。



一方、create-vueという公式ツールが登場しました。こちらはViteをベースにしており、よりVueプロジェクトに適したテンプレートでプロジェクトを作成してくれます。そのため、今後Vue開発を始める際はこちらの create-vueを使用することを強く推奨 します。





このページで紹介するツールのいくつかは、このcreate-vueで作成するときに質問形式で選択することでインストールできます。（下記参照）

ESLint
Prettier
Vue Router
Pinia
Vitest




ESLint



ソースコードに対する構文解析を行ってくれるツールです。
プログラミングを始めた当初はこういった品質向上系のツールの必要性がいまいち感じられませんでしたが、潜在的なバグを防いだり、コードの可読性を高めたりする上で非常に強力なので、今では必須ツールです。
ESLint自体はJavascriptに対する解析ツールなので、例えばTypeScriptの構文やVueの構文に対応しようと思うとそれらに対応したルールを入れる必要があります。このあたりはViteやcreate-vueでよしなにやってくれるのでそれに任せましょう。（もちろん、自分でカスタマイズもできます）
基本的にはコマンドを打つことで指摘事項の一覧化ができますし、物によってはESLintが自動で直してくれるものもあります。VSCodeやWebStormなどのIDEの機能を使ってコーディングの段階で指摘・修正をしておくことがよいでしょう。
また、後述のツールを使ってGitコミット時に自動化したり、CIパイプラインに組み込んでおくことを推奨します。

Prettier



フォーマッティングを行ってくれるツールです。
最初はESLintとは何が違うんだ？と思っていましたが、こちらはあくまで見た目を綺麗にすることが目的です。
これも使うまでは必要性が感じられませんでしたが、

改行の位置
演算子前後のスペース
カンマの自動挿入

などなどが自動で行われるのを体験して、めでたく必須ツールの仲間入りをしました。
可読性ももちろんのこと、複数人で開発するときにGitの差分が生まれにくいのも大きなメリットです。
フォーマットのルールを設定ファイルとして定義できるので、それもGit管理下にしておけば、チーム全体でコーディングルールを統一できます。

simple-git-hooks



Git hooksに応じてスクリプトを実行することができるようになります。



導入のメリットとしては、Gitコミット時に前述のESLintやPrettierによる指摘・フォーマットを自動化できるところになります。コマンドを手打ちするのってどうしても億劫だし忘れがちになるので。自動化バンザイ！

lint-staged



Gitのステージング対象のファイルに対して任意のコマンドを実行できるようになります。
前述のsimple-git-hooksと何が違うんだ、という話ですが、simple-git-hooksはあくまでGitのコミット時やプッシュ時というタイミングに応じて発火するだけであり、コマンドの対象ファイルを絞るのはまた別の話です。例えばフォーマットに関して言えば、「変更対象のファイルにのみ」実行したいとなると、「変更対象のファイル」を抜き出す必要があります。lint-stagedを使用すると、不要なファイルに対してコマンドを実行することを防ぐことができます。

Vueのエコシステム系

Vue Router（Vue限定）



ルーティング処理を担ってくれるライブラリです。
そもそもSPAのSはSingleのSなので、厳密に言えば「ルーティング処理」というものは存在しないのでは？という感じもしなくはないです。
・・・が、現実としてSPAのSingleは「サーバ側がホストするindex.htmlが一つ」という意味のSingleとなっています。というわけで、SPAといいながらも複数の画面を用意しておきたいアプリがほとんどになるのではないでしょうか。
Vue Routerは、（従来のMPAよろしく）URLと表示する画面とをマッピングしてくれます。マッピングだけでなく、

クエリパラメータ
パスパラメータ
画面遷移
ナビゲーションガード

などの機能も提供してくれるので、ペラ一枚のアプリでも無い限りは入れておいたほうが無難です。


ナビゲーションガードとは、画面遷移のたびに実行される前処理（後処理も書けます）を指します。
例えば特定の画面は認証済みでないと表示できない、といったときに使用します。
非常に便利なのでぜひ使いこなせるようになっておきましょう！
Navigation Guards（VueRouterドキュメント）



Pinia（Vue限定）



SPAでは状態（ステート）は基本的にクライアントサイドで管理します。
VueではProps Down/Emit Upの原則に則ることで、親子関係のデータの伝搬を実現します。しかし、この方法では水平方向のコンポーネント間での情報のやり取りをスムーズに行うことができません。（例：ある画面の入力項目を別の画面で参照したい、など）
もちろん、共通的な親コンポーネントまで遡ってから目的のコンポーネントまでさらにバケツリレーを行うことも考えられなくないですが、こういうケースでは状態管理ライブラリを使用することが一般的です。
Vueに状態管理ライブラリとして広く普及しているのがこちらのPiniaになります。
ストアの定義も参照もシンプルなので、癖がなく使えることがメリットです。
Piniaが登場したときはOption Storeという記法のみがサポートされていましたが、現在はscript setupと同等の書き味ができるSetup Storeという記法もサポートされています。後者の方がより直感的に書けるので、僕は基本的にSetup Storeで記述します。
Option Store
export const useCounterStore = defineStore('counter', {
  state: () => ({ count: 0, name: 'Eduardo' }),
  getters: {
    doubleCount: (state) => state.count * 2,
  },
  actions: {
    increment() {
      this.count++
    },
  },
})

Setup Store
export const useCounterStore = defineStore('counter', () => {
  const count = ref(0)
  const name = ref('Eduardo')
  const doubleCount = computed(() => count.value * 2)
  function increment() {
    count.value++
  }

  return { count, name, doubleCount, increment }
})


バリデーション系

zod



TypeScript界隈でとりあえず使っとけ（筆者調べ）と言われるバリデーションライブラリです。
入力フォームで

名前はアルファベットのみ
年齢は20から60の間
郵便番号は数値7桁
「メールマガジンを希望する」にチェックをいれた人だけメールアドレスは必須

のような単項目・相関チェックの両方を宣言的に定義し、バリデーションを行うことができます。
ここでいう「宣言的」とは、各項目が満たすべき条件を簡潔に分かりやすく書ける、という意味で使っています。
開発者が書きたいのは「項目が満たすべきルール」であって、「そのルール通りかどうかを検証するロジック」では無いはずです！
const User = z.object({
  username: z.string(),
  age: z.number().min(20).max(60),
});

このようにルールを直感的に書けるうれしさを感じられるのがzodです。

VeeValidate（Vue限定）



zodはあくまでJS/TSの世界で利用されるバリデーションライブラリで、実際にVueの世界で使うとなると接続するライブラリがあると便利です。
VeeValidateは、（zodに限らず）フォームに入力されたデータをバリデーションルールに基づいて評価し、そのフィードバックを提供してくれるライブラリです。
エラーメッセージをリアクティブに返してくれるので、Vueのデータバインディングと組み合わせることでフォーム部品のコーディングが効率化されます。
const { handleSubmit } = useForm({validationSchema: toTypedSchema(User)});

const { value, errorMessage } = useField('username');

const onSubmit = handleSubmit((values) => console.log(values));

上記のように書くと、


valueにはユーザ名が入る
ユーザ名に対するバリデーションエラーメッセージはerrorMessageに格納される

onSubmitを実行すると、「バリデーションがすべてpassしたときのみ」コンソールにフォームの内容が出力される

といったことを実現できます。
特徴として、useFormしたスキーマの一つ一つをuseFieldで参照するのですが、useForm時の効果は子コンポーネントにまで及びます。すなわち、

親コンポーネントでuseForm

子コンポーネント（フォーム部品など）でuseField


という風に責務をわけることで、子コンポーネント側の汎用性を上げることが可能です。
このあたりは公式ドキュメントのベストプラクティスにも掲載されているので参照ください。




テスト系

Vitest



Vite環境におけるテストツールです。名前の通り、Viteと非常に相性がいいです。（というかViteのコアメンバーが作っている）
正直他のテストツールを試さずにこのツールを使用しているのでこれ以外の選択肢を知らないのですが、Viteにネイティブに対応しているだけあって、導入の容易さと実行速度に不満は全くありません。
テストコード自体はJestと互換性を持たせているため、Jestに慣れている人ならすぐ書き始められると思います。もし「こういうことがしたいんだけどな〜」ということがあっても、Jestは超メジャーなのでググればすぐに解決方法が得られるところもGoodです。

Testing Library



SPAはコンポーネント指向であることが多いため、単体テストもコンポーネント単位で行いところです。
ただ、従来のテストツールだけではDOMも絡めた実際の操作に則したテストが行いづらいところがありました。
Testing Libraryを使えば、コンポーネントが何を描画したか、どのようなイベント（クリックイベント、キーイベント）に対して反応したかを検証することができます。
Testing Libraryが優秀なので、単体テストといいながらも、ある程度の大きさのインテグレーションテストまでVitest+Testing Libraryで済ませてしまうことが可能です。
とあるプロジェクトでは、入力フォーム画面という単位で、その画面のフォームにさまざまな値を入力し、バリデーションエラーメッセージの表示内容や、バリデーションエラーがある場合は次の画面に進むボタンが非活性になることなどを検証していました。
ReactにおけるTesting Libraryを使ったテストプラクティスは、同じアドベントカレンダーの下記記事にもまとめられておりますので、ぜひご覧ください。




MSW



SPAから外部のAPIサーバにリクエストを送る処理を書くことはしょっちゅうありますが、そのAPIサーバをモック化したい場合に便利なのがこちらのライブラリです。
実際のAPIサーバのレスポンスを使うのはもっと後の工程として、開発中はいろいろなバリエーションのレスポンスに対して動作を確認したいはずなので、APIサーバ（というかレスポンス）をモック化することはよくあると思います。
ただ、APIサーバそのものをモック化するのは骨が折れますし、SPA側のInfra層をモック化するとなるとDIなどの仕組み作りが必要になります。
MSWを使えば、ブラウザのサービスワーカーを使ってHTTPリクエストを監視してインターセプトするので、SPAからは本当に外部のAPIサーバにリクエストを送信しているように書くことができます。
一方で、どのようなリクエストに対してどのようなレスポンスを返すか、といった定義は同じプロジェクトのソースコード内に記述できるので、メンテもしやすく開発者間での共有も容易です。
とりあえず書いてみたコードの動作を確認したいときやフィージビリティ検証にも有用なので、とりあえず入れておけば間違いないライブラリになっています。
ちなみに、ブラウザ上だけでなくNodeサーバ上でも動作します。そのため、Nodeアプリケーションから外部サーバへのリクエストに対するモックレスポンスも作成できます。

まとめ
執筆開始時はcreate-vueで何を入れてくれるのかをよく知らなかったのですが（自分はViteのテンプレートから書くことが多かった）、ここで紹介する予定だったライブラリのほとんどを入れてくれるらしく、あまりこの記事の意義が無くなってしまった気もします。ぐぬぬ・・・
とはいえ、ここで書いたものは本当にいろいろなプロダクトにそのまま使えるものなので、まだ使ったことがないものがある場合は、ぜひ試してみてください。
"
https://qiita.com/wayama_ryousuke/items/105a164e5c80c150caf1,"['ベンチマーク', 'LLM', 'ELYZA-tasks-100']","
TL;DR

14個の「日本語が話せるLLM」（日本製・外国製問わず）の性能を、日本語データセット ELYZA-tasks-100 で横断評価しました
海外勢70Bモデルの性能が高いことがわかりました。7Bでは ELYZA-japanese-llama-2 や CALM2 の成績がよかったです
モデルの回答・スクリプトへのリンクは記事内に貼っています


JGLUE などのベンチマークは、モデルの性能を反映しているのか？
2023年は、かつてないほど多くの LLM が（クローズド・パブリック問わず）公開され、まさに LLM フィーバーの年でした。
一方で、どのモデルが高性能なのか、自分の利用用途に合ったモデルはどれなのか、とお悩みの方も多いのではないでしょうか。
LLM の性能を比較するときに役立つのが、ベンチマークです。
英語圏の LLM に関していえば、MMLU や HellaSwag、WinoGrande などが広く使われており、その結果は Open LLM Leaderboard で比較できます。
日本語のベンチマーク用データセットとしては、Yahoo! JAPAN が2022年12月に公開した JGLUE が広く利用されています。
このデータセットは、言い換え（JSTS）、常識問題（JCommonsenseQA）、推論（JNLI）などのタスク別データセットを統合したものです。
JGLUE に含まれる設問の例を見てみましょう。


JNLI
前提と仮説の関係を含意、矛盾、中立の中から回答してください。

制約:
- 前提から仮説が、論理的知識や常識的知識を用いて導出可能である場合は含意と出力
- 前提と仮説が両立しえない場合は矛盾と出力
- そのいずれでもない場合は中立と出力

前提: 誰も居ない学校の教室に机が並べられています。
仮説: 生徒が一人だけの学校の教室に机が並べられています。
関係: 


JCommonsenseQA
[問題]に対する[答え]を[選択肢]の中から選んでください。

[問題]:会社の最高責任者を何というか？
[選択肢]:[社長, 教師, 部長, バイト, 部下]
[答え]:

出典: shunk031/JGLUE, プロンプトは Stability-AI/lm-evaluation-harness による (JNLI, JCommonsenseQA)

この例からわかるように、それぞれの質問は答えが決まっており (closed-ended questions)、正誤の判定を機械的に行うことができます。
その結果をもとに、accuracy やマクロF1などの統計量を算出して、成績を比較する...というベンチマークになっていいます。
このデータセットは日本語LLMの評価に広く使われており、モデル公開時のプレスリリースでも、JGLUE のスコアを掲載している場合が多いです。
一方で、公開されている LLM を使ってみると、ベンチマークで示されたスコアが人間の肌感覚と一致しないのではないか、と思う場面があります。
例えば、JGLUE データセットで高得点をマークしたモデル（stabilityai/japanese-stablelm-instruct-beta-7b）と、それよりもスコアが低い別のモデル（elyza/ELYZA-japanese-Llama-2-7b-instruct）に同じプロンプトを与えてみましょう。
JGLUE のスコアとは逆に、elyza/ELYZA-japanese-Llama-2-7b-instruct の回答のほうがよさそうに感じます。


質問
フルマラソンにチャレンジしたいです。脚力を鍛える方法を教えてください。

stabilityai/japanese-stablelm-instruct-beta-7b の回答 (JGLUE平均スコア: 58.38)
やあああああああああああああああああ

elyza/ELYZA-japanese-Llama-2-7b-instruct の回答 (JGLUE平均スコア: 56.61)
フルマラソンにチャレンジするためには、脚力を鍛える必要があります。脚力を鍛えるためには、走る距離とスピードを意識して変化させる必要があります。以下のようなトレーニング方法がおすすめです。


ジョギングからはじめて、徐々に距離とスピードを伸ばしていく。


10kmを切るような距離のトレイルランニングに取り組む。


ウルトラマラソンに参加する。


これらのトレーニング方法はあくまで一例です。トレーニング方法は個人のランニングスタイルや目標に合わせて選ぶことが大切です。
JGLUE スコア (9タスク平均) の出典: StabilityAI (https://ja.stability.ai/blog/japanese-stable-lm-beta)

このように、人間の感覚とベンチマークスコアの乖離が生じる原因として、JGLUE データセットはクローズエンドの（答えが厳密に決まっている）質問だけで構成されている点が挙げられます。
チャットなどの質問応答・指示追従タスクでLLMを利用する場合、モデルに与えられる指示・質問は、JGLUE のように答えが1つに決まっている質問ばかりではありません。
こうしたタスクでは、あいまいな指示や、答えが決まっていない質問（open-ended questions）に対しても適切な回答を生成することが求められます。
JGLUE のような「答えが決まっているデータセット」を使って、モデルの性能を評価することは、実際の利用シーンでユーザが感じる使いやすさを反映しているとは言い切れないと思います。
こうした問題意識をふまえ、英語圏を中心に、オープンエンドの質問を使ってモデルを評価しよう、という動きがあります。
Alpaca-EVAL や MT-Bench などが、その代表例です。
これらのベンチマークでモデルを評価する場合、回答が正しいか否かを機械的に判定できないため、人間あるいは性能のよい LLM（GPT-4 など）によって、モデルの性能を評価します。
詳細な評価方法はベンチマークごとに異なりますが、回答を5段階評価などで採点する方法（絶対評価）か、2つの回答を並べてどちらがよいか判定する方法（相対評価）に分類できます。
日本語でも、次のようなオープンエンド質問のベンチマークが開発されています。

ELYZA-tasks-100

Japanese MT-Bench（MT-Benchの日本語訳）
Rakuda

これらのベンチマークを用いたモデルの比較記事として、ELYZA のブログ記事 (ELYZA-tasks-100) や shi3z さんの記事 (MT-bench) があります。しかし、オープンエンド質問で構成されるデータセットの成績は JGLUE などのスコアに比べて測定例が少なく、とくに最近のモデルや海外発のモデルについては、評価結果がインターネット上に公開されていない場合も多いです。
そこで今回は、オープンエンド質問データセットのひとつであるELYZA-tasks-100 を用いて、各種 LLM の日本語タスクでの性能を横断的に評価しました。

データセット

ELYZA-tasks-100
ELYZA-tasks-100 は、ELYZA が開発した日本語の LLM の性能を評価するためのデータセットです。
開発元 ELYZA によるデータセットの解説を引用します。

本データセットはinstruction-tuningを行ったモデルの評価用データセットです。[...]
特徴:

複雑な指示・タスクを含む100件の日本語データです。
役に立つAIアシスタントとして、丁寧な出力が求められます。
全てのデータに対して評価観点がアノテーションされており、評価の揺らぎを抑えることが期待されます。

具体的には以下のようなタスクを含みます。

要約を修正し、修正箇所を説明するタスク
具体的なエピソードから抽象的な教訓を述べるタスク
ユーザーの意図を汲み役に立つAIアシスタントとして振る舞うタスク
場合分けを必要とする複雑な算数のタスク
未知の言語からパターンを抽出し日本語訳する高度な推論を必要とするタスク
複数の指示を踏まえた上でyoutubeの対話を生成するタスク
架空の生き物や熟語に関する生成・大喜利などの想像力が求められるタスク

出典: elyza/elyza-tasks-100 (HuggingFace)

実際のデータは、指示・正解例・評価観点の3つの要素から構成されています。
データの例（設問 No.11）を見てみましょう。


指示
以下の製品について、Facebookで流す広告を書いてください。子供のいる親をターゲットにして、キャッチーにしてください。
【製品概要】
子供のためのGPSトラッカー。音声通話ができる。

正解例
トークできるGPS ‍新登場！
遊んでいる子供の様子が心配なこと、ありませんか？💦
このGPSトラッカーなら、子供の位置がすぐ分かります！
しかも、通話で声が聞けるから安心😆
#喋れるGPS #キッズGPS

評価観点

概要に書いていない機能（e.g. ジオフェンシング機能）などが勝手に追加されている場合: -1点
製品概要の内容が抜けていたら-2点
キャッチーではないスタイルの場合: -1点


このように、指示ごとに模範解答や採点基準が与えられており、回答の正誤判定の参考とすることができます。
また、引用した評価観点とは別に、基本的な採点基準（指示に従えていなければ1点、方向性が合っていれば2点、...など）も示されていいます。
このため、評価時は「基本的な採点基準」「問題別の評価観点」の2つを参照して、回答を評価します。

LLMベンチマークのコンタミネーション問題
評価データセットの作成にあたっては、コンタミネーションに注意する必要があります(参照: Kaggle)。
コンタミネーションがあると、学習時に覚えたデータを利用してLLMが回答を生成できる可能性があり、モデルの性能を正しく評価できなくなってしまいます。
モデルの真の性能を評価するためには、学習データセットに含まれないデータを使ってベンチマークをとる必要があるのです。
ここで問題になるのは、LLM の開発者が学習データセットを公開していない場合が多く、学習データセットのテキストを特定することが難しい、という点です。
LLMの学習方法などがどの程度公開されているかまとめている Opening up ChatGPT を見ると、大半のモデルが学習データセット (""LLM data"" や ""RL data"") を公開していないことがわかります。
このような事情から、今回の評価で利用した ELYZA-tasks-100 に含まれるデータが、評価対象モデルの学習データセットに含まれてしまっている可能性は否定できません。
LLM のコンタミネーションに関する問題は、公表されているベンチマークの信頼性にも関わることから、議論の対象となっています。
LLM のベンチマーク成績をまとめたリーダーボードについて、一部のモデルがコンタミネーションによって高いスコアを出しているとの指摘があるほか (参考)、ベンチマークの信頼性を横断調査した研究 ではコンタミネーションがベンチマークスコアに大きな影響を与えることが報告されています。
モデルの回答から各種ベンチマークのコンタミネーションを検出する試み もありますが、この問題への対策はまだ発展途上の段階です。
コンタミネーションの問題は、非公開の評価用データセットを用いることで回避できますが、そうすると第三者がベンチマークスコアを検証できなくなってしまいます。
評価用のデータを新規作成してベンチマークを行いつつ、データセットを公開するという方法もありますが、これでは評価のたびにデータセットを作成しなおさなければならず、作業負担の面で現実的ではありません。
（学習時のデータセットが公開されればこのような問題はないのですが、CV分野における ImageNet のようにデファクトスタンダードとなっている公開大規模学習データがあまりなく、現状では難しいでしょう...。）
この問題は、データセットが公開されているすべてのベンチマークに共通する課題です。
今回ご紹介する ELYZA-tasks-100 での評価結果についても、コンタミネーションによるスコアの上振れが生じているかもしれない、という点にご留意ください。

評価手法

GPT-4 による自動評価
今回は多数のモデルを短時間・低コストで評価するため、GPT-4 による自動評価を行いました。
評価対象の LLM による回答を、設問や評価基準とあわせて GPT-4 に入力して5段階評価を行い、その平均値をモデルの最終スコアとしました。
このスキームで評価する場合、GPT-4 による評価が人間の評価と一致するのか、という点が気になります。
これについては、MT-Bench や ELYZA-tasks-100 の開発元が妥当性の検証を行っています。
MT-Bench の論文では、GPT-4 による評価結果を人間（専門家およびクラウドソース）と比較していますが、GPT-4 と人間の評価の一致率は80%以上であり、人間どうしの評価の一致率と同程度であると報告しています。
また ELYZA の分析では、人間の評価どうしの相関には劣るものの、GPT-4 の評価と人間による評価には相関がある、という結果が得られています。

様々な指標から、human間の相関 > humanとgpt4の相関 という関係がわかりました。しかし、humanとgpt4の相関は0.818と高く、見かけの一致率は60.3%、ファジーな一致率は88.6%あり、人間の評価と相関しているといえます。GPT-4によるランキングも人手評価と概ね一致しており、順位が一致していない箇所も標準誤差の範囲を考えれば妥当だと考えられます。
引用元: 「ELYZAが公開した日本語LLM「ELYZA-japanese-Llama-2-7b」についての解説 : (2) 評価編」(https://zenn.dev/elyza/articles/5e7d9373c32a98#%E8%A9%95%E4%BE%A1%E6%89%8B%E6%B3%95%E3%81%AE%E5%88%86%E6%9E%90%E7%B5%90%E6%9E%9C%E3%81%AE%E8%80%83%E5%AF%9F)

GPT-4 による評価が人間による評価と完全に一致するわけではない、という点に気をつける必要はありますが、手動で評価するコストを考慮すると、GPT-4 を用いた自動評価は有効な手段だと思います。（GPT-4 による評価の妥当性については考察でも触れます）

モデル
今回調査対象としたモデルは下記のとおりです。



開発者
モデル名
HuggingFace
備考




CyberAgent
calm2-7b-chat
HuggingFace



DeepSeek (深度求索)
deepseek-llm-7b-chat
HuggingFace



DeepSeek (深度求索)
deepseek-llm-67b-chat
HuggingFace
TheBloke さんの量子化モデル (Q4_K_M, GGUF フォーマット) を使用


ELYZA
ELYZA-japanese-Llama-2-7b-instruct
HuggingFace



llm-jp
llm-jp-13b-v1.0
HuggingFace



rinna
youri-7b-chat
HuggingFace



Stability AI
japanese-stablelm-instruct-beta-7b
HuggingFace



Stability AI
japanese-stablelm-instruct-beta-70b
HuggingFace
TheBloke さんの量子化モデル (Q4_K_M, GGUF フォーマット) を使用


Weyaxi
OpenHermes-2.5-neural-chat-7b-v3-1-7B
HuggingFace



清華大学/中国科学院
Xwin-LM-7B-V0.1
HuggingFace



清華大学/中国科学院
Xwin-LM-13B-V0.1
HuggingFace



清華大学/中国科学院
Xwin-LM-70B-V0.1
HuggingFace
TheBloke さんの量子化モデル (Q4_K_M, GGUF フォーマット) を使用


01.ai (零一万物)
Yi-6B-Chat
HuggingFace



01.ai (零一万物)
Yi-34B-Chat-4bits
HuggingFace
公式4Bit 量子化モデル (AWQ) を使用



表の備考欄にも記載していますが、30B 以上のモデルは検証環境 (Colab Pro+) の都合上、元のモデルを実行することができませんでした。
このため、TheBloke さんの 4-bit 量子化モデルを使用しています（Yi-34B-Chat は公式の 4bit AWQ を使用）。
量子化前のモデルは、ここで報告するよりも高い性能をもっている可能性があります。
表に記載したモデルのほか Llama 2 (Facebook) や Falcon-7B (TII-UAE)、Phi-1.5 (Microsoft) も試しましたが、ナンセンスな応答しかできなかったり、日本語で質問しても回答が英語になってしまう等の現象があったため、評価結果からは除外しました。

評価結果

全体の成績
各モデルの平均スコアを図に示します。
データセットの各質問に対するモデルの出力と、GPT-4 による採点理由、最終的なスコアは Google スプレッドシート に掲載しています。

図からわかるように、平均スコアが最も高かったのは Xwin-LM-70B-V0.1 で、次いで deepseek-llm-67b-chat、Yi-34B-Chat と続いています。
上位3つはすべて中国勢で、パラメタ数も30B以上の大規模モデルです。
パラメタ数 7B で比較すると、平均スコアの上位3傑は ELYZA-japanese-Llama-2-7b-instruct 、calm2-7b-chat、deepseek-llm-7b-chat でした。
ELYZA-japanese-Llama-2 や CALM2 は、shi3z さんによる MT-Bench での評価でも高いスコアをマークしていましたが、ELYZA-tasks-100 でも7Bモデルの中で最高レベルの性能を示しています。
一方 japanese-stablelm-instruct-beta-7b が他のモデルに比べて大幅に低いスコアとなっています。
これは、StableLM-β がレスポンスなしの回答をすることが多く（プロンプトの直後に end-of-text となる）、それらの回答をすべて最低スコア（1点）として処理しているためです。
（この問題の回避策をご存じの方は、ぜひ教えてください...）
肌感覚では、2点前後のモデルはまだ発展途上、2.5点前後のモデルは用途次第で活用できるレベル、3点前後のモデルは（GPT-3.5 ほどではないものの）複数の用途で活用できるレベル、と感じました。
なお、測定方法が異なるため直接比較はできませんが、ELYZA が実施した人間による評価での平均スコアでは、GPT-3.5 が 3.8 点、GPT-4 が 4.3 点となっています。
ELYZA が取りまとめた各モデルの回答 を読んでみても、GPT-4 や GPT-3.5 の回答は、オープンLLMの回答と比べて高品質であると感じます。
最近はオープンLLMの性能向上が著しく、英語圏を中心に「GPT-3.5超え」「GPT-4超え」をうたうモデルが登場していますが、日本語での受け答えに関していえば、現時点ではプロプライエタリモデルのほうが優位に立っていると思います。

LLMの回答と GPT-4 による評価の例
具体的なテキスト生成例として、まずは最も性能がよかった Xwin-LM-70B-V0.1 による回答を見てみましょう。
このモデルは中国の研究者チーム（清華大学、中国科学院）が開発しており、学習データセットに日本語テキストが占める割合は高くないと推測されますが、日本語の質問に対してもきちんと回答できています。

Q. あなたはゲーム会社のプランナーです。新しいアクションゲームを企画することになりました。ゲームのコンセプトを考えてください。
A. コンセプト：『Chronicle of the Fallen』（落ちた者たちの年代記）
ジャンル：アクションRPG
物語の概要：
西暦20XX年、地球上に突如として多元宇宙からの侵略者が現れた。彼らは高度な技術力を持っており、人類は圧倒的な力で蹂躙されてしまう。地球上のあらゆる国家は滅亡の危機に瀕するが、そんな時、謎の古代遺跡から発掘された「レリクス」と呼ばれる技術が人類を救うことになった。
レリクスは、地球の生命体と機械の融合を可能にするもので、人間と機械が一体化して侵略者に立ち向かうことができる。プレイヤーは、レリクスを使いこなす特殊部隊「フォールン」の一員となり、地球の未来をかけた戦いに挑む。
ゲームの特徴：

レリクスによる強力な武器や能力の獲得
プレイヤーが機械と融合したキャラクターをカスタマイズ可能
高速アクションバトルと、戦術的なオーダー制を組み合わせたバトルシステム
壮大なストーリーと、多様なエンディング
コミュニティ機能を使って、他のプレイヤーと協力したり、競い合ったりすることができる
ストーリーに沿ったサイドクエストや、チャレンジ要素の豊富なマップ


ストーリーが「マブラヴ」に似ている気もしますが、ユーザの指示に沿った回答ができていることがわかります。
GPT-4 による評価も、最高評価の5点でした。
これに対して、特定の文体で回答する必要がある問題（「小説のセリフを書いてください」「メールの返信を書いてください」など）や、ことわざに関する質問など、日本語固有の知識が要求される問題には弱い、という傾向もみられました。
同じ Xwin-LM-70B-V0.1 による、別の質問への回答をみてみましょう。

Q. 小説のセリフを書くことを手伝ってください。物語の中で、主人公は学校生活での人間関係に問題があります。彼には母親がいますが、母親は彼に良い大学に行ってほしいという気持ちからハードな勉強を押し付けてしまっています。
ここで、最終的に主人公の気持ちを理解することができた母親のセリフを考えてください。
A. 母親：「息子よ、これまで僕らの関係がぎくしゃくしていたことを知っています。僕は君に良い大学に行って欲しい一心で、勉強を押し付けすぎてしまったのね。しかし、今日から僕は変わることを決めました。君の人間関係や学校生活に対して、もう僕は干渉しないであろう。君が喜ぶような道を自分で選んでいくことが大切だと気付いたのです。今後は、僕が君の味方となって、どんな悩みや困りごとにも共に乗り越えていくつもりです。信頼しあい、お互いに励まし合いましょう。それが親子の関係だと思っています。」

文意そのものは提示された場面に合っているため、指示の内容は理解できていそうです。
しかし、「息子よ」「僕は君に...押し付けすぎてしまったのね」「僕は干渉しないであろう」など大仰な言い回しや不自然な表現が多く、そのまま小説のセリフとして採用するのは難しいでしょう。
GPT-4 による評価は3点でした。
このように特定の文体での表現が求められるタスクでは、中国語を日本語に翻訳したような文体になってしまう場合がありました。
おそらく、モデルの内部表現が中国語になっており「中国語で考えた文章を、日本語に訳して話している」ような状態になっているのだと推測されます。
続いて、日本発のモデルである elyza/ELYZA-japanese-Llama-2-7b-instruct の回答例を見てみましょう。
パラメタ数は7Bで Xwin-LM-70B-V0.1 の10分の1ですが、日本語データを中心に学習しているため、日本語独特の表現に強いと考えられます。

A. 承知しました。母親のセリフとしては下記のようなものが適切かと思われます。
「私はずっとあなたが大学に行って、自分の力で道を切り拓いて欲しかっただけなんです。それがあなたにとって、苦しくてたまらないことだとは思わなかった。申し訳ありません。」

最後の「申し訳ありません」という表現がちょっと硬い気もしますが、指定されたシーンでのセリフとしてはおおむね妥当です。
先ほどの Xwin-LM-70B-V0.1 による回答と比べれば、高い評価を与えられそうです。
...ところが、GPT-4 の評価は辛口でした。

評価理由: 言語モデルの回答は、母親が主人公の気持ちを理解したというシチュエーションを踏まえており、母親が謝罪し、自分の行動を反省している点が示されています。しかし、正解例に比べると、母親の気持ちの変化や、今後の関係改善に向けた具体的な提案が不足しています。また、小説のセリフとしてはやや短く、感情の深さや複雑さが十分に表現されていないため、完全には適切とは言えません。そのため、問題固有の採点基準に基づき、文体としてやや不適切な点を考慮して-2点とします。
最終評価: 3点

このように、GPT-4 による評価は、人間の感覚と必ずしも一致しないことがあります。
ELYZA のブログ記事でも、GPT-4 が日本語の自然さを甘めに評価する傾向があることが報告されていますが、この例はそれを裏付けるものとなっています。
同じ問題に対する、別のモデルの回答を見てみましょう。
elyza/ELYZA-japanese-Llama-2-7b-instruct に比べて平均スコアのやや低かった youri-7b-chat の回答を引用します。

A. あなたの母親は、彼女があなたに良い大学に行ってほしいという気持ちから、あなたに過剰な勉強を押し付けてしまっていると感じています。あなたは彼女に対して怒りを感じていますが、彼女があなたを心配していることも理解しています。
彼女は、あなたが自分の感情を理解し、自分自身の人生を生きることができるようになることを望んでいます。彼女は、あなたが自分自身の人生を生きることができるようになることを望んでいます。このような状況で、あなたの母親があなたを理解し、あなたが自分自身の人生を生きることができるようになることを望んでいることを示すことができると良いです。

母親のセリフを考える、という指示に対して、母親の気持ちをユーザに説明しており、指示にうまく従えていません。
GPT-4 による評価は2点で、Xwin-LM-70B-V0.1 や ELYZA-japanese-Llama-2-7b-instruct よりも低い評価となりました。
これは納得のいく評価と言えます。
ここまで、具体例を挙げながら、各モデルの回答や GPT-4 による評価の傾向を見てきましたが、実際の回答例を読むことで各LLMの特性をよりよく理解できると思います。
各質問への回答・評価をまとめた Google スプレッドシート を公開していますので、そちらもぜひご覧ください。

分析
ここからは、パラメタ数や学習トークン数といったモデルの特性と、成績の関係を見ていきます。

パラメタ数と成績の比較
まず、パラメタ数と成績の関係をみてみましょう。
下の図は、パラメタ数をx軸、平均スコアをy軸として、各モデルの性能をプロットしたものです。

成績が上位のモデルに着目すると、パラメタ数が増えるにしたがって、スコアが上昇するという関係が読み取れます。
多数のモデルが密集している 7B クラスでは2点台前半がベストスコアですが、パラメタ数が増加するにつれてスコアが漸増し、70B クラスで平均3.0点のラインに迫っています。
一方、先ほどのグラフからは、パラメタ数が多くなるにしたがってスコアの向上が緩やかになっていることもわかります。
この傾向がはっきり見て取れるのが、次の図です。
今度はモデル自体の成績をパラメタ数で割った値を y 軸にとっています。

7B クラスでは、多くのモデルが1Bパラメタあたり0.3点前後ですが、30B以上では1Bパラメタあたり0.1点を下回っています。
量子化の効果を考慮する必要はあるものの、モデルの規模が大きくなるほど性能向上効果が逓減しているようすが見て取れます。
こうした傾向を踏まえると、リソースが限られるコンシューマ向け GPU にモデルを展開する場合、パラメタ数の増加により性能向上を図る戦略には限界があると考えられます。
70B などの大規模モデルを利用できる場合は別としても、一般向けGPUで動作させる場合は 7B ～ 13B あたりのモデルを選択するのがよさそうです。
なお、パラメタ数が回答に与える影響は、Appendixでも実際の回答例を見ながら検証しています。

学習トークン数と成績の比較
続いて、学習トークン数と成績の関係を図示してみます。
下図では、学習トークン数を x 軸、平均スコアを y 軸としています。
プロットした点の大きさは、モデルのパラメタ数を示しています。
指示応答モデルへのファインチューン用データセットについては、トークン数が公開されていないケースがほとんどのため、ここではベースモデルの学習トークン数で比較しました。
Llama 2 からファインチューンしたモデルは、ファインチューン時に使用したトークン数を示しています。

ぱっと見て気づくのは、Llama 2 からファインチューンしたモデル（青丸）が、図の左側（学習トークン数が少ない）に集中している、という点です。
Llama 2 自体は2Tトークンで学習されており、ファインチューン用のデータは少数で済むため、フルスクラッチで学習したモデルに比べて学習トークン数が少なくなっています。
Llama 2 ベースの ELYZA-japanese-Llama-2-7b-instruct と、フルスクラッチで学習した CALM2-7b-chat を比較すると、CALM2 は ELYZA よりはるかに大規模なデータセットで学習していますが、性能はほぼ同等です。
学習に利用できるデータ量や計算資源が少ない場合、Llama 2 からのファインチューンが有効な手段であると言えそうです。
一方、学習トークン数が多いほど性能がよい、と言い切れない面もあります。
先ほどの図のうち、7Bクラス、かつ Llama 2 からのファインチューンを行ったモデルに絞った図を見てみましょう。

最もトークン数が少ない ELYZA-japanese-Llama-2-7b-instruct が最も高いスコアを出しており、次いで youri-7b-chat 、 japanese-stablelm-instruct-beta-7b と続いています。
これを考慮すると、データセットの規模よりも、むしろ学習の手法やデータセットの品質がELYZA-tasks-100 ベンチマークでの性能評価に大きく影響しているように思えます。

考察
ここまで、オープンLLMの性能を評価するためのベンチマークである ELYZA-tasks-100 を使って、各モデルの性能を比較してきました。
今回の比較分析で得られた知見をまとめます。

結局、どのモデルを使えばよいのか
パラメタ数が比較的少ない 7B レンジでは、ELYZA-japanese-Llama-2 や CALM2 などの日本発モデルが高いパフォーマンスを発揮しています。
リソースに制約がある環境では、これらのモデルが最適な選択肢となりそうです。
Xwin-LM や Yi-Chat などの中国発モデルも指示追従能力は高いですが、パラメタ数が少ないモデルでは日本語の不自然さが目立つため、用途を選ぶ必要があります。
一方、パラメタ数 30B 以上の大規模モデルでは、（そもそも日本発のモデルが少ないこともあり）海外モデルが高い性能を示しています。
リソースに余裕がある場合は、大規模な海外モデルを試してみるのもおすすめです。
日本語圏でも 30B レンジ以上のモデルが増えてくれば、高い指示応答能力と自然な日本語を兼ね備えたモデルが登場する可能性があります。
また、最近では MoE などの比較的小規模なモデルでも高い性能を発揮する手法が利用されはじめており、日本語圏でもこうした手法を取り入れたモデルが登場すれば、リソースが制限される環境でも性能のよいモデルが利用できるようになるかもしれません。
今後の動向にも注目したいと思います。

GPT-4 による自動評価は信頼できるのか
今回は GPT-4 を用いて自動評価しましたが、この手法が人間の評価を完全に代替できるわけではありません。
GPT-4 による回答評価の課題として、以下のような点が挙げられます。

言語による性能差が、評価に影響する可能性があります。記事内でもご紹介したように、日本語として不自然な表現があるにもかかわらず見逃しているケースのほか、正しい回答をしているのに減点を適用しているケースもありました。GPT-4 テクニカルレポート では、GPT-4 の日本語の理解度が英語に比べて低いことが示唆されています。
LLM がモデルを評価する際、バイアスがかかっている場合があります。長い回答を選好するバイアス（verbosity bias）や、自分自身の出力によって学習したデータを選好するバイアス（self-enhancement bias）などが知られています。
GPT-4 は API 経由で利用する必要があり、モデルバージョン間の差異や回答のランダム性などが原因で評価の再現性を担保しづらいという問題があります。なお、今回の比較調査では、API バージョンを gpt-4-1106-preview で固定しています。

GPT-3.5 や Claude-2 を上回るベンチマーク成績を出して話題となった Starling-7B の開発者らも、GPT-4 による評価の一般化に対して警鐘を鳴らしています。

It’s important to highlight that the model’s preference ranking by GPT-4 does not necessarily correlate with human preference, a phenomenon that echoes the principles of Goodhart’s Law. Essentially, a higher MT-Bench score, as endorsed by GPT-4, doesn’t automatically imply greater human favorability, especially compared to models with lower scores.
強調すべきは、GPT-4によるモデルの好みのランキングが必ずしも人間の好みと相関するわけではないという点である。これはグッドハートの法則が示す原則を反映した現象である。煎じ詰めて言えば、GPT-4 の評価による MT-Bench スコアが高いからといって、そのモデルが人間にとって好ましいモデルだ、と自動的に判断できるわけではない。特に、スコアが低いモデルと比較した場合はそうである。
出典: ""Starling-7B: Increasing LLM Helpfulness & Harmlessness with RLAIF"" (https://starling.cs.berkeley.edu/), 翻訳は筆者による

とはいえ、GPT-4 が提示した順位は筆者の肌感覚にも近く、性能のよいモデルのスクリーニングとしては活用できそうだと感じました。
人間が実際の回答を読んで評価することはもちろん必要ですが、大量のモデルを比較したいときや、限られたコスト・時間で評価しなければならない場合の選択肢として、GPT-4 による自動評価は有用だと思います。

おわりに
海外では Llama 2 をはじめ「GPT-3.5/-4 を超えた！」とうたうオープンモデルが次々と公開され、モデル間を比較する環境も整っている一方、日本語LLMの性能評価データは十分整備されていない、という現状があります。
JGLUE のようなクローズエンド質問のデータセットは、LLM の基礎的な能力（知識や論理的推論など）を測ることには適していますが、MT-Bench の論文 などでは、モデルがチャット用途でどの程度役立つかの指標としては不向きである、と指摘されてきました。
オープンなモデルも含め、LLMの分野は日進月歩です。
この記事を書いている間にも、Mixtral 8x7B など新しいモデルが次々と公開されており（Mixtral については別記事にまとめています）、これからも LLM の進化は続くでしょう。
日本語圏のLLMベンチマークがさらに充実し、LLMの比較がしやすくなることを期待しつつ、今後もさまざまなモデルを試していきたいと思います。

リンク

評価結果まとめスプレッドシート
GPT-4 による自動評価用スクリプト
LLM の回答生成用 Colab ノートブック

calm2-7b-chat
deepseek-llm-7b-chat
deepseek-llm-67b-chat
ELYZA-japanese-Llama-2-7b-instruct
llm-jp-13b-v1.0
youri-7b-chat
japanese-stablelm-instruct-beta-7b
japanese-stablelm-instruct-beta-70b
OpenHermes-2.5-neural-chat-7b-v3-1-7B
Xwin-LM-7B-V0.1
Xwin-LM-13B-V0.1
Xwin-LM-70B-V0.1
Yi-6B-Chat
Yi-34B-Chat




Appendix. パラメタ数が回答に与える影響の検証
パラメタ数が回答品質に与える影響を示す例として、同じ質問に対する Xwin-LM の 7B、13B、70B モデルの回答を比較してみましょう。

Q: 次に読むべきSFを10冊挙げて

Xwin-LM-7B-V0.1
以下は、SFファンにとって絶対に読むべき10冊の中から選んだものです。

『さらば、宇宙の戦士』（Arthur C. Clarke）：冒険的なテーマと先進技術が絶妙に調和した作品。
『ドラキュラの娘』（Dracula's Daughter）：ファンタジーSFとホラーの境界を描いた不朽の作品。
『地球が静止する日』（The Day the Earth Stood Still）：SFの古典として知られる作品。
『2001年宇宙の旅』（2001: A Space Odyssey）：カンバートンのSF小説の中でも屈指の名作。
『星の戦士』（The Star War）：SFとファンタジーが融合したシリーズ。
『虹の彼方』（The Rainbow's End）：SFの世界観と未来の社会を描いた優れた作品。
『戦士の賦役』（The Soldier's Tale）：コルトーのSF短編で、音楽とSFを融合させた優れた作品。
『ターミネーター』（Terminator）：時間旅行やロボットの哲学を扱った快作。
『バック・トゥ・ザ・フューチャー』（Back to the Future）：時間旅行と科学の進歩をテーマにした楽しい作品。
『秘密結社シャドウラン』（The Shadowrun）：ファンタジーとSFが融合したシリーズ。

これらの作品は、SFファンにとって欠かせないものであり、多くの人々にとって影響を与えた作品です。

4（2001年宇宙の旅）と6（虹の彼方）を除いて、この回答は「読むべきSF」つまりSF小説を挙げるという題意を満たしていません。
挙げられた項目の半数以上は映画（2,3,5,8,9）や劇（7、内容もSFではない）、TRPG (10) になっており、題意を十分理解できていないことが伺えます。
なお 1 は架空の作品で、実際には存在しないと思われます。
次に、13B モデルの回答を見てみましょう。


Xwin-LM-13B-V0.1
以下に、SFファンにおすすめの10冊の小説を紹介します。これらの作品は、革新的なテーマやアイデア、創造的な設定によって知られており、SFの世界にとって重要な作品です。

『地球最後の男』（The Last Man on Earth） - フィリップ・K・ディック（Philip K. Dick）
『異星からの侵略』（The War of the Worlds） - H・G・ウェルズ（H.G. Wells）
『2001年宇宙の旅』（2001: A Space Odyssey） - アーサー・C・クラーク（Arthur C. Clarke）
『スターシップ・トゥルーパーズ』（Starship Troopers） - ロバート・A・ハインライン（Robert A. Heinlein）
『宇宙の旅』（A Wrinkle in Time） - メアリー・D・プロイス（Madeleine L'Engle）
『ルーシャス・エフェクト』（The Sparrow） - メアリー・D・プロイス（Mary Doria Russell）
『火星の人』（The Martian Chronicles） - レイ・ブラッドベリ（Ray Bradbury）
『異星からの使者』（The Left Hand of Darkness） - ウルスラ・K・レッグーム（Ursula K. Le Guin）
『スラングル』（Slaughterhouse-Five） -  Kurt Vonnegut
『タイタンの戦い』（The Battle of the Titans） - サミュエル・R・ディレイニー（Samuel R. Delany）

これらの作品は、SF小説の歴史やジャンルの多様性を代表するものです。各作品は、独自のスタイルやテーマを持っており、SFファンにとってはおすすめの閲覧物となっています。

作品名が日英2言語で挙げられていますが、英語で挙げられた作品はほぼすべてSF小説です（10 は架空の作品ですが）。
一方、作品名の和訳が誤っていたり（たとえば 8. ""The Left Hand of Darkness"" は『闇の左手』が正しい）、作者の日本語表記が英語名と合っていない（5 はメアリー・D・プロイスではなくマデレイン・レングル）など、回答のなかに自己矛盾が見られます。
質問の意図を理解していることは伺えますが、役立つ回答とは言い難いです。
最後に、70B モデルの回答です。


Xwin-LM-70B-V0.1
以下は、今後読むべきSF10冊のリストです。これらの作品は、古典的な名作から現代のベストセラーまで、さまざまな時代とジャンルを代表しています。

『フランケンシュタイン』（Mary Shelley） - この古典的なゴシックSF小説は、1818年に発表されました。研究者が死と再生の境界線を越えて造り出したクリーチャーとその創造主との関係を描いています。
『時計じかけのオレンジ』（Anthony Burgess） - 1962年に発表されたこのディストピアSF小説は、近未来の暴力と狂気に満ちた世界を描いています。
『2001年宇宙の旅』（Arthur C. Clarke） - この1968年の小説は、スタンリー・キューブリック監督による同名の映画と共に発表されました。人類が初めて接触する高度なエイリアン文明との遭遇を描いています。
『猿の惑星』（Pierre Boulle） - 1963年に発表されたこのSF小説は、地球が猿に支配された遠未来を描いています。人類と猿の関係や倫理的な問題をテーマにしています。
『タイム・マシン』（H.G. Wells） - H.G.ウェルズの1895年の小説で、タイムトラベルと未来社会を描いています。この作品は、SFジャンルにおけるタイムトラベル物語の先駆けとなったでしょう。
『審判』（Franz Kafka） - 1925年に発表されたこの小説は、KafkaがSF的な要素を取り入れて描いた神秘的な世界を描いています。
『1984年』（George Orwell） - 1949年に発表されたこのディストピアSF小説は、全体主義的な政府が支配する近未来の世界を描いています。
『ファウンデーション』（Isaac Asimov） - 1951年に発表されたアイザック・アシモフの SF小説。数千年にわたる銀河帝国の歴史を描いており、心理学と歴史の研究がテーマです。
『Dune』（Frank Herbert） - 1965年に発表されたフランク・ハーバートのSF小説。砂漠惑星アラキスを舞台に、政治と宗教の闘争を描いています。
『ニューヨーク2140』（Kim Stanley Robinson） - 2017年に発表されたこの近未来SF小説は、海水面上昇によって変わりつつあるニューヨークを舞台に、環境問題と人類の居住空間をテーマにしています。


6（カフカの『審判』）は通常SFとは見なされませんが、それ以外はSF小説として適切です。
作品を薦める記述もあり、親切な回答ができています。
このように、パラメタ数が少ないモデルでは、問題の趣旨を誤解して的外れな回答をする場合や、回答に矛盾があるケース（英語と日本語で作者名が異なるなど）がみられますが、パラメタ数が増加するにしたがって、問題の趣旨をきちんと理解して適切な回答ができるようになっています。
モデルの規模と回答品質の関係が、このように明確に読み取れるのは興味深いところです。

Appendix 2. 他のモデルも評価してみた
本編の調査終了後に公開されたモデルも、同様の手法で評価してみました。
調査したモデルは次のとおりです。



開発者
モデル名
HuggingFace
備考




清華大学/SAIL(上海人工知能研究所)/01.ai
openchat-3.5-1210
HuggingFace



東京工業大学/産総研
Swallow-7b-instruct-hf
HuggingFace



東京工業大学/産総研
Swallow-13b-instruct-hf
HuggingFace




平均スコアは次の図のようになりました。
OpenChat-3.5-1210 が平均2.08点、Swallow-7B が1.92点、Swallow-13B が 2.12 点、という結果でした。

各モデルの回答例を見てみましょう。

Q. 仕事の熱意を取り戻すためのアイデアを5つ挙げてください。

OpenChat/openchat-3.5-1210

仕事の目標をリフェット: 新しい目標やプロジェクトに取り組むことで、元々の興味や熱意を回復できます。新たな挑戦を選ぶことで、疲れやむりではなく楽しく仕事を敢いすぐれるようになります。
コラボレーションを図る: 他の役割や部門のメンバーとワークショップやコミュニケーションを通して、一緒に新しいアイデアや手法を生み出しましょう。他者からのアイデアや視野を広げることで、仕事の楽しさや取り組みを再び見つけることができます。
自己成長: 自分のスキルや知識を提高することで、仕事の興味を取り戻せます。例えば、新たな工程や技術を学ぶことで、興味深い仕事に取り組むことができるようになります。
ワークライフのバランス: 仕事以外の活動や趣味を重視し、ストレスを解消するために役割を分担しましょう。新たな飾る空間や休日の活動にコミッティーすることで、興味深い人たちとの出会いや新しい視界を見つけることができます。
成果を実感する: 目指す成果やゴールに向かってステップバイステップ取り組むことで、進捗を実感し、成功感をもたらせます。これにより、熱意を取り戻せるようになります。

これらのアイデアを実践し、自分たちの仕事環境に応じて柔軟に取り組むことで、元々の熱意を再生できるでしょう。

回答の方向性自体は合っているのですが、「リフェット」「敢いすぐれる」「新たな飾る空間や休日の活動にコミッティーする」など、日本語の自然さという面では改善の余地があります。
日本語データを使ってファインチューンすれば、もっと良い回答が得られるかもしれません。


tokyotech-llm/Swallow-7b-instruct-hf
仕事の熱意を取り戻す5つのアイデアを紹介します。
1.達成したい目標を設定する。
2.同僚と積極的に関わり、支援を求める。
3.タスクの優先順位を明確にする。
4.プロジェクトを小さく分解し、段階的に進める。
5.新しいテクノロジーを学び、ツールの使い方を向上させる。

日本語の自然さは高く、さすが日本語圏のモデルです。
ただ、仕事の熱意回復というよりは、仕事の効率化につながるアイデアになっているのが惜しいところ。


tokyotech-llm/Swallow-13b-instruct-hf
仕事の熱意を取り戻す5つの方法
1.仕事を好きになる
2.目標を設定する
3.ワークアウト
4.セルフケアを行う
5.感謝をしてください

回答自体は簡潔ですが、質問のテーマには合っていると思います。

Appendix 3. さらに他のモデルも評価してみた
他の日本語モデルとして、Augmxnt の shisa-7b-v1 と、TheBloke さんが公開した Swallow-70B-instruct の GGUF モデル も評価してみました。



開発者
モデル名
HuggingFace
備考




東京工業大学/産総研
Swallow-70b-instruct-hf
HuggingFace
TheBloke さんの量子化モデル (Q4_K_M, GGUF フォーマット) を使用


Augmxnt
shisa-7b-v1
HuggingFace





予想外だったのは、Swallow-70b（4-bit 量子化）の成績が 2.10 点と、Swallow-13b （平均2.12点）よりも低かったことです。
他のモデル（Xwin、StableLM-beta、Yi など）では、パラメタ数の増加にあわせてスコアが向上していたのですが、Swallow-70b はこの傾向とは逆の結果になりました。
本編でも触れたように、今回の比較では 30B 以上のモデルに対して、原則として TheBloke さんの GGUF フォーマット（4bit）による量子化を適用していますが、何らかの理由で Swallow が量子化による精度低下の影響を強く受けている可能性があります。
この問題については、今後検証してみたいと思います。
具体的な回答例を見てみましょう。

Q. 仕事の熱意を取り戻すためのアイデアを5つ挙げてください。

Swallow-70b-instruct-hf
ここでは、あなたの仕事への熱意を再燃させるかもしれないアイデアを5つ紹介します:
1.自分がなぜその仕事に就きたいと思ったのかを思い出してみてください。もし、あなたがその職業を選んだ動機や目標を思い出したら、それらを紙に書き出して、今でもモチベーションにつながるかどうか確かめてみましょう。また、自分のキャリアで達成したいことをリストアップする時間を取ってもいいでしょう。
2.同僚と話してみる:仕事が退屈でストレスがたまっているときは、友人や家族、同僚に相談するとよいでしょう。そうすれば、彼らがどのようなアイデアを持っているか、またあなたの状況を経験したことがあるかどうかを知ることができます。
3. 新しいスキルを学ぶ:もし、あなたが仕事でより多くの刺激ややりがいを求めているなら、その分野に関連する新しいスキルを学ぶ時間をとってみましょう。これは、読書をしたり、オンラインコースに登録したり、地元のワークショップやクラスに参加したりすることかもしれません。
4. 休憩を取る:もし仕事がストレスやフラストレーションの原因になっているなら、しばらく休みをとって自分自身の面倒を見てもいいでしょう。友人と交流したり、運動したり、新しい場所を訪れたりしてみましょう。そうすれば、リラックスでき、エネルギーが湧いてくるような新しい視点で仕事に戻れるかもしれません。
5. 上司やメンターに相談する:自分のキャリアをもっとエキサイティングでやりがいのあるものにしたい場合は、上司やメンターと話してみましょう。より重要なプロジェクトへの参加や、チームメンバーと協力した新しいアイデアの追求など、あなたが仕事に対して熱意を持てるような方法を提案できるかもしれません。
このような戦略を用いることで、あなたのキャリア目標を見直し、達成したいことをよりよく理解することができます。

Shisa-7b-v1


目標を設定する: 仕事に対する情熱を取り戻したい場合、まず自分のキャリアのビジョンと現在のポジションとのギャップを特定することが重要です。また、自分自身の価値観や興味に沿った仕事に就くことでやりがいを感じることができるかもしれません。


新しいスキルを学ぶ: 新たなスキルを身につけることで、職場での役割を再定義し、新たな挑戦に備えることができます。また、特定の分野の知識を深めることも、モチベーションの向上につながります。


キャリアカウンセラーに相談する: キャリアカウンセリングを受けることで、今の状況を客観的に分析し、その後の計画を立てることができます。また、キャリアカウンセラーは、あなたの強みと弱み、キャリア上の目標、将来の夢について話し合うことができます。


自己啓発プログラムに参加する: 自己啓発プログラムやワークショップに参加することで、自信を高め、成功への意欲を高めることができます。これらのプログラムでは、コミュニケーション能力、リーダーシップ、問題解決などのスキルを学びます。


プロジェクトチームに参加する: 他の同僚と共同で作業に取り組めば、より創造的な方法で仕事を進めることができ、共通の目標に向かって協力しやすくなります。



どちらのモデルも、おおむね題意に沿って回答ができていそうです。

Q. 以下の説明を読んだユーザーが感じるであろう疑問とは？
大量のゴミの処理にお困りではありませんか？
「生産で出るゴミ処理費用が大幅にかかっている」
「指定ゴミ袋が導入され、一気にコストが上がってしまった」
「工場が狭いので、ゴミ保管スペースを減らしたい」
「ISO14001でゴミ削減が大きな課題となっている」
場所をとる嵩張るゴミをつぶしてまとめて運びやすい形にすることで、処理・保管・運搬にかかる様々なコストを削減したり、環境負荷の低減にも貢献できるのが、廃棄物圧縮減容機→『ゴミ圧縮機』です。
ゴミ圧縮機の主な用途は廃プラスチック・紙・段ボール・袋類・ペットボトル・プラスチック容器などつぶれるが元に戻ってしまうゴミを圧縮梱包します。

Swallow-70b-instruct-hf
質問

Shisa-7b-v1
ユーザーが感じているかもしれない疑問を以下に示します。

大量のごみ処理に困っている？
「廃棄物の排出量が大きく増えた」と感じますか？
工場が広いため、ごみ保管スペースを節約する必要がありますか？
ISO 14001の要件により、廃棄物を減らすことが課題になっていると想像できますか？


Swallow-70B の回答は、題意から外れた内容になってしまいました。
Shisa の回答は方向性が合っているものの、質問に対する的確な回答とは言い難い面があります。
"
https://qiita.com/kaita_kakihara/items/bf66e9e4927a3e3589e4,"['初心者', 'pandas', 'データ分析', 'Kaggle', 'データアナリティクス']","この記事はNuco Advent Calendar 2023の21日目の記事です。

第１章：はじめに

本記事の目的
さてさて、この記事を読んだらいったい何が出来るようになるのよ！っていうのが、気になるところではあると思います。簡単に対象レベルを述べるのであれば、以下のような人が挙がってくるのではないでしょうか？
＜対象レベル＞
・何となくPythonを勉強し始めてみたけど、何やってみようかな？と考え中の方
・データ分析をこれからやってみたいなって思っている方
・就活もしくは転職でデータサイエンティスト、データアナリストを目指している方
・普段SQLは使っているけど、Pandasだとどうやるんだ？となっている方
などなど、様々な境遇の皆さんに刺さる記事になってくれれば。。！
＜出来るようになること＞
・グラフ出力に必要なデータの加工
・よく使う一般的なグラフの出力
・各グラフの持つ特徴の理解
最後の『特徴』って何だよって感じですが、実務では出力をしたグラフの体感半分くらいの行先は他者の目に止まる資料などです。彼らが例えあなたの出力したグラフの背景知識に乏しかったとしても、「あぁなるほどね！理解理解！」と言わせなければなりません。であれば、どのグラフを使用するか決定するのに『グラフの持つ特性』を理解することは必要不可欠です！
ではでは、最後までお付き合いよろしくお願いしますm(__)m


弊社Nucoでは、他にも様々なお役立ち記事を公開しています。よかったら、Organizationのページも覗いてみてください。
また、Nucoでは一緒に働く仲間も募集しています！興味をお持ちいただける方は、こちらまで。






タイタニックデータについて

1.タイタニック号沈没事件
それでは早速ですが皆様、こちらの船はご存知でしょうか？
ドン！！


https://voi.id/ja/memory/273057 『1912年4月15日:大西洋に沈没したタイタニック号』

はい、かの有名なタイタニック号ですね！
まあ歴史事実としても有名ですし、映画化もされていますよね。
いや観たことないよ！って人でも主題歌『My Heart Will Go On』を聴けば、
あぁこれタイニックなんだ！！ってなるでしょう。
多くの犠牲者を出してしまったタイタニック号事件ですが、Kaggleという機械学習のコンペサイトには、乗客情報とその乗客の生死が記録されているデータがあります。
本記事では、そのデータを用いて解説をしていこうと思います。

2.タイタニックデータをダウンロードしよう
Step 1： Kaggleにアクセス
Step 2: 右上のRegiterをクリック
Step 3: Googleアカウント、もしくはメールアドレスで登録
Step 4: トップページ検索欄から『Titanic - Machine Learning from Disaster』と検索
Step 5: 検索結果から『Titanic - Machine Learning from Disaster』を選択
Step 6：『Data』を選択
Step 7: 下にスクロールして、右下の『Summary』からダウンロード

3.タイタニックデータを見てみよう
では、早速タイタニックデータをダウンロードして、中身を見てみましょう。
データの取り込みと出力は以下のように実行します。
import pandas as pd

# CSVファイルの読み込み
df = pd.read_csv('train.csv')

# trainデータを出力
df





カラム名
説明




PassengerId
乗客の一意の識別番号


Survived
乗客が生存したかどうか（1: 生存, 0: 死亡）


Pclass
乗客の社会経済的地位を示すクラス（1st, 2nd, 3rd）


Name
乗客の名前


Sex
乗客の性別


Age
乗客の年齢


SibSp
タイタニック号に同乗している兄弟または配偶者の数


Parch
タイタニック号に同乗している親または子供の数


Ticket
チケット番号


Fare
乗船料金


Cabin
部屋番号


Embarked
乗船した港（C: Cherbourg, Q: Queenstown, S: Southampton）



いかがでしょうか？
このような生々しいデータに触れるなんて凄い時代ですよね😭
それでは、次の章からこのデータをつかってPandasの基本操作を確認していきます！！

第２章：データ操作ためのPandas基礎知識

そもそもPandasって何？
PandasはPythonでモデリングする際のいわゆる前処理段階で力を発揮するライブラリーです。様々なデータ加工処理をスムーズに行うことができ、表計算やデータの抽出・検索などを行うことができます。例えば、読み込んだデータからある条件を満たすindex(行)を抽出したり、あるカラム（列）の平均値などの統計量を算出、さらにはデータ同士の結合などの操作を行うことができます。データベースのSQLや、より一般的なものだとExcelとほぼ同じような役割を果たしてくれます。

Pandasの入力形式・出力形式
CSVが一般的ですが、他の形式も多く対応していますので以下の表にまとめておきます。



データ形式
入力メソッド
出力メソッド




CSV
read_csv
to_csv


Excel
read_excel
to_excel


JSON
read_json
to_json


HTML
read_html
to_html


SQL
read_sql
to_sql


Parquet
read_parquet
-


Feather
read_feather
-


Clipboard
read_clipboard
to_clipboard


HDF5
read_hdf
to_hdf


Msgpack
read_msgpack
to_msgpack


Stata
read_stata
to_stata


SAS
read_sas
-


Pickle
read_pickle
to_pickle




データの基本情報を調べる

1.先頭・末尾の５行を抽出
まずは簡単にデータの中身を見たい時に役立つメソッドのご紹介から、、、


先頭５行の表示
df.head()
先頭N行の表示
df.head(N)
末尾５行の表示
df.tail()




()内に何も入れなければ、デフォルトの５行が表示されます。


頭のhead,尻尾のtailなので覚えやすいですよね☺️
では、試しに実行して出力結果を見てみましょう。
#先頭５行を出力
df.head()


#末尾５行を出力
df.tail()


想像通りの結果ですね。
ちなみにindex（行番号）は０始まりなので、一緒に覚えておきましょう！

2.データフレームの大きさ
『データフレームの大きさ』と言われても、
まずデータフレームってなんだよ！！って最初はなりますよね。
今回で言えば、読み込んでいまdfに入っている”表”のことですから、
つまりは何行何列の表かを知りたいわけです。
そんな時は以下のように書けばOKです！


#データフレームの大きさの確認
df.shape


#データフレームの大きさを確認
df.shape

そうすると出力は、

となります。
この出力結果は行・列の順番に記載されています。
覚え方は『行列』でもいいですし、（x,y）と数学で記載するように
（x方向、y方向）つまり（行数、列数）でも良いんじゃないでしょうか？

3.基本統計量の表示
次に、読み込んだデータの統計量を簡単に確認する方法を紹介します。
ちなみに統計量とは、平均や最大値などのデータから計算できる値のことを言います。


基本統計量の表示
df.describe()


英語でdescribeは『〜を説明する。〜を描写する。』という意味ですので、覚えやすいですね！
では出力させてみましょうかね。
#基本統計量の表示
df.describe()


簡単に表示された統計量についてまとめておきます。



カラム名
説明




count
件数


mean
平均（算術平均）


std
標準偏差


min
最小値


25%
第１四分位数


50%
中央値（第２四分位数）


75%
第３四分位数


max
最大値



数Ⅰまでに習ったものばかりですね。（今は中３で中央値をやってるらしい。。）
中央値は後程述べますので、第１四分位数・第３四分位数に関して少しだけ復習を、、、
第１四分位数：最小値と中央値（第２四分位数）の”中央値”
第３四分位数：中央値（第２四分位数）と最大値の”中央値”
でしたね？
こんな感じで、describe()を使うことで、
数行眺めただけでは得られない情報が一瞬で確認できるので、是非とも覚えておきましょう🥸

データの選択とフィルタリング

1.列や行の選択
カラムを選択したい時は、


df[['欲しいカラム名1', '欲しいカラム名2',...]]


とするだけです！
このように、DataFrame 型のオブジェクトに対して、[] でインデックスを指定することで対象カラムのデータのみ抽出可能です。このとき、インデックスを対象カラム名のリスト [""カラム名1"", ""カラム名2"", ...] で与えなければならないことに注意しましょう。
それでは、早速乗客の名前と生存情報を抽出してみましょう！
#NameとSurvivedのみ抽出
df[['Name', 'Survived']]

そうするとこんな感じ、、、

簡単ですよね！！
df[['Name', 'Survived']]からNameとSurvivedのみしか出力されていませんね。
ちなみに３つ以上のカラムを出力したいときは、以下のようにリスト内に加えていけばOKです！
#Name,Sex,Survivedのみ抽出
df[['Name', 'Sex', 'Survived']]


2. 条件に基づくフィルタリング
条件を課したい時は、以下のように記述します。


df [df['条件を課すカラム名'] == num]　（等号成立）
df[df['条件を課すカラム名'] >= num] （以上）
df[df['条件を課すカラム名'] < num]　（未満）


Pythonを少し触ったことがある人なら分かるかもしれませんが、
条件式なのでif文と同様、＝を２つ付けるんですね〜
では、生存した乗客のデータのみピックアップしてみましょう。
言い換えれば、Survivedが1となる列のみ出力すればいいわけです！
# Survivedが1の行だけを抽出
df[df['Survived'] == 1]

結果はこんな感じ、、、

ご覧のとおり、Survivedの列は1しか無くなってしまいましたね👍
少し補足します。
df['条件を課すカラム名'] == numの部分だけを実行したらどうなるんでしょうか？
df['Survived'] == 1


はい。条件式が成立したインデックス(行番号)がTrue, 不成立がFalseで表記されていますね！
つまり、前項と同様に[]でTrueとなるインデックスを指定しているわけです。

3.複数条件の組み合わせ
次は複数の条件を課してみましょう。
条件式を複数使用する場合は、以下のように論理演算子を用います‼️


df[(条件式１) 論理演算子 (条件式２)]





　論理演算子　
　意味　




&
積集合（かつ条件）


｜
和集合（または条件）


^
排他的論理和（一方が真、他方が偽のもの）



３つ目を除けば、「あ！これね！」って感じですよね！主に上２つがメインです。
自分の今やりたいことは、どの演算子でいけるかな？と考えてみて下さいね(^-^)
では例えば、
『Queenstownから乗船した女性』という条件は『EmbarkedがQ,かつSexがfemale』と言い換えできますね！
なので、
# 生存した女性のデータを抽出
df[(df['Embarked'] == 'Q') & (df['Sex'] == 'female')]


というわけですね！
今回はかつ条件（積集合）なので&でしたが、
または条件（和集合）であれば | に置き換えればオッケーです⭕️
どうですか？少しずつ幹らしきモノが見えてきましたか？
これまでと同様にどうやってインデックスを指定するかですよね！

4.行・列の削除
さて、次は行・列の削除について扱います。
上でやったように、カラムの要素に条件を与えることで『削除』と同義になりますが、
今回はdropメソッドを使ってやってみましょう☝️
使い方は以下の通り！


＜行の削除＞
df.drop([""削除したい行のindex番号１"", ""削除したい行のindex番号2"",・・・], axis=0) 
⇨index指定の場合はリスト型[]に！
＜列の削除＞
①消したいカラム（列）が１つの場合
df.drop(""削除したい列のカラム名１"", axis=1) 
②消したいカラム（列）が複数の場合
df.drop([""削除したい列のカラム名１"", ""削除したい列のカラム名2"",・・・], axis=1) 




dropメソッドですので、drop[]としないように注意⚠️




列の削除の場合でも、複数カラムの場合はリスト型[]で指定しないといけないので注意⚠


上記のことに注意して、まずは先頭の５行を消してみましょう。
#リストは０番スタートなので先頭５個は０〜４
df.drop([0, 1, 2, 3, 4], axis=0)


０〜４のindexが削除されていますよね！
次に列（カラム）を複数削除してみましょうか。
まあ何でもいいんですが、
個人情報になり得そうな名前、性別、年齢、チケット番号を消しましょう。
（本当に何でもいいですよ）
#Name,Sex,Age,Ticketを削除
df.drop([""Name"", ""Sex"", ""Age"", ""Ticket""], axis=1)


消したかった４つのカラムが消えていますよね！
メソッドを使うとより直感的に理解できるのではないでしょうか？

欠損値の取扱い

1.欠損値の検出
先から出力されたデータフレームを見ていると、NaNが目に入ってきません？
これが欠損値と呼ばれものです。まあ、情報がないってことですよ！
（発生原因は、入力忘れや無回答、システム上の問題など様々）
まずは、各カラムに欠損値がどれだけあるかを確認してみましょう！
#欠損値の検出
missing_values_per_column = df_data.isnull().sum()

#欠損値のあるカラムを出力
missing_values_per_column


言うまでもありませんが、一応まとめときましょうか。。
年齢（Age）不明が１７７人
部屋番号（Cabin)不明が６８７人
乗船した港（Embarked）不明が２人
こんな感じです。

2.欠損値の補完と削除
欠損値を把握したら、今度は補完（妥当と思われる値を入れること）してみましょう！
今回は年齢（Age）を平均値で補完してみようと思います！
# 年齢の平均値を計算
average_age = df_data['Age'].mean()

# 年齢の欠損値を平均値で補完
df_data['Age'] = df_data['Age'].fillna(average_age)

#欠損値の検出
missing_values_per_column = df_data.isnull().sum()

#欠損値のあるカラムを出力
missing_values_per_column


先程まで１７７個も欠損値があったAgeが０個になっていますね！
コードの説明ですが、
.mean()で平均値の計算　　
.fillna()で欠損値の補完をしているわけです！

3.欠損値の統計情報
少々ここで脱線します。（一回休憩☕️）
機械学習を学び始めたばかりだと私を含め、
『欠損値はない方がいい！！埋めなければ！！！！』なんて思ったりもします😅
しかし必ずしもそうではありません！
例えば、
・ユーザーがある意図を持ってアンケートで空欄にした
・駐車場の有無を1,0で表していたが、nullはデータの入手不可を意味していた
つまり、
欠損値であることが有効な情報になることもあるのです！
だからこそいきなり欠損値補完するのではなく、一回思考を踏む必要があるわけですね。
無視するのがいいか、除外するのがいい、それらしい値を入れるのか、これが重要です。
取る方法によっては、大きなバイアスのある結果を与え、誤った意思決定に繋がってしまい、大きな損失を被る可能性があります。
ですから、より慎重に。。。
って、私も尊敬する先輩から教わりました笑笑

データの集約
さてさて、ここではよく使う集約メソッド４つ紹介します。



カラム名
説明




count()
件数


sum()
合計


mean()
平均（算術平均）


median()
中央値（第２四分位数）


max()
最大値


min()
最小値


corr()
相関係数



では実際に使っていきましょう！！

1.件数(count)
countメソッドを使って、生存者者数・非生存者数を出してみましょう。
# 生存者と非生存者の人数を計算
survived_count = df_data[df_data['Survived'] == 1]['PassengerId'].count()
not_survived_count = df_data[df_data['Survived'] == 0]['PassengerId'].count()

# 結果を表示
print(""生存者の人数:"", survived_count)
print(""非生存者の人数:"", not_survived_count)



2.合計(sum)
『欠損値の補完（欠損値の検出）』のとこでしれっと登場していましたが、
もう一度確認しときます！
今回はFare（料金）の合計値でも出してみます。
# Fare列の値の合計を計算
total_fare = df['Fare'].sum()

print(f'Total Fare: {total_fare}')


最近の為替で円に直せば大体400万程度といったところでしょうか。
やはり、物価は当時に比べたら遥かに上がっていますね〜

3.平均(mean)
はい！！！
一個前の節では年齢の平均を扱いましたので、今回はチケット代を平均してみましょう。
# Fareの平均を計算
average_fare = df_data['Fare'].mean()
# Fareの平均を小数点第一位まで丸める
average_fare = round(average_fare,1)

# 結果を表示
print(""Fareの平均:"", average_fare)


ちょっと知識の補充がてらroundメソッドを使用してみました😎
使い方は小数第N位に丸めたいとするなら、round(丸めたい数字,N)とすればいいです！

4.中央値(median)
平均値の次は中央値です。
何となく似たような値かな？なんて高校生の頃は思ったりもしましたが、実はちがう。。。
# Fareの中央値を計算
median_fare = df_data['Fare'].median()
# Fareの中央値を小数点第一位まで丸める
median_fare = round(median_fare,1)

# 結果を表示
print(""Fareの中央値:"", median_fare)


平均値と中央値でどちらを選ぶべきかという話を少しだけしましょう。
結論、外れ値の影響を受けやすいかどうかです！
「5人が物理のテストを受けた結果、
　4人が10点以下を取ったが1人だけ100点だった。」
これで平均値が機能しないのは何となく想像できますよね？
え！？じゃあ少し実験してみましょう。
『A：100点　B：5点　Cさん：2点　Dさん:8点　Eさん：10点』
だったとします。この状況で平均値と中央値それぞれ求めると、
平均値：25点 中央値：8点
さあ、普通の人がこのテスト受けた時に取りそうな点数はどちらの値ですか？
中央値の方ですよね！
このように上のような事象では中央値の方が、私たちの持つ『平均』のイメージとなるのです。
以下のように覚えておいて下さいね！


中央値は平均値に比べ、外れ値の影響を受けいにくい



5.最大値・最小値(max・min)
では次に乗客の最年長、最年少を求めてみましょう。
# Ageの最大値と最小値を計算
max_age = df_data['Age'].max()
min_age = df_data['Age'].min()

# 結果を表示
print(""Ageの最大値:"", max_age)
print(""Ageの最小値:"", min_age)


なんか不思議な値が出てきましたね😅
おそらく、生後５ヶ月という意味なのでしょう。

6.相関係数(corr)
では集約メソッドの締めくくりに年齢（Age）と料金（Fare）の相関係数を算出しましょう。
相関係数は-1から1の値を取り、1に近いほど正の相関,-1に近いほど負の相関が強いと言えます。
よく使う指標ですので、思い出しておくと良いでしょう。
# Age列とFare列の相関係数を計算
correlation = df[['Age', 'Fare']].corr()

print(correlation)


表の見方ですが、行名と列名が異なる値がその二つの相関係数です。
今回はAgeとFareの相関係数が0.096067ですので一応正の相関ですね？
私は年配の乗船客の方が高めの運賃を払っているかと予想していましたが、
あながちそうとも言い切れない微妙な結果でした・・・😭

その他の実用的なメソッド
Pandasの文法紹介の締めくくりは、これまでで紹介できなかったデータ操作について扱います。この章もあと少しです。頑張りましょう！

1.並びかえ（ソート）
「ソート」とは、データをある基準に基づいて順番に並べ替えることを指します。これにより、データが整理され、特定のパターンや傾向を見つけやすくなります。


①要素を並べたいカラムが１つ
df.sort_values(by='要素を並べ替えたいカラム', ascending=True)
②要素を並べ替えたいカラムが複数
df.sort_values(by=['要素を並べ替えたいカラム1', '要素を並べ替えたいカラム2',・・・], ascending=True)




・昇順（1, 10, 100, 1000）の場合
ascending=True
・降順（1000, 100, 10, 1）の場合
ascending=False




複数のカラムについて並べ替える際、リスト内に記述したカラム順が優先順位となる。


dropメソッドのときと同様に、列の場合も複数選択の場合はlistで入れることを忘れないでください！
では、今回は年齢と料金の昇順ソートを行ってみましょう。
#Age列とFare列で昇順ソート（Age優先）
df.sort_values(by=['Age', 'Fare'], ascending=True)


indexが644,831のところを見てみましょう。
ここでは、Ageの昇順が優先されているので、Fareの大小が逆転していますよね？
このように複数カラムでソートする場合は、リスト内に記載する順番にも気を付けなければなりません。

2.グループ化
Pandasではグループ化をgroupbyメソッドを用いることで行うことが出来ます。
そもそもグループ化は何ぞやという人のために説明すると、
例えば『性別ごとの年齢の平均』を出したいとします。
この時の『〜ごとの、〜ごとに』をコード上で表現するために使われるのが、
groupbyメソッドです。


①単独のカラムでグループ化したい場合
df.groupby('グループ化したいカラム')
②複数のカラムでグループ化したい場合
df.groupby(['グループ化したいカラム1', 'グループ化したいカラム2',・・・])


はい、毎度恒例の複数のカラム選択の場合はリスト化ってやつですね！
今回は、乗客の性別ごとの平均年齢を出してみたいと思います。
#性別ごとの平均年齢の算出
df.groupby('Sex')['Age'].mean()


コードが少々見づらい、理解しづらいと思う方はdf.groupby('Sex')で一つの塊のdfと考えれば、納得できるのではないでしょうか？

3.カラム名の変更
元データのカラム名を変更して、より直感的にわかりやすいカラム名に変更したいことは、
しばしばあることです。そんな時はrenameメソッドを使用します。


df.rename(columns={'変更前のカラム名': '変更後のカラム名'}, inplace=True)




inplace=True:元のdfが変更され、新しいdfを作成せずに変更が反映される。
inplace=False:新しいdfが返されるが、元のdfは変更されない。


では、PclassをPassengerClassにSexをGenderに変更してみましょう。
#PclassとSexのカラム名を変更
df.rename(columns={'Pclass': 'PassengerClass', 'Sex': 'Gender'}, inplace=True)

#新しいdfの出力
df


カラム名が変更されていることが確認できますね！
inplaceのTrueとFalseはしっかりと区別しましょう。

4.locとilocを使った行・列指定
locとilocを使用すれば、より詳細に行・列の指定がしやすくなります。
locメソッド:
ラベルベースのインデックスを使用してデータを選択します。


2~5までの行とカラム１とカラム２の列を選択
df.loc[2:5, ['カラム1', 'カラム2']]


ilocメソッド:
整数ベースのインデックスを使用してデータを選択します。


2~5までの行とカラム１(１列目)とカラム２（２列目）の列を選択
df.loc[2:6, [0, 1]]


それでは、両方のメソッドを使用して「2~5行目までのPclass,Name,Ageの列」を抽出しましょう。
# locメソッドを使用して特定の行と列を選択
df.loc[2:5, ['Pclass', 'Name', 'Age']]


#ilocメソッドを使用して特定の行と列を選択
df.iloc[2:6, [2, 3, 5]]



5.queryメソッドによる抽出
この章の最後にqueryメソッドを扱います。
『条件に基づくフィルタリング』で扱った内容とゴールは同じですが、書き方が少々異なります。
SQLに慣れている人は、このメソッドの方が使いやすいのではないでしょうか？


df.query(""条件式"")


それでは、『条件に基づくフィルタリング』の時と同様に「生存した女性」をQueryメソッドを使って出してみましょう。
#queryメソッドを使った抽出(フィルタリング)
df.query(""Sex == 'female' and Survived == 1"")


ちゃんと狙い通りのフィルタリングがされていますね！
ちなみに和集合(または条件)の時はorとします。これも復習ですね？

6.文字列の数値化
この章の締めくくりに文字列の数値化を行いたいと思います。
具体的にはSexのカラムがmale→１,female→0とするわけです。これは機械学習よりの話になっていしまいますが、機械学習をさせるには読み込む特徴量は文字列ではなく数字でなければなりません。
このように文字列で表現されたカテゴリを、
値(数字)に変化することをカテゴリ変数化と言います。
from sklearn.preprocessing import LabelEncoder

# Sexを数値化
le_sex = LabelEncoder()
df_data['Sex'] = le_sex.fit_transform(df['Sex'])

# 数値化後のデータを10個表示
df_data.head(10)


性別のカラムが数字で表現されていますね！
こちらに関しては、データ分析というよりは機械学習で求められる操作ですね。
これから先のために覚えておいて下さい😎

第３章：データのグラフ化

下準備
この章から遂にデータの可視化に入っていきます。
以下のライブラリをインポートしておきましょう！
import seaborn as sns
import matplotlib.pyplot as plt



matplotlib:
Pythonのグラフの描画ためのライブラリです。Pandasだけでも簡単な描画はできますが、matplotlibを使うとより柔軟なグラフ描画が可能になります。
seaborn:
matplotlibをベースとした、データ可視化ライブラリです。matplotlib単体よりも、さらに豊富なパターンの可視化機能を有しており、かつ美しく鮮明な見た目も特徴です。


これらのライブラリを組み合わせて使用することで、データの可視化や解析を効果的かつ美しく行うことができます🤩

棒グラフ
まず最初は皆さんお馴染みの棒グラフです。
カテゴリー毎の数量の比較や時系列データなど幅広く活躍してくるグラフです。
今回は乗客の社会経済的地位を示すクラス（Pclass）を横軸、人数を縦軸として出力してみます。
# Pclassごとの乗客数を取得
pclass_counts = df_data['Pclass'].value_counts()

# 棒グラフの作成
plt.figure(figsize=(8, 6))
sns.barplot(x=pclass_counts.index, y=pclass_counts.values, palette=""viridis"")
plt.title('Passenger Count by Pclass')

#軸のタイトルを命名
plt.xlabel('Pclass')
plt.ylabel('Passenger Count')

#棒グラフ出力
plt.show()




長所
・視覚的な理解が容易　・異なるカテゴリー間の比較が容易　
・時系列データにも使える




短所
・比率の情報が薄い　・連続データには向かない　・大量のデータには向かない


使用例としては、『各地域の年間降水量の比較』や『異なるブランドの製品売上の比較』などのカテゴリー分けされ且つ絶対量で見たいものに最適です。

ヒストグラム
ヒストグラムは度数分布表の度数を縦軸、階級を横軸に持ってきたものです。
では横軸を年齢、縦軸を人数にとして出力してみましょう。
# Ageのヒストグラムを作成
#　サイズを指定
plt.figure(figsize=(8, 6))
sns.histplot(df_data['Age'].dropna(), bins=30, kde=False, color='skyblue')
plt.title('Histogram of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()




長所
・分布の理解が容易　・中央傾向が把握しやすい　・異常値の発見が容易




短所
・階級の選択の影響を受ける　・個々データの損失　・外れ値への敏感性


使用例としては、『人口の年齢分布』や『製品の品質管理における寸法のばらつき』などの、データに幅を持たせての""小さいグループ(階級)""ごとの比較に便利です。

円グラフ
こちらもお馴染みのグラフですね。
直感的にわかりやすい一方で、項目（カテゴリー）が増えると非常に見辛くなりますから注意です。
では、男女の比率を円グラフで見てみましょう。
# Sexごとのデータを集計
sex_counts = df_data['Sex'].value_counts()

# 円グラフの作成
plt.figure(figsize=(4, 4))
plt.pie(sex_counts, labels=sex_counts.index, autopct='%1.1f%%', startangle=90, colors=['skyblue', 'lightcoral'])
#タイトルの命名
plt.title('Passenger Distribution by Sex')
plt.show()




長所
・直感的に理解できる　・比率が瞬時に読み取れる




短所
・絶対量の情報が消える　・視覚的に項目数に限度がある　
・不連続データに向かない ・僅かな比率の差が捉えにくい


使用例としては、『企業の売上高における各製品ラインの割合』や『市場調査での消費者の好みの分布』などの少数カテゴリー分類されるものに最適です。

箱ひげ図
なんか響きが懐かしいですよね箱ひげ図。。
私は高校の先生に、箱から髭が生えているから「箱ひげ図」なんだと教わりましたが、縦線が線が全て髭なんだみたいな教わり方もあるようです。
見方としては、縦線が重要です。縦線を左から見ていって、
1本目：最小値　2本目：第一四分位数　
3本目：中央値　4本目：第三四分位数　5本目：最大値
ヒストグラムなどを描かずとも、山がどの辺りで膨らんでいるのか把握できます。
それでは、箱ひげ図で年齢の分布を見ていきましょう。
# Ageの箱ひげ図を作成
plt.figure(figsize=(8, 3))
sns.boxplot(x='Age', data=df, orient='h', color='skyblue')
#タイトルの命名
plt.title('Boxplot of Age')
plt.show()




長所
・複数のデータセットの比較に最適　・分散, 外れ値, 中央値を示せる




短所
・前提知識を必要とする　・平均値や最頻値は直接示せない


出力結果に関してですが、最大値を示す右側にプロットされている⚪︎は外れ値を示しています。
使用例としては、『異なるクラスの試験成績の比較』や『複数地域の気温変動の比較』などの異なるグループ間の比較に最適です。

散布図
続いては散布図です。
集約メソッドの所で扱った相関係数と一緒に使われるグラフです。つまりこのグラフは二つの変数の相関関係を調べるときに役立ちます。
では、横軸を年齢, 縦軸を乗船料金として散布図を描いてみましょう。
# AgeとFareの散布図を作成
plt.figure(figsize=(8, 6))
sns.scatterplot(x='Age', y='Fare', data=df, color='coral', alpha=0.5)
plt.title('Scatter plot of Age vs Fare')
plt.xlabel('Age')
plt.ylabel('Fare')
plt.show()




長所
・二つの変数の関係が視覚的に理解できる　
・データのパターンや傾向を把握しやすい




短所
・三つ以上の変数を扱うのが難しい　・データのプロットが重なると読みづらい


今回、「若い人ほど安いチケットを買って、年配の人ほど高いチケットが買える」という正の相関を予測して、カラム選択を行いましたが、ちょっと予測が外れてしまいましたね😭
集約メソッドの相関係数のところで紹介しましたが、正の相関が強い程データのプロットは傾きが１の直線上に並び、負の相関が強い程傾きが−１の直線上に並びます！

散布図行列
さあ、やってきました最後の学習項目です！ここまで一気に通した方はお疲れかと思いますが、本当に最後です！私もこれまで通り、気合を入れて書いておりますので、最後の最後までお付き合いよろしくお願いしますm(__)m
散布図の短所のところで、『三つ以上の変数を扱うのが難しい』と言いました。確かに、全ての変数に対して散布図を出力するのはあまりに面倒過ぎますよね？途中で「あれ？何やってっけ？」となるのが目に見えてますね？笑
そんな時に使える方法をご紹介したいと思います！以下のように入力してみましょう！
# 散布図行列を作成
sns.pairplot(df)
plt.show()


なんか凄いですよね笑笑
見てわかる通り、全てのカラムの組み合わせによる散布図がこんなに短い手順で出力されてしまいました！！
これを使えば、今まで気付けなかった意外な相関関係にも気付けそうですよね👍

第４章：最後に
いかがだったでしょうか？
Pandasは結構直感的になんとなく理解しやすいものですので、もういけそうな感じがしませんか？
今回の記事でPandasの全てのメソッドを扱ったわけではありませんが、これから自学自習するフェーズにおいて、何も手が出せないということはなくなったはずです。あとは、皆さんが各々知識の補填をしていくことで、本当にPandasをものにしたと言えます。
次は実際にKaggleでタイタニック号の生存予測のモデルを組んでも良いですし、統計学をPythonを使いながら学んでいっても良いでしょう。
どの道にしろ、やっぱり手を動かすことが大事です！
自分の手で「この部分だけ実行したらどうなるんだ？」とか「ここを変えたらどうなる？」とある種の実験をしていきます。そうすると「あ！こういうことかも！」と、またある種の仮説が出来ます。その仮説が芯を食う場合もあれば、また他の混乱を呼ぶこともあります。しかし、この過程を踏むことが何だかんだ近道であると私は思います。ですから、暗記暗記と考えず「まぁそのうち覚えるでしょ」という楽な感覚で取り組んでみてください。
ここまでお付き合い頂き、ありがとうございました。


弊社Nucoでは、他にも様々なお役立ち記事を公開しています。よかったら、Organizationのページも覗いてみてください。
また、Nucoでは一緒に働く仲間も募集しています！興味をお持ちいただける方は、こちらまで。





"
https://qiita.com/rubytail/items/6ad1e658d8f4093d831e,['UNIXコマンド'],"
はじめに
昔の自分がやらかした過ちをここらへんで懺悔しておきます。
願わくば同じ失敗をしてしまう人が少しでも減りますように。
「いやこんなコマンド打たんだろ…」と思いますよね。なんか流れでやっちゃう時があるんです。ホントに。


【第１章】不穏な依頼
もう何年前になるでしょう。IT業界に就職して数年、とあるUNIX系OSで管理系機能はそれなりに理解し、開発にも操作にもすっかり慣れてきて、ちょっとした自信もついてきた頃のことでした。
先輩：「わたし君、明日お客さんセンターに搬送するマシンで最後にちょっとしたテストしたいんだ。やっといてくれないかな。オレ今日はもう用事があって。」
わたし君：「あ、いっすよ。どこに置きました？」
先輩：「作業ディレクトリ消しちゃった後だから、/直下にtest.tmpって作ってftpで入れてあるから実行権限付けて動かしてみて。じゃあ、お先～」
わたし君：「（ええ、/test.tmpってw・・・まあいいか）」


【第２章】破滅への第一歩・rootを使用
状況を確認。
# こんな出力でしたーというイメージです
#
[watashi@host1 test.tmp]$ ls -lR
.:
合計 0
-rw-------. 1 ftpusr ftpusr  0 12月 20 23:40 test_main.sh
drwxrwxrwx. 2 ftpusr ftpusr 48 12月 20 23:41 test_sub1
drwxrwxrwx. 2 ftpusr ftpusr 48 12月 20 23:41 test_sub2
drwxrwxrwx. 2 ftpusr ftpusr 48 12月 20 23:42 test_sub3
：
：
./test_sub1:
合計 0
-rw-------. 1 ftpusr ftpusr 0 12月 20 23:41 test_sub1.sh

./test_sub2:
合計 0
-rw-------. 1 ftpusr ftpusr 0 12月 20 23:41 test_sub2.sh
：
：

わたし君：「あー、ftpusr:ftpusrで600か。実行するならファイルのownerもwatashiにしたいし、rootでオーナー変更して実行権限も付けるか」
[watashi@host1 test.tmp]$ su -
パスワード:
[root@host1 ~]#
[root@host1 ~]# cd /test.tmp
[root@host1 test.tmp]#



【第３章】破滅への第二歩・再帰処理
わたし君：「サブディレクトリあるから-R付けなきゃ」
[root@host1 test.tmp]# chmod -R a+x *.sh
[root@host1 test.tmp]# chown watashi:watashi -R *

わたし君：「よし実行っと」
[root@host1 test.tmp]# ./test_main.sh
エラー：設定ファイル .test_main.ini を読めません
[root@host1 test.tmp]# ll .test_main.ini
-rw-------. 1 ftpusr ftpusr 0 12月 20 23:41 .test_main.ini
[root@host1 test.tmp]#

わたし君：「は？ "".test_main.ini"" とか何で設定ファイルを隠しにしてんだよ。*でヒットしないからftpusrのままじゃん。
もう面倒だからこれで全設定ファイル直そ。」
# 非常に危険な処理です！ダメ！絶対！
#
[root@host1 test.tmp]# chown watashi:watashi -R .*

そして流れるように出力される大量のエラー。しかもやたら時間がかかってる。なにこれ怖い。
もう一個ログインセッションがあるから覗いてみるか。


【終章】そしてすべてが私になった
# こんな出力でしたーというイメージです
#
[watashi@host1 ~]$ ls -l /
合計 28
drwxrwxrwx.   3 watashi watashi   18 12月  6 16:35 adm
lrwxrwxrwx.   1 watashi watashi    7  6月 21  2021 bin -> usr/bin
dr-xr-xr-x.   5 watashi watashi 4096 10月 16 16:52 boot
drwxr-xr-x.  20 watashi watashi 3180 12月 20 23:31 dev
drwxr-xr-x. 144 watashi watashi 8192 12月 20 23:39 etc
drwxr-xr-x.  14 watashi watashi  185 12月 20 23:39 home
lrwxrwxrwx.   1 watashi watashi    7  6月 21  2021 lib -> usr/lib
lrwxrwxrwx.   1 watashi watashi    9  6月 21  2021 lib64 -> usr/lib64
drwxr-xr-x.   2 watashi watashi    6  6月 21  2021 media
drwxr-xr-x.   4 watashi watashi   28 10月 27 13:32 opt
：
：

わたし君：「ちょ、え、何これ？全部のファイルのオーナーがwatashiになってる？なんで？…」
賢い諸氏はお分かりでしょうが解説します。
わたし君はドットファイルをヒットさせる為に、よく考えずに次のワイルドカードを使った。
.*

確かにドットファイルはヒットする。しかし、当然ながら次のパスがヒットする。
..

親ディレクトリ………そしていま作業している /test.tmp の親ディレクトリは /
つまりrootで以下のコマンドを打ったのと同じ結果となる。
[root@host1 test.tmp]# chown watashi:watashi -R /

/ 以下を再帰処理で全ファイルの所有者を一般ユーザwatashiに変更…
そして、すべてのファイルが私(watashi)のものになった…

その後
その後、システムは丸っと動かなくなりました。セッションは暫くして切れ、もうログインも出来ません。ローカルターミナルもダメ。再起動してももう正常には起動しません。
そこから、OSのシステムファイルをインストールメディアから上書きインストール。デプロイした各種ファイルやミドルウェアのオーナー設計を設計書から確認して手で復旧。そして、全ファイルシステムに渡る私(watashi)探しの旅を終えて、OS、ミドルウェア、アプリの全正常起動を確認した時には、もう明け方でした。

教訓
root使ってるときはコマンド打つ前に一拍考えろ
"
https://qiita.com/TokomaBaou/items/06fca9c5abfc3460c587,"['Design', '音楽', 'ポエム', 'React', 'エンジニア']","📣：プログラミングを学び始めたばかりの人や、学習や業務を続ける上でモチベーションを探している人に向けて書きました ✍️

はじめに🖐️
まずは、とある２人の話から

オードリー若林は、ASIAN KUNG-FU GENERATIONの『ブラックアウト』のギターリフを聴き、その繰り返しの中に新たなインスピレーションを見出しました。
この瞬間は、同じボケを何度も繰り返しても良いという彼らの漫才スタイルの原点になったのです。
人工知能の父と言われるマービン・ミンスキーの名言：

""2つ以上の方法で学ばなければ何かを理解することはできない""。
（You don't understand anything until you learn it more than one way.）

「俺には関係ないかな」と思った方！まだ離脱しないでください！
この考えは、異なる分野からの知識がいかに私たちの理解を豊かにするかを表しているんです。
この記事では、
芸術と技術がどう絡み合い、互いにどう影響しあってるか、そしてそれが私たちのプログラミングの学びにどう役立つか、どう手助けになったかを書いていきます。

免責事項
あくまで私見です。
少し強引さが伺えるかもしれません。


目的 🧠

芸術と技術の分野がどのように相互に影響し合い、共通の原則を共有しているかを探っていきます。


背景 🌅

私も実際に、中学から現在に至るまで音楽を作ったり、学生の頃デザインを専攻した経験からプログラミングの学習の際に何か類似点を感じ、どのように学習や実践に役立ったかの個人的な視点を言語化したいなと感じたことが発端です。


1. 共通の原則 📝
芸術と技術は、それぞれ異なる分野のように思えますが、実は共通する原則や考え方を持っています。
改めてそれぞれの原則を挙げてみると...

『プログラミングの原則』

1. 上から下(線形):
　処理の流れは直線であり、順序立てて記述する。
例) 関数やクラスを組み合わせて、複雑な処理を分割し、順序立てて記述することで、プログラムの理解しやすさと可読性を高めることができる

example.
// 左から右に表現されることも同様

// 関数
function add(a, b) {
  return a + b;
}

// コンポーネント
const Counter = () => {
  const [count, setCount] = useState(0);

  return (
    <div>
      <button onClick={() => setCount(count + 1)}>+</button>
      <button onClick={() => setCount(count - 1)}>-</button>
      <h1>{count}</h1>
    </div>
  );
};



2.分岐:
条件に応じて異なる処理を行う。
例) if文やswitch文を使用して、条件に応じて異なる処理を行うことで、プログラムの処理を柔軟に制御することができる

example2.

const App = () => {
  const age = 21;

  if (age >= 20) {
    return <h1>あなたは成年です。</h1>;
  } else {
    return <h1>あなたは未成年です。</h1>;
  }
}




3. 繰り返し:
同じ処理を条件に応じて繰り返す。
例) for文やmapを使用して、同じ処理を繰り返すことで、処理を効率化したり、統一感や連続性を表現したりすることができる

example3.
const App = () => {
  const names = [""山田太郎"", ""鈴木次郎"", ""佐藤三郎""];

  return (
    <div>
      {names.map((name) => (
        <h1>{name}</h1>
      ))}
    </div>
  );
}



『音楽の３原則 (3大要素)』

1. メロディー:
旋律は音楽の核となる要素。音の高低・長短の変化の連続した流れ。

2. リズム:
律動は音楽の時間的な構造を形作る。拍子・ビート、一定周期またはパターンの音。

3. ハーモニー:
和声は異なる音の組み合わせによって深みを加える。複数の高さの音が重なり合いながら変化し、進行していく。
(詳しい例は以下をご覧ください👇)




『デザインの４大原則 (グラフィック)』

1. 近接:
関連する要素をまとめる。

2. 整列:
要素を規則正しく配置する。

3. コントラスト:
情報に強弱をつける。

4. 反復:
同じパターンを繰り返す。
(詳しい例は以下をご覧ください👇)




共通点の探求🧐
これらの原則は、それぞれの分野で
「構造」、「流れ」、「調和」、「バランス」といった共通のテーマを持っています。
プログラミングの 「上から下」 は、音楽のリズムやデザインの整列原則と同様に、作品の構造と流れを形成する重要な要素です。処理の流れを順序立てて記述することで、プログラムの理解しやすさと可読性を高めることができます。
同様に、プログラミングの分岐や繰り返しは、音楽のハーモニーやメロディー、デザインのコントラストや反復と共鳴します。
「分岐」 は、音楽の「リズム」や「ハーモニー」のように、表現が変化する原則です。プログラミングでは、条件分岐や繰り返し処理によって、プログラムの流れに変化を加えることで、表現が変化しプログラムの効果を高めることができます。音楽では、リズムやハーモニーの変化によって、音楽のテンポ感や雰囲気などを柔軟に変えることができます。
「繰り返し」 は、デザインの「反復」や音楽の「メロディ」のように、統一感や連続性を表現する原則です。プログラミングでは、繰り返し処理によって、プログラムの処理を効率化したり、統一感や連続性を表現したりすることができます。
これらの原則は、異なる分野においても創造的な思考や問題解決のプロセスを支える基盤となっているのです。

具体的な例💡

音楽のコード進行とプログラミングの制御構造の類似性
コード進行1は、音の組み合わせによって音楽の感情や雰囲気を表現します。
例えば、ハ長調のⅠ-Ⅴ-Ⅵ-Ⅳというコード進行は、明るく楽しい雰囲気を表現します。
Ⅰ-Ⅴ-Ⅵ-Ⅳの解説 💬
このコード進行は、Ⅰ（トニック）の音が安定感を与え、Ⅴ（ドミナント）の音が緊張感を与え、Ⅵ（サブドミナント）の音が緩和感を与え、Ⅳ（サブドミナントの５度下）の音が安定感を与えることで、明るく楽しい雰囲気を表現しています。

👇 Ⅰ-Ⅴ-Ⅵ-Ⅳ進行の例



同様に、プログラミングの制御構造も、条件分岐や繰り返しなどによって、プログラムの流れや挙動を表現します。
例えば、if-else文は、条件によって処理を分岐させる。while文は、条件が満たされるまで繰り返し処理を行います。
このように、コード進行とプログラミングの制御構造は、どちらも組み合わせによって、感情や雰囲気、流れや挙動を表現するという共通点があります。

デザインのレイアウトとプログラミングの構造設計の類似性
デザインのレイアウトは、要素の配置によって視覚的な印象や効果を表現します。例えば、水平方向に要素を配置すると、安定感や安心感を与えます。同様に、プログラミングの構造設計も、要素の配置によってプログラムの可読性や保守性を高めます。例えば、モジュール化によって、プログラムを小さな単位に分割すると、各単位の役割を明確にし、変更や拡張が容易になります。




コンポーネント指向の考え方
近年のプログラミング界では、Reactのようなライブラリやフレームワークが重要な役割を果たしています。Reactは、コンポーネントベースのアーキテクチャを提供することで、開発者が複雑な処理を隠蔽し、再利用可能なパーツとして効率的に扱えるようにします。さらに部品ごとに小さく分けることにより、開発者はパズルを組み替えるようにしてアプリケーションを構築することができ、開発の効率化や品質向上が実現されます。

 example4.
function Video({ video }) {
  return (
    <div>
      <Thumbnail video={video} />
      <a href={video.url}>
        <h3>{video.title}</h3>
        <p>{video.description}</p>
      </a>
      <LikeButton video={video} />
    </div>
  );
}


(引用：コンポーネントからユーザインターフェースを作成)



音楽においても同じようにパーツを使い回したり、パズルを組み替える感覚で作曲をすることができます。DAW2では、音源やエフェクトなどの素材を組み合わせて、曲を作成することができます。
例えば、ドラム音源やベース音源などの素材を組み合わせることで、簡単にリズムトラックを作成することができます。また、ギターやピアノなどの音源を組み合わせることで、メロディを作成することができます。さらに、エフェクトを組み合わせることで、音色や音質を調整することができます。


2. 音楽制作からの学び🎹

目先のテクニックよりも構造に目が向いた
楽曲制作をしていると、特に全体を把握するという考え方が身についていることが多いです。音楽を作る過程で、徐々に演奏技術を超え、曲全体の構造に焦点を当てるようになります。
たとえば、一流のシンガーやギタリストの技術を真似ることから始めても、実際に自分の曲を作ることで、メロディー、リズム、ハーモニーといった要素のバランスや組み合わせに注意を払うようになります。これは、各要素がどのように全体に貢献するかを理解することで、より洗練された音楽を作るための重要なステップです。
これは、プログラミングの学習や実践においても、テクニックにとらわれず、全体の構造を理解することに注力することで、より効率的かつ効果的なプログラムを作成することにもつながります。

3. プログラミング学習への応用🧑‍💻

まずは全体の構造を理解すること
私自身、新しいフレームワークなどを勉強する際も大切にしているのが、全体を把握してから細かい部分を理解するという考え方です。
初学者の頃は、デザインの経験でUIの観点が頭の片隅にあったため、HTMLやCSSを学ぶ時は構造からすんなり頭に入ってきましたが、そういったコーディングテクニックを超えて、アプリケーション全体の構造を理解することが重要です。
例えば、初心者の時には、チュートリアルを参考にしながら、一つのWebアプリを丸々完成させることが推奨されます。
これにより、単にコードを書くことだけでなく、アプリケーションがどのように機能するか、各部分がどのように連携するかを理解することができます。全体の流れを捉えることで、より効果的なプログラムを設計し、開発する能力が養われます。

学習の助けとなる視点 👀:

要素の組み合わせ：
特に抽象化、モジュール化、など、Reactの学習においての理解に役立ちました。
音楽では、コード進行やメロディーなどの要素を組み合わせて、音楽全体の雰囲気や印象を表現します。同様に、プログラミングでは、変数や関数などの要素を組み合わせて、プログラム全体の構造を理解することが重要です。

先ほどの例にも出したようにReactの設計思想や構造は、コンポーネント指向を体現しています。Reactでは、コンポーネントと呼ばれる小さな単位を組み合わせて、画面全体を構築します。このコンポーネントは、それぞれが独立した役割を持っており、全体の構造を意識して設計することで、複雑な画面でもわかりやすく、保守性の高いプログラムを作成することができます。




4. 相互作用・影響 👊

音楽経験による問題解決能力の向上
音楽制作とプログラミングの分野は、異なる表現形式を持ちながらも、目標達成のための問題解決プロセスという共通の原則を共有しています。音楽制作では、曲を作る際に特定のイメージや目標があり、作曲家やアーティストはこの目標に到達するために楽曲をアレンジし、構築します。例えば、特定の感情を表現するために適切なメロディーやハーモニーを選び、聴衆に特定の反応を引き出すためにリズムやダイナミクスを調整します。
一方、プログラミングでは、特定の要件を満たすソフトウェアやアプリケーションの開発が目標です。開発者は、これらの要件を実現するために最適な技術的解決策を見つける必要があり、ユーザーフレンドリーなインターフェースの構築や効率的なコードの記述などが求められます。

デザインとプログラミングにおける情報整理と優先順位付け
デザインでは、要素の配置や色使いによって視覚的な印象や効果を表現します。ここで重要なのは、どの情報に強調を置き、それをどのように視覚的に表現するかです。対照的に、プログラミングではコード内の情報を整理し、各要素の役割や関係性を明確にすることが重要です。これにより、コードの可読性と保守性が向上します。
これらの分野では、情報を整理し優先順位を付けるプロセスが重要です。どちらの分野でも、クリアで整理された構造は作品の理解と操作を容易にし、効率的な問題解決を促進します。このアプローチは、視覚的な表現と技術的な実装の両方において、より効果的でユーザーフレンドリーな結果を生み出す基盤となります。
私は実際に、コードを書く前に設計として簡単に絵や図にして紙に書き出したりする習慣があるのですが、学生時代の経験が活きているなと実感します。

5. まとめ 🤲
クリエイティブな経験がプログラミングの学習と実践に与える影響の重要性を再確認できました。
芸術と技術は、それぞれ異なる分野のように思われますが、共通する原則や考え方を持っています。個人的には芸術の経験は、プログラミングの学習と実践において、問題解決能力の向上、コード設計の改善、情報整理力の向上、優先順位を付ける習慣付けなど、さまざまなメリットをもたらしました。
プログラミングを学習している人は、ぜひ自分の好きな分野との共通点を探し、その知識や経験をプログラミングに活かしてみて欲しいです。

6. おわりに🦶
日々のコーディングを仕事として割り切るのもありですが、どうせやっているなら楽しんでプログラミングをすることで日々のスキルアップであったり、ライスワークの充実に繋がるはずなんです。もし、今後別の職業に就く場合でもこの考えが皆さんの役に立てばとても嬉しく思います。

参考/引用
プリンシプル オブ プログラミング 3年目までに身につけたい 一生役立つ101の原理原則 - 上田勲 (著)













脚注



音楽において、ひとつの音のことを「単音」と呼びます。そして高さの異なるふたつ以上の音が重なった状態を「コード（和音）」といい、コードの流れを示すのが「コード進行」です。 ↩


Digital Audio Workstation(デジタルオーディオワークステーション)を略した単語で、通称「ダウ」と発言したりします。 ↩



"
https://qiita.com/nakampany/items/910e17dcbf1f4ef19e44,"['初心者', 'ポエム', 'Slack', 'アウトプット', 'AdventCalendar2023']","
はじめに
こんにちは、なかじです！
自分はアウトプットを始めて、もう少しで半年が経とうとし、投稿数が10記事を超えようとしています！
なんとなく始めた記事執筆ですが、初めはエンジニアだからアウトプットするもんだと思っていました。
メリットも知らずに記事執筆を始めましたが、実際に投稿を続けて記事執筆をしてすごくよかったなと感じています！
今回の記事は、
『若手エンジニアがアウトプットする意味』
『メリットってあるのかな？』
について自分の考えを紹介し、若手エンジニアのアウトプットのモチベがあがるような記事にしたいと思います！

対象読者

若手エンジニア
アウトプットをしたことがないエンジニア
アドベントカレンダーに参加したことがないエンジニア


若手エンジニアがアウトプットするメリット
若手エンジニアがアウトプットする意味について、自分が思うメリットは7つです！


1. アウトプットが名刺の代わりになる
2. セルフブランディング・成長の記録
3. 言語化能力が向上する
4. 得意を見つけることができる
5. 小さな成功体験を積むことができる
6. 評価される場が多い
7. 組織の文化形成



1️⃣ アウトプットが名刺の代わりになる
自分は、まだプログラミングを始めて1年半のインターン学生です。なので、自己紹介の時に、大学でやっている研究や趣味しか話せることがないです。特に、自分は個人開発で目立ったアプリも作ったことなければ、これまでですごい経歴があるわけでもないです。
しかし、その自己紹介の内容をすっ飛ばして、自分というものを知ってもらうために、記事（アウトプット）は効果をすごく発揮します！
例えば、『記事執筆（アウトプット）頑張っています！』と言って、自分のこれまで書いた記事（いいねが一番多い記事でもなんでもOK）のリンクを共有したところ、『この記事見たことあります！書いたのなかじさんだったんですね！』や『面白い記事ですね！LGTMしときました！』と言われるとてもとても嬉しいことが度々ありました。

エンジニアとしてごく普通な自分に、アウトプットの内容で自分の人柄やどんな技術が好きでを知ってもらえるのは最高に素敵なことですよね？
そういう面でも、アウトプットに力を入れている人とアウトプットにあまり力を入れていない人では、人間関係のスタートダッシュでちょっとした差がつくかもしれないです！

2️⃣ セルフブランディング・成長の記録
「1. アウトプットが名刺になる」にも少し関連しますが、アウトプットすることで、自分の積み上げてきたもの、これまでやってきたことがアウトプットというモノで、個人資産になります！就職活動や、これからの転職活動の際の実績としても、判断されることだってあると思います！
また、2, 3年後、新卒という立場ではなくなった時、自分の溜めてきたアウトプットたちが、背中を押してくれる時があるかもしれないです！『あの時の自分はこんこともできなかったけど、今は開発チームを引っ張ってるんだぜ』や『あのときは、こんな考え持っていたな』とか、エモい感情になれることかもしれないですね！
自分は今でも、以前に書いた記事（ちょっとした失敗談について）などを見ると、その当時頑張っていた光景がいまでも鮮明に浮かびます〜w
しかし、セルフブランディングの大前提として、他人に評価されるには、一定質の高いアウトプットが要求されると思います！
しかし、記事はターゲットを決め、読者目線の文章かつ軸のぶれない戦略的アウトプットをしていれば、絶対あなたの記事は評価されると思っています！（自分がプチバズり記事を書けるのだから）
ご参考までに、自分は、若手エンジニア向けで、自分の考えや失敗談、技術系で言えばGo言語のアウトプットをしています！（少し上から目線ですみません🙇）

3️⃣ 言語化能力が向上する
人に伝えることが得意ではなく、うまく説明できないと感じている方はいますか？自分はすくなくとも得意とは言えないです！例えば、自分はあの時なんでうまくいったんだろうやなんで失敗したんだろうと思った時に、いつも言語化できておらず、感覚やなんとなくで片付ける人間でした。今でこそ、とくに失敗ややらかしてしまった時には、言語化するために、アウトプットするように心がけていますが、、、
記事を書くことで、体験したことや理解した技術を、読者に向けて読みやすく言語化したものを発信しているので、その分野・技術について精通することはもちろん、他者に伝える能力も養うことができます！
また、LTなどで人前で話すことも、当たり前ですが、言語化能力を高めることができます！
直近では、10月に社内でのLT会に登壇しました。そのLTの内容は、どういうエンジニアになりたいかという内容でしたが、その一回のLTだけでも、自分のキャリアについての目標ができましたし、その思い描いている自分に最短でなるにはどういう行動をしたら良いかを考え、他人に自信を持って説明できるようになりました。
LTは、一回一回のハードルが高いですが、言語化は登壇する前とした後で全く違います！

4️⃣ 得意を見つけることができる
以前、社内のBizの若手役員のTさんとのインタビューで、このようなアドバイスされたことがあります。

自分　 「Tさんは、新卒時代をどういうマインドで過ごしていましたか？」❓
Tさん 「苦手を作らないことを意識してましたねー」💪
自分　 「なるほど。」🤔
Tさん　「『得意を伸ばすこと』と、『苦手をやらないこと』は違う意味で、その得意なことってどんどんやっていったほうがいいし、伸ばしたほうがいいんですけど、まず大前提20代なんで、苦手と決めちゃったら、向こう仕事人生20年は、苦手というジャンルに入るじゃないですか。」🔥
自分　 「なるほど。」🧐
Tさん 「まだ経験がないことが大半なんで、『得意』と『苦手』とかじゃなくて、『得意』と『未知』に分けるのは結構大事だとおもってますね!」🙆
自分　 「はい！」🫡
Tさん「やってみないとわからないので、一旦全部『未知』の状態におく。同じことを5年やって苦手だったら、苦手と思っていいんですけど、数回やって苦手って当たり前なんですよね。『得意』じゃないから。」🚀

得意を見つけることは、アウトプットの姿勢にも、当てはめることができると思います！
文章を書くことや話すこと書くことが苦手で、モチベが続かないから、ブログ執筆や登壇を避けているという同年代の方をよく見かけます。自分も、どっちかといえば、元々苦手を決めて、得意を伸ばす側の人間だったので、気持ちはすごくわかります！ちなみに、直近の自分の場合は、フロントエンドが苦手だから、得意と感じるバックエンドの開発しかやらないの人間でした。w
そのアドバイスをもらってから、苦手を決めることに対してすごく勿体無いと思うようになり、ものごと全般に対して、『苦手』という分類に入れるのではなく、『未知』だから、とりあえずやってみるという考えができるようになりました！
その過程を経て、文章を書くのをそこそこ自分自身の得意にできたのも積極的アウトプットをしたおかげかなと思っています！

5️⃣ 小さな成功体験を積むことができる
皆さんは、成功体験積んでますか？いきなりぶっ飛んだ成功を掴もうとしていませんか？地に足をつけて、「小さな成功体験を積み自信がつく」を繰り返した先に、大きな成功体験がつめると考えています！
その小さな成功体験を積み、自信をつけるため、記事執筆はおすすめです！
初めから、毎日投稿やいいね数を稼いで多くの人に評価されたいを目指すのは、おすすめできないです！自分に合った目標で、1つずつ成功体験を積み自信をつけるのに、アウトプットはおすすめです！
しかし、がんばって記事を書いても、誰にも見てもらえないことがあるかもです。でも、そういう日もあると広い心を持ちましょう！全部が全部自分の思い通りになったら、面白くないだろうのスタンスで！！！

6️⃣ 評価される場が多い
若手は、アウトプットするだけで、評価される場が多いです！
自分の体験で言えば、Qiita Engineer Festa 2023でのいいね数は社内で三番目ぐらいでしたが、その時が初めての記事投稿であったため評価されたことがあります！

自分の自己満足でアウトプットした記事が他人から評価されるという、これほど嬉しいことはないです！！！
また、社内でも、多くの方がアウトプットに対して反応をしてくれます！温かい人たちばかりで、自分の記事かのように、喜んでいただき、とても嬉しい気持ちになります！
その他で言えば、自分の記事ではないですが、以下の記事たちはすごい盛り上がりでしたw




7️⃣ 組織の文化形成
一言で言えば、若手が積極的な発信をすることで、組織全体がアウトプットの文化が根付くということです！
その例としては、アドベントカレンダーがすごくいい例です！
ちなみに、Qiita Engineer Festa 2023でも、別のインターン生が盛り上げ役をやっていました！！！
HRBrainは、1年前のアドベントカレンダーから本格的にテックブログやイベントに力を入れ始め、Qiita Engineer Festa 2023やAdvent Calendar 2023では若手を中心に積極的に発信しました。
若手がファーストペンギンとなり積極的にアウトプットすることでハードルを下げ、Bizも含めてエンジニア以外からも発信する流れができ、会社全体として接極的に発信する文化が形成されました！
その結果、組織全体の効果としては、「HRBrainさん技術発信すごいっすね！」と言われることがかなり増え、認知の面で、すごく大きな影響を与えたと思っています！（自分で言うのもなんですがw）
以下は、Slackの『記事書こうよ！』という全力盛り上げアナウンスです！www


次の若手盛り上げ隊はだれかな〜😏😏😏

🔥 この記事を見たあなたへ初めの１歩
不器用でもいいから、とにかくアウトプットをしてみる！！！
そのアウトプットが、自分自身はもちろん、周りの人、組織に良いメリットを与えます！！！

おわりに
これを見て、アウトプットする若手エンジニアが増えるといいな〜
また次回の記事でお会いしましょう👋

PR
HRBrainでは一緒に働いてくれる仲間を募集しています！😁



"
https://qiita.com/miguchi_ako/items/f25767b6c941f45580e8,['初心者'],"初めまして！！
株式会社VISIONARY　JAPAN営業の三口と申します
実は、私の夫はインフラエンジニアとして自社開発の会社で働いております。
何年も一緒にいますが、
正直仕事内容は良くわかっていません…。
そこで、夫に仕事内容を教えてもらい、
勉強してみます
夫のプロフィール:
・文系大学卒業（英語専攻）
・21卒プログラミング未経験でSES企業へ
・就活中にRubyを使ってアプリを自己学習しながら作っていた。
・テレビ局とユニクロでバイトしていた。（私が学生時代働いていたスタバの常連客）
①仕事内容は？
自社の製品のシステムの基盤を設計・構築・運用・保守を行います。
毎日作業内容は違いますが、リファクタリングがメインです。
リファクタリングとは、サービスの欠陥などを改善することです。
バージョンアップなどを行います。
あとは、自社のサービスを運営するための基盤を構築しています。
②主に扱っているものは？？
バージョンアップにはAWSの機能を用いたり、Ansibleを使います。
最近導入したNew Relicもよく使います。
お客さまに安定したサービスを提供するために、パフォーマンスを日々New Relicで監視します。
New Relicは導入時担当させてもらっているので、インフラチームの中でも自分がメインで触っています。
③なぜインフラエンジニアになったの？
もともと就活生の頃は開発エンジニアになりたかったけど、
新卒で入社したSES企業の社長からインフラやらない？と誘われてインフラの運用・保守案件に参画することになり、
そこからインフラエンジニアになりました。
④インフラエンジニアの楽しさややりがいは？
開発エンジニア達がより良いサービスを提供するためのお手伝いができる。
直接お客様へサービスを提供できるエンジニアではないけど、日々の業務を自動化したり、自分が構築したシステムで誰かのためになるのが楽しいです。
⑤今なにを勉強しているの？
GitHub ActionsとKubernetesです。
GitHub Actionsは主にアプリのデプロイを自動化するために使っています。
TerraformもAWSなどの構築をコードで管理できる様にするために、日々勉強しています。
⑥インフラエンジニアの大変なところは？
インフラというより、今自分の所属してる部署は自社サービスのリソースを全て管理しているので、かなり大変です。
運用保守案件に入っていた頃は、夜間対応があったり、エラー対応が急に発生していたので、大変でした。
⑦インフラエンジニアはどんな人が向いてると思う？
業務の効率化や仕組みづくりを考えるのが好きな人が向いている気がする…。
⑧資格は何を取得した？

・AWS SAA
・AWS SOA
を取得しました。
⑨インフラエンジニアに興味ある人は何から始めたら良い？
ネットワークの仕組みを理解するところから始める。
⑩社長にインフラエンジニアを勧められた時どんな気持ちだった？
正直開発やってみたかったけど、インフラに関しては何もわからなかったので、とりあえずやってみるか…と了承しました。
でも、ネットワークについて詳しくなれたので、
とても良い選択になったと思います。
今後は自社の開発チームにインフラの経験を活かして挑戦したいです！
初めて夫にインタビューをしながら、
記事を執筆してみましたが、
夫はかなり細かい（私の間違いなどすぐ気付いて訂正してきます笑）部分まで注意深く見る性格なので、
業務内容などもマッチしているのかな…と感じました。
インフラエンジニアは、目立つ仕事ではないけれど、誰かのために必ず役立つやりがいがある仕事ですね！
まだまだ知らないことばかりですが、
この業界について知ることができたら嬉しいです💭💭💭
"
https://qiita.com/kiyo_nakahata/items/dfb3c4bba471692bdbf0,"['機械学習', 'AI', '生成AI', 'ChatGPT', 'LLM']","
はじめに
株式会社イノベイティアのCEO、中畑です。
今回は生成AIについて、基礎の基礎から解説をします。
Twitter(X)もよかったらフォローしてください。




おことわり
細かいAIの技術的な仕組みは、本記事では割愛します。

生成AIとは
生成AI (Generative AI)とは、様々なコンテンツを生成できるAI

生成AIの事例

画像生成

生成AIテクノロジーを利用し、リアルな顔や風景画像などを生成する (Midjourney)


テキスト生成

GPT-3やGPT4などのモデルが、与えられたプロンプトに対して、人間のように自然な文章を生成する


音楽生成

生成AIが特定のスタイルやジャンルに基づいてオリジナルの音楽を生成する




従来のAIと生成AIの比較




従来のAI
生成AI




学習の視点
情報の整理・分類・検索
パターンや関係の学習


出力の目的
特定や予測
創造


ビジネスインパクト
決められた行為の自動化
新しいコンテンツの生成


学習データ
具体的なデータセット
構造化されていないデータセット




なぜ生成AIが注目されるようになったのか
キーワードとして「精度」「学習量」「スピード」「利便性」の4つが挙げられます。

精度
質問に対する回答の精度や出力される文章表現の自然さ、入力条件にあった画像など出力されるものが様々なビジネスシーンで使えるレベルになりました。

学習量
学習するためのデータ量が飛躍的に拡大して、モデルの精度が上がりました。

スピード
コンテンツ生成のスピードが早く、条件を入力するだけで簡単に文章や画像を生成してくれるようになりました。

利便性
ChatGPTなどは、誰でも簡単に使うことができる。マニュアルなどが不要で操作性もかなり良いです。

生成AIのビジネスにおける活用事例

エンジニア

コード補完を提供し、エンジニアの開発効率が上がる (GitHub Copilot)
正しいコードかの判定は人間がやる必要がある

コードを0→1で書いてもらうよりもテストコードやレビューやリファクタリングでの利用が良い印象




マーケティング

広告の文章やキャッチフレーズ、ブログ記事などマーケティングコンテンツの自動生成
顧客データをもとにAI分析を通じて最も効果的なマーケティング戦略を立案

GoogleアナリティクスからWebサイト内のユーザーの行動を分析し提案


SNSやオンラインデータから新しいマーケティングキャンペーンの戦略を構築

既存のSNSのバズった投稿などを読み込ませて新規にツイートを生成してもらう




カスタマーサービス
AIを使用したチャットボットを作成することで24時間365日稼働させ、サポート担当の稼働工数を減らす

セールス

様々なデータリソースから潜在的な顧客のリードを自動的に生成する
過去データをもとに将来の売上予測を作成


財務

データを分析し、不正を未然に防ぐ
キャッシュフローや利益の予測をする


LLMとは

大規模言語モデル(Large Language Models、LLM)は非常に巨大なデータセットとディープラーニング技術を用いて構築された言語モデル
大規模とは「計算量」「データ量」「パラメータ数」を大幅に増やして構築したことからくる


大規模言語モデルの特徴 (Scaling Laws for Neural Language Modelsより)
パラメータ数が増えると、精度が高い言語モデルを生成することができます。


言語モデルとは

文章や単語の出現確率を用いてモデル化したもの
人間の自然な会話に対して高い確率を割り当て、文章として成立しない並び方には低い確率を割り当てる


大規模言語モデルの応用


文学と創作

文章生成が得意なので、作家は創造的なプロセスにより時間を割けるようになる。



医療業界

診断報告や医療文書の作成を自動化し、医師の書類作成作業の工数を削減し、より患者へのヘルスケアに時間を割ける



教育分野

個別の学生に合わせたカスタマイズされた学習資料を生成し、教育の個人化がより推進される



ビジネス分析

競合分析や戦略策定など数値的に複雑なタスクを支援する。具体的には大量のデータから考察をし、意思決定を補完する提案をする




大規模言語モデルの懸念点


データセキュリティとプライバシー

個人情報を含むデータでの学習が悪用されるとプライバシー侵害の可能性がある
顧客データなどをそのまま載せてしまう



品質と信頼性

LLMの生成する文章は品質が一貫していない場合があり、特定の専門家レベルの正確さを達成するのは困難



不可解な挙動

LLMの動作がブラックボックスなので、どのように結果を出したのかを解析するのが難しい




最後に
Twitterでも最新のAI情報を発信しているので、ぜひフォローしてみてください。



"
https://qiita.com/Nagumo-7960/items/2cd77e38e0b5c5a8ae63,"['アジャイル', 'スクラム', 'スクラムマスター', 'アジャイル開発']","この記事はディップ Advent Calendar 2023の20日目の投稿です。

はじめに
こんにちは。ディップ株式会社に今年新卒入社して、今はAndroidアプリエンジニアをやってる氏家拓海と申します。
今年の5月にスクラムの資格であるProfessional Scrum Masterを取得しました。
今回はその経緯についてお話ししたいと思います。

スクラムってなんぞや
まずスクラムについて解説します。スクラムとは、アジャイル開発のフレームワークの一つです。
ソフトウェア開発は主に2つの開発手法に分けられます。
それがウォーターフォール開発とアジャイル開発です。
ウォーターフォール
ウォーターフォールはイメージしやすい一般的な開発手法で、下記の図のように、設計、開発、テスト、リリースといった流れです。

アジャイル開発
それに対してアジャイルは、ウォーターフォールの開発の一連の流れを、短いサイクルで回しながら進めていきます。
アジャイル開発は2001年に「アジャイルソフトウェア開発宣言」が出されて以降、広く知られるようになりましたが、近年は特に注目を集めています。
アジャイルの強みは、ユーザーに速く価値を届けられることで、時代の変化にプロダクトを対応させやすいということです。
例えば、今年はChatGPTを登場を筆頭に、AIの活用が時代の節目を迎えていると感じます。AIが注目され始めたのもずいぶん昔のことだと思いますが、あそこまで自然な解答を会話形式で得られる時代が急に訪れるのは、誰にでも予想できたものではないでしょう。
といったように、市場を取り巻く状況は急に変わります。
どんどん変わります。ぐんぐん変わります。
どんな需要が生まれてどんな需要がなくなるか予想しにくい現代では、速く世に出してユーザーのフィードバックを得ることが大事だと感じます。

スクラムはそんなアジャイル開発のフレームワークの一つです。
アジャイル開発にはいくつかフレームワークがあります。代表的なものは以下のものです。

スクラム
カンバン
XP(エクストリームプログラミング)

この中でも、特に採用率が高いのがスクラムです。
スクラムには3つのロールと5つのイベントがあります。
簡単に図にするとこんな感じです。

詳細な説明をしようとすると長くなるので省きますが、
なんとなく役割や流れがある程度決まっているんだなーってことは伝わるかなと思います。
これは個人的な考えですが、スクラムはスプリントの概念がアジャイル開発を体感的に掴みやすいのと、進め方や役割がある程度決まっていることから共通認識を持ちやすいんだと思います。
実際に現場で使われる時はアジャイルのフレームワークの比較は特にされず、アジャイルといえばスクラムくらいの気持ちで採用されてると思いますが。

スクラムの資格を取ろうと思った経緯
弊社では入社後の新卒研修で、チーム開発を実践しました。開発の手法や進め方についてはかなり自由になっていて、学生時代にスクラムで開発をした経験がある私は、これを機会にスクラムマスターをやってみたいと考えました。
しかし私が学生時代に実践していたスクラムの知識は、かなりまばらで良い加減なものでした。
スプリントごとに区切って開発をすること、スクラムマスターはスクラムの管理を、プロダクトオーナーはプロダクトの管理を、開発者は開発をする。
そこらへんは基本は覚えていましたが、当時の曖昧な記憶を頼りに自分がスクラムマスターをやるのは、さすがに無責任だなと思い、勉強を始めました。
まず、SCRUM BOOT CAMP THE BOOKを買って読みました。大学生の私がスクラムを始めた時、最初に読んでスクラムに興味を持ったきっかけでもあります。



この本は文章と漫画でスクラムについて解説している本で、細かい実践を省いている代わりにわかりやすいです。スクラムの目的や基本的な概念、進め方を一読してわかった気になれます。
2年くらい読んでなかったので、久しぶりにワンピース全巻を読み直すような気持ちで楽しく読みました。
しかし1冊本を読んだだけでは、スクラムマスターとしてスクラムを提案し、軌道に乗せる自信はありませんでした。
そこで考えたのが、資格を取るという勉強のやり方です。
まず、資格なら「なんとなくわかった気」になっても取れないので、少なくとも合格ラインまで問題を解けるようになる必要があります。
また、資格を取ったというアピールにもなります。「スクラムの勉強をした」「スクラムの書籍を読んだ」これは例えば1冊本をななめ読みしただけでも同じことが言えますが、資格を取得することでスクラムの最低限の知識が担保されることになります。
あと、周りから褒められます！嬉しいね！

私はそういう狙いで、スクラムの資格を取ることにしました。

スクラムの資格の種類
スクラムにはいくつか有名な資格があります。

Certified ScrumMaster
Professional Scrum Master
Registered Scrum Master

スクラムマスターの資格で有名なものを3つ挙げてみました。
私が取ることにしたのは、Professional Scrum Masterの資格です。
理由はいたってシンプルで、値段が安いからです。
他の2つの資格は、資格取得にあたって研修を受ける必要があります。この研修が高く、20〜30万以上の値段がかかります。新卒1年目でこの値段を払うのはお財布に厳しいわけです。
それに対して、PSMは研修が必須ではないため、資格取得のために必要な金額は200ドル(3万円くらい)なので、なんとか手が届く金額でした。
そういうわけでPSMを選びます。

PSM(Professinal Scrum Master)について
Scrum.orgが提供している認定スクラムマスターの資格です。
3つのレベルがあり、私が受けたのはProfessional Scrum Master Iです。
簡単に概要だけご紹介したいと思います
形式：オンライン
問題形式：選択問題、複数回答、正誤問題
問題数：80問
合格：85点以上
時間：60分間
永久認定・年間更新料不要
一度取得すれば、その後の更新が必要ないのも嬉しいですね




勉強方法
前提として、Scrum Boot Camp The Bookを読了して、
スクラムの基本的な概念、スクラムイベント、スクラムの5つの役割などについては押さえているものとします。
基本的に、ひたすらUdemyの問題集を解きます。



こちらがおすすめのUdemyの問題集です。問題はたっぷり80問×4回セット！
値段がセール時に1500円くらいですので、コスパも良くお手頃価格です。
とりあえず問題を解いていき、間違えた箇所をスクラムガイドを読みながらメモしていきます。
合格点が85点なので、90点以上を安定して取れるようになれば本番での合格が見えてきます。

結果
結果は…
91点(合格ライン85点)で合格しましたー！！

いやーちょっと危なかったですね。100点に近い点数で合格したかったんですが、いやいや、意外と難しいんですよ。
というわけで、どこらへんが難しい問題だったのかをご紹介したいと思います。

めっちゃ間違えた問題
よかったら、読みながら答えはどれか考えてみてください。
チームに6人の開発者がいて、スプリントの長さが2週間の場合、スプリントレビューの通常のタイムボックス化された期間はどれくらいですか?

2時間
4時間
8時間
3時間

はい、答えは・・・
・・・
・・・
・・・
・・・
・・・
・・・
・・・
・・・
・・・
1番の2時間です
こういう問題も出ます。
スクラムガイドには1ヶ月あたりのスクラムイベントのタイムボックスが記載されているので、その時間さえ覚えておけば、単純な割り算で計算することができます(デイリースクラムは除く)。



スクラムイベント
1ヶ月あたりのタイムボックス




スプリントプランニング
最大8時間


デイリースクラム
15分固定


スプリントレビュー
最大4時間


スプリントレトロスペクティブ
最大3時間



ちなみに私はどれがどれがよくわからなくなったので繰り返し間違えました。
本来は暗記するものではないので忘れてしまうのです。
続いて紹介する問題がこちら
スプリント中に変更すべきではないものは次のうちどれですか?

プロダクトバックログ
最低限の品質レベル
スプリントバックログ
完成の定義

はい、答えは・・・
・・・
・・・
・・・
・・・
・・・
・・・
・・・
・・・
・・・
2番の最低限の品質レベルです
これは難問なんじゃないでしょうか。パッと答えられる人は、スクラムに慣れている人かと思います。
間違いから解説していきます。
まず1つ目。プロダクトバックログはスプリント中に変えていいのか？変えていいんです。
プロダクトバックログはいつでも変更される可能性があります。
スクラムガイドには、以下のように記載されています。

スプリントでは、プロダクトバックログを必要に応じてリファインメントする。

3つ目。スプリントバックログもスプリント中に変えてOKです。
スクラムガイドにいくつか根拠があります。

スプリントバックログには、スプリントゴールを達成するために開発者がスプリントで⾏う作業がリアルタイムに反映される。


デイリースクラムの⽬的は、計画された今後の作業を調整しながら、スプリントゴールに対する進捗を検査し、必要に応じてスプリントバックログを適応させることである。

上記の通り、スプリントバックログは主にデイリースクラムの中で検査され、必要に応じて変更されていくのを前提としています。
4つ目。完成の定義は変更されることがあります。この辺はちょっとややこしいです。まず、スクラムガイドに「完成の定義を変更してはいけない」といった記載はありません。
完成の定義とは、すべてのプロダクトバックログアイテムに対して共通して適用されるもので、コードレビューやテストの実行範囲、ドキュメントなどが挙げられます。
完成の定義はチームの成熟度や進行に応じて変更していくものなので、スプリント中での変更が必要となる場面も考えられます。
最後に、正解の選択肢2番の解説をします。
最低限の品質レベルは、スプリント中に変えてはいけないことがスクラムガイドで明記されています。

スプリントでは、スプリントゴールの達成を危険にさらすような変更はしない。品質を低下させない。プロダクトバックログを必要に応じてリファインメントする。学習が進むにつれてスコープが明確化され、プロダクトオーナーとの再交渉が必要になる場合がある。

品質基準がもれた状態だと、次回以降でバグや保守性に欠けたコードを修正する手戻りが発生し、進捗を妨げる原因になることがあります。
いかがでしょうか？
どの問題も、基本的にスクラムガイドを参照すれば根拠を拾うことができます。
でも、実はさっき挙げた2つ目の問題は最後に間違えた問題で、当時は調べてもよくわからなかったので面倒臭くて放置していました。
今記事を書き始めて、スクラムガイドやアジャイルコーチの記事を参照して、やっと理解できたと思います。やはりスクラムでわからないことがあった時、基本的にスクラムガイドを参照するべきなんですよね。実感します。
間違えた問題を理解しながら繰り返し問題を解いていけば、3、4週目あたりで90点台には到達できると思います。
ちなみに私は、最初やった時の正答率が57%でした。うーん、難しい

取ってみての感想
意外と時間がかかりました。1週間くらいでサクっと取得するくらいの気持ちでいたんですが、結果として3週間近くスクラムの勉強をしてから、なんとか合格できました。
最初に受けた模擬試験で57%しか正当できず、スクラムの経験があっても知識レベルやスクラムへの理解はこの程度なんだな…とショックを受けたのを覚えています。
この記事を読んでくださっている人の中にも、スクラムで開発しているという人がいらっしゃると思います。
ぜひPSMを受験してみることをオススメします。学び直しになりますし、新しい発見もあって楽しいです。
"
https://qiita.com/hidebo_yamada/items/90551d3fa921f5ffc12f,"['JavaScript', '初学者向け']","初めまして、現在プログラミングスクールで学習中のやまちゃんと申します。
Reactを勉強してみたいなと思っていましたが、そもそもJavaScriptの基礎を
さらっとしか触れていなかったので勉強することにしました。
その中で非同期処理で大事だとされているPromiseについて少し調べてみました。
今回はcatAPIを用いて猫画像を取得する非常にシンプルなコードで見ていこうと思います。
※ この記事は「RUNTEQ Advent Calendar 2023」の21日目を担当した記事です。



現在プログラミング勉強中の初学者です。
内容に誤りや補足情報等ございましたら、コメント・DM等で
教えていただけると幸いです。



そもそも非同期処理とは？
JavaScriptでは基本一つの処理が完了するまで他の作業はできませんが、
非同期処理は一つの作業をしている間に他の作業ができるようにした仕組みの事です。
参考記事
非同期処理とは何か、何が嬉しいの？

Promiseとは
Promiseとは非同期処理の結果を表すオブジェクトで成功時と失敗時の結果を扱うもので
可読性が上がるように導入されたものです。
Promiseには「Pending・Fulfilled・Rejected」の３つの状態(ステータス)を持っています。
・Pending(保留中)
初期状態で、成功も失敗もしていません
・Fulfilled（履行済み）
非同期処理が成功し、結果が利用可能になった状態
このステータスになると、resolveという関数が呼ばれ、通信が成功したことを意味します。
・Rejected(拒否)
非同期処理が失敗し、エラーが発生した状態
このステータスになると、rejectという関数が呼ばれ、通信が失敗したことを意味します。
Promiseの基本的な構文は以下です。
new Promise((resolve, reject) => {
  //成功 or 失敗するかの条件に基づいてresolveかrejectを呼び出す非同期処理を記述
}).then((response) => {
  // 成功時の処理(ステータスがresolve時の処理)  
}).catch((error) => {
  // 失敗時の処理(ステータスがreject時の処理)
}).finally(() => {
  // 成功/失敗に関わらず実行される処理
});


ではPromiseを用いてcatAPIから猫の画像を取得していきます。
document.addEventListener(""DOMContentLoaded"", function () {
  document.getElementById(""catButton"").addEventListener(""click"", function () {
    fetchCatData();
  });
});

function fetchCatData() {
  fetch(""https://api.thecatapi.com/v1/images/search"") // APIから猫の画像を取得
    .then((response) => response.json()) //JSONデータを処理する
    .then((data) => {
      const imageUrl = data[0].url;   // 取得したデータから画像のURLを取り出す
      displayImage(imageUrl);
    })
    .catch((error) => console.error(""Error:"", error));
}

function displayImage(imageUrl) {
  const imageContainer = document.getElementById(""imageContainer"");
  imageContainer.innerHTML = `<img src=""${imageUrl}"" alt=""Pet Image"" />`;
}


いやいやPromiseがないやん！！！
実はこのfetch関数自体がpromiseを含んでいるのです。
Fetch関数はリクエストを送るとすぐにPromiseオブジェクトを返却し、このPromiseは
HTTPリクエストの結果(通信の成功か失敗か)を表します。
リクエストから返却される結果に基づいてPromiseの状態が決定される様です。
※下記fetch関数でリクエストを送った直後のコンソール画面です。

確かにPromiseが返却されています！
PromiseStateのプロパティは""fulfilled""が返ってきています
実際に取得できた画像はこちらです。
ランダムに表示されるので見ていて飽きないですね！



まとめ
また現在ではPromiseを発展させたasync/awaitを用いる事が多い様ですが、
Promiseが基本となってできています。
JavaScriptでの非同期処理とひと口に言っても構成される技術は多岐に渡り、理解するには
かなりの時間が要するなと感じました。
学習を進めていく中で非同期処理は必須だと思うので、今後アプリ開発などにも活かせていければと思います！
参考資料
【ES6】 JavaScript初心者でもわかるPromise講座
フェッチAPIの使用 MDN Web Docs
"
https://qiita.com/yugami/items/f46aaaff7be32922d0f3,"['初心者', 'Laravel', '初心者向け', 'axios', 'ひよっこ']","
はじめに
社内ではlaravelとnuxt.jsや、laravelとnext.jsを組み合わせた開発が行われています。
ただ、私自身業務でcodeigniterを使って開発しているので知見がありません。
そこでログイン機能を作って知見を深めたいと思いました。
また、laravelとnext.jsでの組み合わせで調べると記事が少ないような気がしたので、少しでも貢献できれば嬉しいです☺️

環境構築
macOS Ventura 13.4.1で開発します。

事前作業

事前にインストールが必要なもの
docker Version4.19.0
node.js v18.17.x（next.js）
php 8.x
composer 2.6.x（laravel 10.x）

事前に作成が必要なもの
事前にdockerのコンテナを作成しておいてください。
また、この中で以下のようにnodeをインストールしておいてください。

dockerfile
# node install
RUN curl -fsSL https://rpm.nodesource.com/setup_18.x | bash -
RUN yum install -y nodejs



next.jsの環境構築
next.jsをローカル環境にインストールしたいと思います。
公式の通りにコマンドを実行してみます
$ npx create-next-app@latest
Need to install the following packages:
create-next-app@14.0.4
Ok to proceed? (y)
✔ What is your project named? … src
✔ Would you like to use TypeScript? … No / Yes
✔ Would you like to use ESLint? … No / Yes
✔ Would you like to use Tailwind CSS? … No / Yes
✔ Would you like to use `src/` directory? … No / Yes
✔ Would you like to use App Router? (recommended) … No / Yes
✔ Would you like to customize the default import alias (@/*)? … No / Yes

アプリ名をsrcにしたこと以外はデフォルト設定です。
さて、コンテナに入ってnextアプリが立ち上がることを確認します
$ docker container exec -it app bash
# npm run dev

> src@0.1.0 dev
> next dev

   ▲ Next.js 14.0.4
   - Local:        http://localhost:3000

 ✓ Ready in 4.4s

ブラウザで確認してみます

表示できました🥳

laravelの環境構築
laravelをローカル環境にインストールしたいと思います。
公式の通りにコマンドを実行してみます
$ composer create-project laravel/laravel src

アプリ名をsrcにしたこと以外はデフォルト設定です。
アプリが立ち上がることを確認します

表示できました🥳

認証機能の実装

laravel側
sanctumを使って認証システムを作りたいと思います。
変更点は以下です

routes/api.php
+ use App\Http\Controllers\API\AuthController;

+ Route::post('register', [AuthController::class, 'register']);
+ Route::get('login', [AuthController::class, 'login'])->name('login');
+ 
+ Route::middleware('auth:sanctum')->group(function() {
+     Route::post('logout', [AuthController::class, 'logout']);
+ });



config/cors.php
- 'supports_credentials' => false,
+ 'supports_credentials' => true,



app/Http/Kernel.php
- // \Laravel\Sanctum\Http\Middleware\EnsureFrontendRequestsAreStateful::class,
+ \Laravel\Sanctum\Http\Middleware\EnsureFrontendRequestsAreStateful::class,



app/Http/Controllers/API/AuthController.php
<?php

namespace App\Http\Controllers\API;

use App\Http\Controllers\Controller;
use App\Models\User;
use Illuminate\Http\Request;
use Illuminate\Support\Facades\Hash;
use Validator;

class AuthController extends Controller
{
    public function register(Request $request)
    {
        $validator = Validator::make($request->all(), [
            'name'=>'required|max:191',
            'email'=>'required|email|max:191|unique:users,email',
            'password'=>'required|min:8',
        ]);

        if ($validator->fails()) {
            return response()->json([
                'validation_errors' => $validator->messages(),
            ]);
        } else {
            $user = User::create([
                'name' => $request->name,
                'email' => $request->email,
                'password' => Hash::make($request->password),
            ]);

            return response()->json([
                'status' => 200,
                'username' => $user->name,
                'message' => 'register success',
            ]);
        }
    }

    public function login(Request $request)
    {
        $validator = Validator::make($request->all(), [
            'email' => 'required',
            'password' => 'required',
        ]);

        if ($validator->fails()) {
            return response()->json([
                'validation_errors' => $validator->messages(),
            ]);
        } else {
            $user = User::where('email', $request->email)->first();
            if (!$user || !Hash::check($request->password, $user->password)) {
                return response()->json([
                    'status' => 401,
                    'message' => '入力情報が不正です',
                ]);
            } else {
                $token = $user->createToken($user->email . '_Token')->plainTextToken;

                return response()->json([
                    'status' => 200,
                    'username' => $user->name,
                    'token' => $token,
                    'message' => 'ログインにせいこうしました。',
                ]);
            }
        }
    }

    public function logout()
    {
        auth()->user()->currentAccessToken()->delete();
        return response()->json([
            'status' => 200,
            'message' => 'ログアウトせいこう',
        ]);
    }


insomniaで認証できているか確認してみます
アカウント登録

ログイン

ログアウト

laravel側の認証機能はできていそうです🥳

next側
フロント側で行うことは以下です。

localstorageにlaravelで発行したapiトークンを保存する
api通信時にはトークンを設定したaxiosを使う


 app/login/page.tsx
'use client'
import axios from ""@/app/api/axios""

const clickSignUp = () => {
    const email = document.querySelector<HTMLInputElement>('input[name=""email""]')
    const password = document.querySelector<HTMLInputElement>('input[name=""password""]')

    if (email && password) {
        const data = {
            email: email.value,
            password: password.value,
        }

        axios.post('api/register', data).then(res => {
            if (res.data.status !== 200) {
                alert('アカウント登録しっぱい')
            }
        })
    } else {
        alert('emailかpasswordが存在してません')
    }
}

const clickLogin = () => {
    const email = document.querySelector<HTMLInputElement>('input[name=""email""]')
    const password = document.querySelector<HTMLInputElement>('input[name=""password""]')

    if (email && password) {
        const data = {
            email: email.value,
            password: password.value,
        }

        const urlSearchParam = new URLSearchParams(data).toString()

        axios.get('api/login?' + urlSearchParam).then(res => {
            if (res.data.status === 200) {
                console.log('000000000000000000')
                console.log(res.data)
                localStorage.setItem('auth_token', res.data.token);
            } else {
                alert('ログインしっぱい')
            }
        })
    } else {
        alert('emailかpasswordが存在してません')
    }
}

const clickLogout = () => {
    axios.post('api/logout').then(res => {
        if (res.data.status === 200) {
            localStorage.removeItem('auth_token')
            console.log('aaaaaaaaaaa')
        } else {
            alert('ログアウトしっぱい')
        }
    })
}

const login = () => {
    return (
        <>
            <header>
                <nav>
                    <ul>
                        <li>
                            <button onClick={clickSignUp}>新規登録</button>
                        </li>
                        <li>
                            <button onClick={clickLogin}>ログイン</button>
                        </li>
                        <li>
                            <button onClick={clickLogout}>ログアウト</button>
                        </li>
                    </ul>
                </nav>
            </header>
            <main>
                <div className='login_form'>
                    <label>
                        メールアドレス：
                        <input type='input' className='login_form__input' name='email' />
                    </label>
                    <label>
                        パスワード：
                        <input type='input' className='login_form__input' name='password' />
                    </label>
                </div>
            </main>
        </>
    )
}

export default login



 app/api/axios.tsx
import axios from 'axios'

const new_axios = axios.create({
  baseURL: 'http://localhost',
  withCredentials: true,
  headers: {
    'Content-Type': 'application/json',
    'Accept': 'application/json',
  }
})

new_axios.interceptors.request.use(
  (config) => {
    const token = localStorage.getItem('auth_token')
    config.headers.Authorization = token ? 'Bearer ' + token : ''
    return config
  }
)

export default new_axios



 app/globals.css
@tailwind base;
@tailwind components;
@tailwind utilities;

:root {
  --foreground-rgb: 0, 0, 0;
  --background-start-rgb: 214, 219, 220;
  --background-end-rgb: 255, 255, 255;
}

@media (prefers-color-scheme: dark) {
  :root {
    --foreground-rgb: 255, 255, 255;
    --background-start-rgb: 0, 0, 0;
    --background-end-rgb: 0, 0, 0;
  }
}

header nav {
  max-width: 1200px;
  margin: 0 auto;
  padding: 30px;
}

header nav ul {
  display: flex;
}

header nav ul li {
  &:not(:last-child) {
    margin-right: 20px;
  }
}

header nav ul li button {
  width: 150px;
  height: 40px;
  border: solid 1px;
  border-radius: 30px;
  cursor: pointer;
}

.login_form {
  margin: 0 auto;
  padding: 0 30px;
  display: flex;
  flex-direction: column;
  gap: 10px;
  max-width: 1200px;
}

.login_form .login_form__input {
  border: solid 1px black;
}



課題

nextauth.jsを使い、認証をセッションで管理したい🐣
cookieだとセキュリティ的に値が丸見えなため、insomniaなどのツールからAPIトークンを誰でも簡単に設定して値を取ってこれるので、セッション管理したいと思いました。

おわり
laravelとnext.jsに興味があったので今回勉強するための良いきっかけになったと思います。引き続き情報を収集して面白そうなことがあったら自分の環境で開発してみるのも良いのではないかと思いました。
"
https://qiita.com/pollenjp/items/aa54c7b236e40faae776,"['Rust', 'Ansible', 'JetPorch']","本記事は KMC Advent Calendar 2023 と KLab Engineer Advent Calendar 2023 の 19 日目の記事です.

TL;DR
JetPorch とは...

Ansible の開発者である Michael DeHaan によって開発された新しい構成管理ツール
Rust で記述されていて速い
Ansible Playbook と似た記法 (YAML)
単純明快で直感的な仕様 (を目指している)
最低限の機能は揃っているがまだまだ開発段階


本記事の目的
最近 JetPorch という構成管理ツールがリリースされたのを知り, 個人的に使っている Ansible の移行先として興味を持ったので調べてみました.




Jet
Ansible のソースが Any 型ばかりで読みにくく泣きそうだった中見つけた
ちょっと追っていきたい


JetPorch とは
JetPorch とは Ansible の開発者である Michael DeHaan による新しい構成管理ツールです.
基本的なコンセプトは Ansible とよく似ていますが, 昨今の複雑しつつある自動化に対応するため Ansible では足りない部分・実現困難な部分を Michael DeHaan さん自らが見直し改善しています.
また, Ansible が Python で書かれているのに対し JetPorch はより強い型システムや強力な並列性をサポートする Rust で書かれています. より学習しやすくユーザーにシンプルに映ること12も目的の一つとされています.
2023 年 10 月 に v0.1.0 として初リリースされ, SourceHut にてソースの公開・管理が行われています5.

https://www.jetporch.com/
https://git.sr.ht/~mpdehaan/jetporch



基本コンセプト
基本的なイメージは Ansible と同じです. JetPorch を実行するコントロールマシンから操作対象のマネージドノードに対して, SSH6で並列に命令を送り, 数台から数千台以上の規模6のサーバーを一括で操作・構成管理することが目的です.


Ansible と比較した特徴
同様の既存ツールである Ansible と比較すると以下のような特徴を持っています.




Ansible
JetPorch




開発言語
Python
Rust


マネージドノード7への要件
Python が必要
無し


実行速度
遅い
速い


単純明快さ
低い

高 くなるように慎重に実装1



命令の仕方
Push 型 ( / Pull 型8)
Push 型 ( / Pull 型(実装予定))


命令の送信方法
SSH
SSH ( + α6 (実装予定) )


Playbook の記述
YAML
YAML


標準モジュール
多い
少ない (追加実装予定)


外部モジュールの言語
何でも9

何でも910



3rd party モジュール
多数 (Ansible Galaxy 等)
今後の広がりに期待




開発言語: Rust
まず個人的に一番うれしい点は Rust によって実装されていることです.
Rust は強力な型システムや所有権を持つため安全性の高い並列性・並行性のあるプログラミングを可能にしており, 実行速度の向上をもたらします.
最近の Python プロジェクトでは広く型ヒントが導入されつつありますが Ansible にはまだ無く, ソースコードを読む際には労力がかかります. その点においても Rust の型システムは開発者の理解を助けてくれます. 所有権による強力なコンパイラチェックも高い安全性を保証してくれます.

マネージドノードへの要件
Ansible ではマネージドノードに Python を必要としますが JetPorch にはそのような要件はありません. そのため Python がインストールされていない環境であっても JetPorch であれば利用することができます.

I’m excited about this part - Core modules should work by streaming shell commands and not require any Python on the remote machines or anything like that.
(訳11) この部分についてはとてもワクワクしています. Core モジュールはシェルコマンドをストリーミングすることで動作し, リモートマシンには Python などのインストールは必要ありません.
...
This should be particularly nice because bash just keeps working and there is nothing to keep updating to deal with different python versions.
I look forward to refining by bash skills.
これは特に素晴らしいことで, bash はそのまま動作し, 異なる Python のバージョンに対応するために更新する必要があるものは何もありません.
bash のスキルを磨くことができるのを楽しみにしています.

※ 個人的な解釈としては JetPorch のコア機能自体にはそこまで複雑な機能を乗せるためのものではなく, 複雑なことは外部モジュールに任せる前提で設計を行えているからこそ Bash を選択できているのではないかと考えます.

単純明快さ
今や Ansible は様々な機能を持っていますが, その分仕様が複雑になり学習コストが高くなりがちです.
その点, JetPorch は直感的な仕様・単純明快さを重視1しており, あるケースに置いて便利であっても非直感的になり得る機能の導入には慎重な態度をとっています12.

I would like to see a move back towards minimalism, grocery-list style simplicity, and looking at things through a more systems-programming / hard-engineering type lens.
私は最小限主義, 食料品リストのようなシンプルさ, よりシステムプログラミング / ハードエンジニアリングのようなレンズを通して物事を見ることを望んでいます.
...
Simply put, things should be rock-solid, lightening fast, well-engineered, and readable and auditable by people who have never even seen the content before.
単純に言えば, 事柄は堅牢で, 高速で, よく設計されており, これまでにその内容を見たこともない人によって読みやすく監査可能であるべきです.

また, JetPorch をそれのみでで完結させるのではなく, 様々なツールと連携させる上ではこのシンプルさが不可欠です4.

It should be simple and easy to understand and be easy to call from other tools.
Adapt the Unix philosophy basically.
...
それはシンプルで理解しやすく, 他のツールから呼び出しやすいことが望ましいです.
基本的には Unix の哲学を採用しています.


JetPorch の基本
ここからは簡単なサンプルを用いて JetPorch の使い方を紹介します. Ansible を使ったことがない人に対してもわかるように説明しますが, Ansible を使ったことがある人は殆ど変わらぬ使用感で使うことができると思います.
利用する際の基本要素は以下の２つから成り立っています.


Inventory ... マネージドノード7 (操作対象のサーバー) の情報を記述したファイル群

Playbook ... 実行するタスクを記述したファイル群

このように処理と処理対象を分けて記述することで, 例えば Inventory だけを切り替えることで同じ処理を異なるサーバーに対して実行することができます.
""Playbook"" の仕様は以下のようになっており, ""Play"" と呼ばれる単位のリストで与えられます.
- name: Play name
  hosts:
    - some_host_or_host_group # マネージドノードのリスト
  tasks: # ""Task"" の配列
    - !echo
      name: Some task
      msg: ""ready to go!""
- name: Play name 2
  hosts: ...
  tasks: [...]

""Play"" はどのマネージドノードに対して, どのような処理を行うかを記述する単位です.
""Play"" が持つ tasks には処理の単位である ""Task"" の配列が渡されています.
一つ一つの ""Task"" では 例えばファイルを作成する, パッケージをインストールする, ファイルの内容を変更するなどの抽象化された処理 (""Module"") を呼び出すことで記述できます.
この Module ごとの処理に 冪等性 をもたせることで構成管理ツールとしての真価を発揮しますが, 冪等性に関する説明は他の Ansible 関連の記事を読んでみてください.

とりあえず実行してみる
JetPorch を触ってみたい人向けに本当に初歩的なハンズオンをしてみます.
マネージドノードに cowsay package をインストールし実行した cowsay をファイルに書き出すという処理を行います.
環境

コントロールノード: Ubuntu 22.04
マネージドノード: Ubuntu 22.04

node1.example.com
node2.example.com




インストール
以下のうちのどちらの方法でもインストール可能です.


Installing From Packages

$ cargo install jetp
https://crates.io/crates/jetp



Installing From Source

リンク先の説明では make していますが, 現状の実装はただ cargo build しかしていない13ので, そのまま cargo build しても問題ありません.




準備

ssh-agent を起動しておき, マネージドノードの ssh 鍵を登録しておいてください (ssh-add).
また, 今回の例ではマネージドノードにはパスワード無しで sudo できるユーザーを ssh 対象とする必要があります14.

以下のように inventory, playbook を配置します. (実際はディレクトリをここまで階層的に分けるルールは無いため, プロジェクトの規模に応じて適宜変更してください.)
zsh ❯ tree --charset ascii
.
|-- inventory
|   `-- dev
|       `-- groups
|           |-- some_host_group1
|           `-- some_host_group2
`-- playbooks
    `-- debug
        `-- debug.yml

some_host_group1 (拡張子なし)
hosts:
  - node1.example.com

some_host_group2 (拡張子なし)
hosts:
  - node2.example.com

debug.yml
- name: Sample
  groups:
    - all # all は すべてのホストに対して実行する
    # - some_host_group2 # 個別にグループやホストも指定可能
  tasks:
    - !echo
      name: Echo
      msg: ""ready to go!""
    - !apt
      package: cowsay
      with:
        sudo: root
    - !shell
      name: Cowsay hello world
      cmd: |-
        cowsay ""hello world"" > /tmp/cowsay_hello_world.txt



echo module は msg を標準出力に出力するだけの module です.

apt module は apt を使ってパッケージをインストールする module です.

shell module は cmd に与えられたコマンドを実行する module です.


実行
jetp ssh \
  --playbook ""./playbooks/debug/debug.yml"" \
  --inventory ""./inventory/dev"" \
  --threads 2

ログ
----------------------------------------------------------
> playbook start: /home/.../jetporch/playbooks/debug/debug.yml
----------------------------------------------------------
> play: Sample
----------------------------------------------------------
> batch 1/1, 2 hosts
----------------------------------------------------------
> begin task: Echo
… node1.example.com => running
  ..... node1.example.com : ready to go!
… node2.example.com => running
  ..... node2.example.com : ready to go!
----------------------------------------------------------
> begin task: apt
… node1.example.com => running
… node2.example.com => running
✓ node1.example.com => matched
✓ node2.example.com => matched
----------------------------------------------------------
> begin task: Cowsay hello world
… node2.example.com => running
… node1.example.com => running
✓ node1.example.com => complete
✓ node2.example.com => complete
----------------------------------------------------------
> play complete: Sample
----------------------------------------------------------

┌─────────┬─────┬─────┐
│Results  │Items│Hosts│
├─────────┼─────┼─────┤
│Roles    │0    │     │
│Tasks    │3    │2    │
├─────────┼─────┼─────┤
│Matched  │2    │2    │
│Created  │0    │0    │
│Modified │0    │0    │
│Removed  │0    │0    │
│Executed │2    │2    │
│Passive  │2    │2    │
│Skipped  │0    │0    │
├─────────┼─────┼─────┤
│Unchanged│4    │0    │
│Changed  │2    │2    │
│Failed   │0    │0    │
└─────────┴─────┴─────┘

(✓) Actions were applied.

ここのマネージドノードにそれぞれ ssh して cat /tmp/cowsay_hello_world.txt すると以下のような結果になっているはずです.
 _____________
< hello world >
 -------------
        \   ^__^
         \  (oo)\_______
            (__)\       )\/\
                ||----w |
                ||     ||


check mode
jetp ssh の箇所を jetp check-ssh に変えると Check Mode として実行されます. このモードでは実際には実行して変化を及ぼさず, どのような処理が行われるかを事前に確認することができます. Ansible の Check Mode と似たものだと思われます.

感想
今回は JetPorch の基本的な事項や Dev ブログなどで公開されている思想などをお伝えしてきました.
個人で軽く触ってみた感想ですが Ansible と比べると標準で使えるモジュールが非常に少なく, 移行しようにも不足感が拭えません. とりあえずは !shell 等でどうにか凌いで行く必要がありそうです.
一方で, 最近外部モジュールのサポート12 が入ったため, JSON 入出力が可能であれば自由なモジュールを開発し利用することができるようです. 冪等性のあるモジュールの開発は大変そうですが容易に外部モジュールを取り込めることは JetPorch の可能性を広げてくれると思います.
今後の JetPorch の発展に期待し, 今後どのようなツールと連携した使い方をなされていくのかが楽しみです.

おわりに
あと関係ないですが, Michael DeHaan さんはどうやら clippy のスタイルの一部を嫌っており独自のフォーマットで記述しているようです. 自分は Trailing Spaces などがあるとムズムズするのでソースコードを見ていると一気にフォーマットをかけたくなります.

参考

https://www.jetporch.com/
https://git.sr.ht/~mpdehaan/jetporch
JetPorch Dev Blog #1 - by laserllama
https://www.jetporch.com/modules/external-modules
https://jetporch.substack.com/p/external-modules-preview





I would like to see a move back towards minimalism, grocery-list style simplicity 3

↩ ↩2 ↩3



The goal is to deploy or automate things and get you more time to do something else. 4

↩


A New IT Automation Project? Moving Beyond Ansible And Keeping The Spirit - An Invitation. ↩


JetPorch Dev Blog #1 - by laserllama ↩ ↩2 ↩3


SourceHut - JetPorch Documentation ↩



Support local connectivity, SSH, and new message bus deployment architectures supporting potentially 50k-100k systems at once.
A New IT Automation Project? Moving Beyond Ansible And Keeping The Spirit - An Invitation

↩ ↩2 ↩3


SSH 元のマシンをコントロールノード, SSH 先のマシンをマネージドノードと呼ぶ. ↩ ↩2


ansible-pull — Ansible Documentation ↩


対応した JSON での入出力が可能な形態になっていればモジュール化可能. 4 ↩ ↩2


https://www.jetporch.com/modules/external-modules ↩


訳すのが面倒だったため英語だけ載せようとしたら GitHub Copilot くんが提案してくれたので脳死で Tab 押しました. ↩



Popular Questions: What About Role Dependencies?
...
The answer is they intentionally cannot, because we want things to be very straightforward to read, audit and develop.
https://jetporch.substack.com/p/external-modules-preview

↩ ↩2


https://git.sr.ht/~mpdehaan/jetporch/tree/main/item/Makefile#L4-5 ↩


パスワードで sudo に入る方法はまだ実装されていない ↩



"
https://qiita.com/erika_h/items/a7b1aef7c809cc41ee51,"['Python', 'Django', '初心者向け']","
はじめに
初めまして。
こちらの記事ではDjangoでToDoリストを開発するテーマで記載しています。
Django初心者向けになります。

Django（ジャンゴ）とは
DjangoはPythonプログラミング言語で書かれたオープンソースのウェブアプリケーションフレームワークです。Djangoは高い生産性と効率を提供し、ウェブ開発者が迅速に安定したウェブアプリケーションを構築できるようにすることを目的としています。
Python（パイソン）とは
Pythonは汎用の高水準なプログラミング言語で、シンプルで読みやすい構文が特徴です。
Pythonは機械学習、データ分析、ウェブ開発、自動化、科学計算、ネットワークプログラミングなど、多岐にわたる用途で使用されています。

Djangoの特徴
Djangoは広く使われ、多くの大規模なウェブアプリケーションやウェブサイトで採用されています。
Djangoフレームワークは以下の特徴があります。
ORM（Object-Relational Mapping）
DjangoはORMを提供し、データベースとの対話をPythonコードで行えるようにします。これにより、SQLクエリを直接書かなくてもデータベースとのやり取りができます。
管理画面
Djangoには管理画面が組み込まれており、データベースの管理やコンテンツの編集などが容易に行えます。
セキュリティ機能
Djangoはセキュリティを重視しており、クロスサイトスクリプティング（XSS）、クロスサイトリクエストフォージェリ（CSRF）などのセキュリティ攻撃から保護されています。
URLルーティング
URLパターンを使用して、どのビューが特定のURLに対応するかを定義できます。
フォームとバリデーション
フォームの作成とデータのバリデーションが簡単に行えます。

MVT（Model-View-Template）
Djangoフレームワークで使用されるウェブ開発のアーキテクチャパターンです。DjangoはMVTアーキテクチャを採用しており、MVTの構成要素は次の通りになります。
Model（モデル）
データベースやデータの取得・保存・処理などを担当します。
アプリケーションのデータ構造やビジネスロジックがモデルに定義されます。
DjangoではモデルはORM（Object-Relational Mapping）を用いてデータベースとやり取りします。
View（ビュー）
ユーザーのリクエストを受け取り、モデルからデータを取得してテンプレートに渡すなど、ビジネスロジックの制御をしています。
MVTではビューがコントローラの役割もしています。
ビューはユーザーに対して表示するデータを持たず、データはモデルから連携されます。
Template（テンプレート）
ユーザーに表示されるHTML文書を生成しています。
ユーザーに対して可視な部分であり、ウェブページの構造やデザインが定義されています。

開発環境の設定
ToDoリストを作成するために以下の手順で、開発環境を設定しておきましょう。

開発ツールをインストール
Pythonでは、主に以下の開発ツールが使用されています。
・Visual Studio Code (VSCode)
・PyCharm
・Atom
ご自身が使いやすい開発ツールを使用してください。

Pythonをインストール
以下のサイトから最新のバージョンをダウンロードします。



Pythonのインストール手順は以下のサイトを参考にしてください。




DjangoでToDoリストを作成する

ToDoリストの階層構造
ToDoリストの完成した場合の階層構造は以下になります。

階層構造
todo_project/              # プロジェクトのルートディレクトリ
|-- todo_project/          # プロジェクトの設定やルーティングを行うディレクトリ
|   |-- __init__.py
|   |-- asgi.py
|   |-- settings.py
|   |-- urls.py
|   |-- wsgi.py
|-- todo/                  # Todoリストのディレクトリ
|   |-- migrations/        # マイグレーションファイルが生成されるディレクトリ
|   |   |-- __init__.py
|   |-- __init__.py
|   |-- admin.py           # 管理者ページの設定
|   |-- apps.py            # アプリの設定
|   |-- forms.py           # フォームの定義
|   |-- models.py          # データベースモデルの定義
|   |-- tests.py           # テストのためのファイル
|   |-- urls.py            # アプリのルーティング
|   |-- views.py           # ビューの定義
|-- db.sqlite3             # データベースファイル（SQLiteを使用している場合）
|-- manage.py              # Djangoプロジェクトの管理コマンド
|-- requirements.txt       # プロジェクトの依存関係が記述されたファイル
|-- templates/             # HTMLテンプレートが格納されるディレクトリ	
|   |-- todo/              # アプリ別にテンプレートを保存することもある
|       |-- add_todo.html
|       |-- edit_todo.html
|       |-- todo_list.html



Djangoのインストール
以下のコマンドを実行し、Djangoをインストールする。

bash
pip install Django


pip(Pip Installs Packages)とは
Pythonパッケージを管理するためのパッケージ管理システムです。
pip を使用すると、Pythonプロジェクトに必要な外部ライブラリやツールを容易にインストールできます。

プロジェクトとアプリの作成
以下のコマンドを実行する。
Djangoプロジェクトとその中に含まれるアプリケーションを作成しています。
「todo_project 」はプロジェクトのルートディレクトリであり、「todo」 はそのプロジェクトに含まれるアプリケーションです。
アプリケーションは、プロジェクト内で機能ごとにモジュール化されます。

bash
django-admin startproject todo_project
cd todo_project
python manage.py startapp todo



データベースの設定
Djangoプロジェクトの設定ファイルである settings.py でデータベースの設定を行っています。
ここでは、SQLite3を使用し、データベースファイルの名前を db.sqlite3 に指定しています。
todo_project/settings.py ファイルを開き、データベースの設定については、以下の内容で設定されていれば、特に変更は不要です。

python
# todo_project/settings.py
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': BASE_DIR / ""db.sqlite3"",
    }
}


SQLite3とは
SQLite3は、軽量でサーバーレスのデータベース管理システム（DBMS）です。SQLite3は、ファイルベースのデータベースエンジンであり、データベースが1つの単一のファイルに格納されます。
SQLite3は小規模なプロジェクトや開発、プロトタイプ作成などの用途に適しています。ただし、大規模なデータベースや高い同時アクセスが必要な場合、より大規模なデータベースエンジンを検討する必要があります。

ToDoモデルの作成 (todo/models.py)
Todoアプリケーション内でデータベースモデルを作成しています。
ToDoItem モデルは、Todoアイテムの情報を表現するためのもので、「title」と「description」の2つの項目を持っています。

python
# todo/models.py
from django.db import models

class ToDoItem(models.Model):
    title = models.CharField(max_length=200)
    description = models.TextField(blank=True, null=True)

    def __str__(self):
        return self.title



ToDoビューの作成(todo/views.py)
ToDoアプリケーション内でビューを作成しています。
ビューは、ユーザーからのリクエストを受け取り、それに対する応答を返しています。
今回、ToDoのビューについては、以下の内容で設定をしています。
ビューの説明
todo_list ビュー：すべてのTodoアイテムを取得
add_todo ビュー：新しいToDoアイテムを追加
edit_todo ビュー：既存のToDoアイテムを編集
delete_todo ビュー：ToDoアイテムを削除

python
# todo/views.py
from django.shortcuts import render, redirect
from .models import ToDoItem
from .forms import ToDoForm

# ToDoアイテムのリストを取得
def todo_list(request):
    todos = ToDoItem.objects.all()
    return render(request, 'todo/todo_list.html', {'todos': todos})

# ToDoアイテムを追加
def add_todo(request):
    if request.method == 'POST':
        form = ToDoForm(request.POST)
        if form.is_valid():
            form.save()
            return redirect('todo_list')
    else:
        form = ToDoForm()
    return render(request, 'todo/add_todo.html', {'form': form})

# 既存のToDoアイテムを編集
def edit_todo(request, todo_id):
    todo = ToDoItem.objects.get(pk=todo_id)
    if request.method == 'POST':
        form = ToDoForm(request.POST, instance=todo)
        if form.is_valid():
            form.save()
            return redirect('todo_list')
    else:
        form = ToDoForm(instance=todo)
    return render(request, 'todo/edit_todo.html', {'form': form, 'todo_id': todo_id})

# ToDoアイテムを削除
def delete_todo(request, todo_id):
    todo = ToDoItem.objects.get(pk=todo_id)
    todo.delete()
    return redirect('todo_list')



ToDoリストのURLを設定(todo/urls.py)
ToDoリストのURLパターンを管理をしています。
ToDoリストの各機能に対応する URL パターンが定義され、それぞれのビュー関数が呼び出されるようになります。
例えば、「/add/」 にアクセスすると、「add_todo」ビューが呼び出され、新しい ToDo アイテムを追加するためのフォームが表示されます。

python
# todo/urls.py
from django.urls import path
from .views import todo_list, add_todo, edit_todo, delete_todo

urlpatterns = [
    path('', todo_list, name='todo_list'),
    path('add/', add_todo, name='add_todo'),
    path('edit/<int:todo_id>/', edit_todo, name='edit_todo'),
    path('delete/<int:todo_id>/', delete_todo, name='delete_todo'),
]



ToDoリストのURLをプロジェクトのURLに組み込む(todo_project/urls.py)
プロジェクトのルートURL設定にTodoアプリケーションのURLを組み込んでいます。
「/todo/」 以下のURLにアクセスするとTodoアプリケーション内のURL設定が適用されます。

python
# todo_project/urls.py
from django.contrib import admin
from django.urls import path, include
from todo.views import todo_list  # Make sure todo_list is imported correctly

urlpatterns = [
    path('admin/', admin.site.urls),
    path('todo/', include('todo.urls')),
    path('', todo_list, name='home'),
]



ToDoリストのHTMLテンプレート(todo/templates/todo/todo_list.html)
ToDoリストの表示に使用されるHTMLテンプレートを作成しています。
テンプレート内では、ToDoリストをループし、各ToDoアイテムを表示しています。
その際、各ToDoアイテムに編集と削除のリンクを追加しています。
また、新しいToDoアイテムを追加するためのリンクもあります。

html
<!DOCTYPE html>
<html lang=""ja"">

<head>
    <meta charset=""UTF-8"">
    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">
    <title>ToDoリスト</title>
    <!-- Bootstrap CSS -->
    <link rel=""stylesheet"" href=""https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css"">
    <!-- Custom styles for female-oriented design -->
    <style>
        body {
            background-color: #f0f8ff; /* AliceBlue */
        }

        .container {
            background-color: #ffffff; /* White */
            border-radius: 10px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            padding: 20px;
            margin-top: 20px;
        }

        .btn-info {
            background-color: #87cefa; /* LightSkyBlue */
            border-color: #87cefa; /* LightSkyBlue */
        }

        .btn-info:hover {
            background-color: #add8e6; /* LightBlue */
            border-color: #add8e6; /* LightBlue */
        }

        .btn-danger {
            background-color: #ff6347; /* Tomato */
            border-color: #ff6347; /* Tomato */
        }

        .btn-danger:hover {
            background-color: #ff4500; /* OrangeRed */
            border-color: #ff4500; /* OrangeRed */
        }
        /* Add more styles as needed */
    </style>
</head>

<body>
    <div class=""container mt-4"">
        <h2 class=""mb-4"">ToDoリスト</h2>
        <ul class=""list-group mt-3"">
            {% for todo in todos %}
            <li class=""list-group-item d-flex justify-content-between align-items-center"">
                {{ todo.title }}
                <span>
                    <a href=""{% url 'edit_todo' todo.id %}"" class=""btn btn-info btn-sm mr-2"">編集</a>
                    <a href=""{% url 'delete_todo' todo.id %}"" class=""btn btn-danger btn-sm"">削除</a>
                </span>
            </li>
            {% endfor %}
        </ul>
        <a href=""{% url 'add_todo' %}"" class=""btn btn-primary mt-3"">新しいToDoを追加</a>
    </div>
    <!-- Bootstrap JS and dependencies -->
    <script src=""https://code.jquery.com/jquery-3.5.1.slim.min.js""></script>
    <script src=""https://cdn.jsdelivr.net/npm/@popperjs/core@2.5.3/dist/umd/popper.min.js""></script>
    <script src=""https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js""></script>
</body>

</html>



ToDo追加のHTMLテンプレート(todo/templates/todo/add_todo.html)
新しい ToDo アイテムを追加するためのHTMLテンプレートを作成しています。
フォームを使用してタイトルを追加できるようにし、追加ボタンをクリックすると内容が保存され、Todoリストにリダイレクトされます。

html
<!DOCTYPE html>
<html lang=""ja"">
<head>
    <meta charset=""UTF-8"">
    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">
    <title>新しいToDoを追加</title>
    <link rel=""stylesheet"" href=""https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css"">
    <style>
        body {
            background-color: #f0f8ff; /* AliceBlue */
        }
        .container {
            background-color: #ffffff; /* White */
            border-radius: 10px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            padding: 20px;
            margin-top: 20px;
        }
        .btn-primary {
            background-color: #00bfff; /* DeepSkyBlue */
            border-color: #00bfff; /* DeepSkyBlue */
        }
        .btn-primary:hover {
            background-color: #00acee; /* DodgerBlue */
            border-color: #00acee; /* DodgerBlue */
        }
        .btn-secondary {
            background-color: #87ceeb; /* SkyBlue */
            border-color: #87ceeb; /* SkyBlue */
        }
        .btn-secondary:hover {
            background-color: #5f9ea0; /* CadetBlue */
            border-color: #5f9ea0; /* CadetBlue */
        }
    </style>
</head>
<body>
  <div class=""container mt-4"">
    <h2 class=""mb-4"">新しいToDoを追加</h2>
    <form method=""post"" action=""{% url 'add_todo' %}"" class=""mt-3"">
      {% csrf_token %}
      <div class=""form-group"">
        <label for=""{{ form.title.id_for_label }}"">タイトル</label>
        {{ form.title }}
      </div>
      <div class=""form-group"">
        <label for=""{{ form.description.id_for_label }}"">詳細</label>
        {{ form.description }}
      </div>
      <button type=""submit"" class=""btn btn-primary"">追加</button>
      <a href=""{% url 'todo_list' %}"" class=""btn btn-secondary"">キャンセル</a>
    </form>
  </div>
  <script src=""https://code.jquery.com/jquery-3.5.1.slim.min.js""></script>
  <script src=""https://cdn.jsdelivr.net/npm/@popperjs/core@2.5.3/dist/umd/popper.min.js""></script>
  <script src=""https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js""></script>
</body>
</html>



ToDo編集のHTMLテンプレート(todo/templates/todo/edit_todo.html)
Todoアイテムを編集するためのHTMLテンプレートを作成しています。
フォームを使用してタイトルを編集できるようにし、保存ボタンをクリックすると内容が保存され、Todoリストにリダイレクトされます。

html
<!DOCTYPE html>
<html lang=""ja"">
<head>
    <meta charset=""UTF-8"">
    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">
    <title>ToDoを編集</title>
    <!-- Bootstrap CSS -->
    <link rel=""stylesheet"" href=""https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css"">
    <!-- Custom styles for female-oriented design -->
    <style>
        body {
            background-color: #f0f8ff; /* AliceBlue */
        }
        .container {
            background-color: #ffffff; /* White */
            border-radius: 10px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            padding: 20px;
            margin-top: 20px;
        }
        .btn-primary {
            background-color: #00bfff; /* DeepSkyBlue */
            border-color: #00bfff; /* DeepSkyBlue */
        }
        .btn-primary:hover {
            background-color: #00acee; /* DodgerBlue */
            border-color: #00acee; /* DodgerBlue */
        }
        .btn-secondary {
            background-color: #87ceeb; /* SkyBlue */
            border-color: #87ceeb; /* SkyBlue */
        }
        .btn-secondary:hover {
            background-color: #5f9ea0; /* CadetBlue */
            border-color: #5f9ea0; /* CadetBlue */
        }
    </style>
</head>
<body>
  <div class=""container mt-4"">
    <h2 class=""mb-4"">ToDoを編集</h2>
    <form method=""post"" action=""{% url 'edit_todo' todo_id %}"" class=""mt-3"">
      {% csrf_token %}
      {{ form.as_p }}
      <button type=""submit"" class=""btn btn-primary"">保存</button>
      <a href=""{% url 'todo_list' %}"" class=""btn btn-secondary"">キャンセル</a>
    </form>
  </div>
  <!-- Bootstrap JS and dependencies -->
  <script src=""https://code.jquery.com/jquery-3.5.1.slim.min.js""></script>
  <script src=""https://cdn.jsdelivr.net/npm/@popperjs/core@2.5.3/dist/umd/popper.min.js""></script>
  <script src=""https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js""></script>
</body>
</html>



Djangoの開発サーバーを起動
コマンドプロンプト等で以下のコマンドを実行し、開発サーバを起動します。

bash
python manage.py runserver


サーバが正常に起動すると、コマンドプロンプトに次のようにメッセージが表示されます。

bash
Starting development server at http://127.0.0.1:8000/
Quit the server with CONTROL-C.


ブラウザを開き、以下のURLにアクセスします。
http://127.0.0.1:8000/

正常に起動した場合の画面
正常に起動した場合、以下のToDoリスト画面が表示されます。

サーバを終了する際にはコマンドプロンプト等でCtrl + Cを押下します。

まとめ
今回、DjangoでToDoリストを作成してみましたが、ブログアプリケーションやポートフォリオサイトの作成も初心者の方にはおすすめされていますので、これからDjangoを勉強してみたい方は是非、挑戦してみてください。
最後まで読んで頂き、有難うございました。
"
https://qiita.com/homma_koki/items/a93ec064d96ebcaf3eba,"['BigQuery', 'dwh', 'dataengineering', 'trocco', 'AdventCalendar2023']","この記事は、エムスリーキャリア Advent Calendar 20223 の21日目の記事です。

はじめに

データ基盤移行の背景
こんにちは。エムスリーキャリア Webグループの本間です。
これまで弊社では、データ基盤としてオンプレのDWHを利用していました。しかし、オンプレのDWHはハードウェアやソフトウェアのメンテナンスが必要で、運用コストや管理負荷がかかってしまうという課題がありました。また、弊社では複数のシステムが散在しており、オンプレのDWHでは、全てのデータを一元管理することができず、データの一元化が困難である状況でした。
そこで、これらの課題に対処するために、データ基盤の構築方法を見直すことに決定しました。その結果、クラウド上でデータ基盤を構築することを選択しました。クラウドへの移行によって、ハードウェアやソフトウェアのメンテナンスが不要になり、運用コストや管理負荷が軽減されることを目指しました。さらに、クラウドのデータウェアハウスは、複数のシステムからのデータを一元管理できるだけの拡張性を持っているため、データの一元化を実現することも目指しました。

こんなデータ基盤にしたかった

データ基盤の完成図
こだわったのは、とにかくシンプルであること、それとフルマネージドであることです。基盤の維持にかかる運用コストを最小化できる構成にこだわりました。

左側に各種システムのデータソース、真ん中にデータウェアハウス、右側に各アプリケーションを配置しています。
データソースの対象は、業務で利用している複数組織にまたがるSalesforceのデータ、各システムのクラウド上にあるDB、マーケティングで利用している各種広告データなどが対象になります。
データウェアハウスの部分は「BigQuery」を採用しました。BigQueryは論理的にデータレイク層、DWH層、データマート層に分けることにしました。データレイク層にオブジェクトストレージを利用することも検討しましたが、データウェアハウスと同じ分析用DBに格納することにしています。
データ収集基盤としては「trocco」を採用しました。データ変換の部分もtroccoのデータマート定義を使って実現しますが、将来的にはdbtの利用も視野にいれています。データウェアハウスからReverseETLにより各アプリケーションにデータを書き戻すこともtroccoで実現したいと考えています。

データ基盤の構築に求めた要件
弊社の現状を考慮し、以下の3点を重視したデータ基盤を構築したいと考えました。

基盤のメンテナンスが不要で、専任の担当者が不要であること
データパイプラインの構築に開発が必要ないこと
処理速度が高速であること


データウェアハウスにBigQueryを採用した理由
データウェアハウスに BigQuery を採用した理由は、主に以下の3点です。

処理の高速化が成熟していること
エコシステムが充実していること
GCPの各種サービスとの連携が容易であること

まず、「処理の高速化が成熟している」という点です。次に、「充実したエコシステム」という点になります。国内において多数の事例が存在し、情報が豊富に揃っている点になります。最後に、「GCPの各種サービスとの容易な連携」という点が挙げられます。これにより、将来的にデータを活用した様々な施策に伴う開発がスムーズに実施できると考えました。
これらの理由から、BigQueryを採用することに決定しました。
データウェアハウスの選択については、最後まで悩みました。他の製品をギリギリまで検討していましたが、最終的にBigQueryを採用することに決断しました。現在、すでに運用が始まっており、弊社の環境においてこの決断は正しかったと感じています。

データパイプラインにtroccoを採用した理由
データパイプラインに trocco を採用した理由は、主に以下の3点です。

必要な機能が提供されている
国内のサービスである
他サービスでは実現できないことができたこと

まず大前提として、「データ収集に必要な機能が提供されている」点が挙げられます。データ転送だけでなく、各処理のワークフロー管理が行えること、さらにReverseETLとして利用できることも、採用した理由の一つ目になります。
次に「国内のサービスである」という点が重要です。日本のサービスであるため、情報収集が容易で、サポートも受けやすいということが、二つ目の理由となります。
最後に、「他サービスでは実現できないことができた」点が挙げられます。技術検証を実施した際に、実現したい機能がtroccoで実現可能であったことが、最後の理由となります。
これらの理由から、troccoを採用することに決定しました。

実際のスケジュール
2023年2月後半にデータ基盤の構築プロジェクトが立ち上がりました。そこから、要件の整理、設計、技術検証、社内手続き、契約、移行作業の実施を進めてきました。メインのデータに関しては、既存のオンプレからBigQueryに移行できたのが2023年8月に実施することができました。およそ半年でクラウド上にデータ基盤を構築することができました。

今後
データ基盤の構築が完了した後も、継続的な改善と最適化を進めていく予定です。データ基盤を活用し、当初考えていた目的の達成に向けて積極的に取り組んでいきます。
また、今後の展望として以下のような事項にも取り組んでいきたいと考えています。

履歴データの管理
メタデータの管理
dbtの導入


さいごに
最後まで読んでいただきありがとうございます。
私たちエムスリーキャリアでは、様々なシステム開発を通して、医療業界や事業の課題に向き合っています。エムスリーキャリアでは積極的にエンジニアを募集しております。
明日以降も エムスリーキャリア Advent Calendar 2023 は続きます。引き続きお楽しみください。

参考文献
データ基盤をクラウドに構築するにあたり、様々な書籍や動画を参考にさせていただきました。ありがとうございます。

実践的データ基盤への処方箋
エンジニアのためのデータ分析基盤入門
データ指向アプリケーションデザイン
Google Cloudではじめる実践データエンジニアリング入門
DX時代のデータマネジメント大全
実践ビッグデータ分析基盤開発 ストーリーで学ぶGoogle BigQuery
Google Cloud実践活用術　データ分析・システム基盤編
エンタープライズのためのGoogle Cloud
Tableauデータ分析 ～入門から実践まで～
Data Engineering Study（すべての回を視聴させていただきました）
2022年、dbt で作るデータ基盤の現場の話

"
https://qiita.com/ubugeeei/items/73a2416fd46cfe6311a8,"['compiler', 'Vue.js', 'VirtualDom', 'Optimization', 'VaporMode']","
初めに

本記事は All we know about Vue 3's Vapor Mode の日本語翻訳版です。
このブログは現時点で出回っている Vapor Mode についてのリソースでおそらく最も体系的にまとめられているもので、主に Evan You 氏の講演やインタビューをもとに書かれています。
このブログを書いてくださったのは :icarus.gk 氏で、彼はいつも Vue のアップデートのまとめや、機能の紹介に関して素晴らしい発信をしています。
日本語翻訳を公開する件について :icarus.gk 氏 は快く了承してくださいました。
この場を借りて感謝したいと思います。
また、コンテンツの最後に幾つかの注釈を加えました。
こちらは元のブログにはないものであり、筆者 (@ubugeeei) が追記しているという点に注意してください。

Vapor Mode とは？ 🧐
Vapor Mode は、Solid.js から着想を得た新しい代替コンパイル戦略です。これにより、コードがより効率的な JavaScript 出力にコンパイルされることで、アプリのパフォーマンスが向上します。
アプリレベルで使用する場合、仮想 DOM を完全に削除することができ、アプリのバンドルサイズを削減することができます。
✅ 利点

より高速なパフォーマンス
より少ないメモリ使用量
より少ないランタイムのコード

Solid.js は Vue と非常に似たリアクティビティシステムを持っており、両者とも、Proxy 1 を使用した読み取りベースの自動トラッキングによるリアクティビティシステムを実装しています。これが Vue が Solid.js と同様のコンパイル戦略を実装できる理由です。

Vapor Mode はどのように機能するのか？ 🤔
いつもと同じ .vue のシングルファイルコンポーネントを使用して、Vapor Mode はより高速な JavaScript 出力にコンパイルすることができます。

最高のパフォーマンスを最適化するために、Vapor Mode は Vue の機能のサブセットのみをサポートします。これには、新しい Composition API と <script setup> を使用した .vue SFC が含まれます。詳細は Vue's Year in Review で読むことができます。

現在のアプローチはどうなのか？
現在、Vue はハイブリッド仮想 DOM / コンパイラのアプローチを取っています。コンパイラは SFC のテンプレートを取り、分析して仮想 DOM にフィードします。
この コンパイラー情報に基づく仮想 DOM のアプローチにより、Vue は Svelte 4 のような 仮想 DOM レスのツールよりも DOM 操作を行う際にわずかに優れたパフォーマンスを発揮することができます。




Vapor Mode はどのように使用できる？ 2


コンポーネントレベルで
.vapor のファイル名の接尾辞を使用して有効にすることができます。

最終的な目標は、Vapor コンポーネントと非 Vapor コンポーネントを同じアプリ内で自由に混在させることです。

📝 * 注意
最初は、Vapor Mode は 仮想 DOM に統合された Vapor 専用のツリーで始まり、そこから相互運用性を改善していく予定です。

アプリレベルで

この方法でコンパイルされたアプリは、仮想 DOM ランタイムを完全に削除し、@vue/reactivity と Vapor Mode のランタイムヘルパーのみを含みます。この方法で書かれたアプリのベースラインサイズは約 6kb で、現在の 仮想 DOM を使用した Vue 3 アプリのベースラインサイズの約 50kb と比較して、88%の削減です！ 🤯

Key Points


既存のコードベースに影響を与えないオプションの機能です。
Vue のバージョンを Vapor Mode を含むバージョンにアップグレードする場合、これは Vapor Mode に関連する破壊的な変更を導入しません。

Vapor Mode は仮想 DOM コンポーネントと相互運用できます。
したがって、Vuetify のような仮想 DOM を使用するライブラリを使用したい場合、Vapor Mode は引き続きサポートできます。


Vapor Mode のステージ 3

開発中ですが、いくつかのニュースがあります：

Stage 1: コア機能のランタイム
Vapor ランタイムヘルパーがすべて導入され、これらのヘルパーは基本的に生成コードをサポートするコードです。このステージはほぼ完了しています。
❇️ ゴール

コアディレクティブ（v-on、v-if、v-forなど）とコンポーネントツリーのサポート
パフォーマンスの前提条件の検証
既存の SSR 出力とのハイドレーション互換性


Stage 2: コア機能のコンパイラ
❇️ ゴール

共有コード生成 IR（中間表現）
JSX AST / テンプレート AST - IR - Vapor Mode コード


なぜ中間表現が必要なのですか？
Vapor Mode のコンパイルプロセスでは、仮想 DOM の欠如により手動のレンダリング関数の作成の必要がなくなります。ただし、一部のユーザーは JavaScript の広範な柔軟性が必要です。そのような場合、JSX を Vapor コードにコンパイルすることが可能です。
テンプレートと JSX の両方は、同じ中間表現に変換され、最終的に Vapor コードにコンパイルされます。


Stage 3: 統合
Vue は Vapor を既存のアプリケーションにシームレスに組み込むことを目指しており、現在のセットアップに対して変更を必要としません。コンポーネントレベルでオプトインする柔軟性があり、アプリケーションの一部のサブセットまたはパフォーマンス重視のエリアに徐々に導入することができます。
❇️ ゴール

スタンドアロンの Vapor アプリのためのツールサポート
既存のアプリ内での Vapor コンポーネントの実行
Vapor 内での vDOM コンポーネントの実行


Stage 4: 機能の同等性
最初のリリースでは、Vapor Mode は基本的なコア機能のみを提供し、<Transition />、<KeepAlive />、<Teleport />、Suspense などのより補助的な機能は、Vue チームが以前のゴールをすべて整理した後に実装されます。

まとめ


Vapor Mode は、Vue.js によって開発されている新しいパフォーマンス指向のコンパイル戦略です。

同じテンプレートを使用します（Composition API と<script setup>を使用している場合）し、より高パフォーマンスな出力を生成します。

段階的に採用可能 - Vapor Mode を個々のコンポーネントまたはアプリ全体に有効にすることができます。

最終的な目標は、Vapor コンポーネントと非 Vapor コンポーネントを問題なく混在させることですただし、Vapor Mode は最初に vDOM に統合された Vapor 専用のツリーで始まり、そこから相互運用性を改善していく予定です。


このブログのために使用されたコンテンツ
The Vue Point - 2022 Year in Review
Jan 1, 2023
Vue: What to Expect in 2023 by Evan You - Vue.js Nation 2023
Feb 1, 2023
State of Vuenion 2023 - Vuejs Amsterdam 2023
Mar 1, 2023
Vue Mastermind Evan You answers Vue devs' questions - Vue Mastery
Jun 22, 2023
State of Vuenion 2023 VueConf US 2023
Jul 11, 2023

以下、@ubugeeei による注釈



Solid.js について、 createSignal に関しては Proxy オブジェクトではなく、getter 関数を扱うインタフェースになっています。
[参考: solid/packages/solid/src/reactive/signal.ts/readSignal]
getter 関数のインタフェースになっていない props に関しては、Proxy を使用しているようでした。
[参考: solid/packages/solid/src/render/component.ts]
(※複数箇所あるので特定の行へのリンクになっていません。Proxy が使われているところがちらほらあると思います。)
また、Vue.js に関しても、基本となる reactive 関数に関しては Proxy で実装されていますが、それをラップした ref の実装に関しては、value への get/set は Proxy ではなく class の getter/setter で実装されています。
[参考: vue/packages/reactivity/src/ref.ts]
value よりもネストしたところでは proxy の get/set が動きます。 ↩


Vapor Mode は現在研究開発段階です。今後の開発の中で大きく変更がある可能性があります。
これらのインタフェースは現時点での草案であることに注意してください。 ↩


vuejs/core-vapor というリポジトリをご存知の方もいるかもしれませんが、こちらのリポジトリが公開されたのは icarus 氏のブログが投稿されてから数ヶ月後です。
このリポジトリが公開される前までにも、Evan 氏を含む Vue.js のコアチームのメンバーがプライベートな場で議論を重ね、開発は進められていたようなので、このブログ(インタビューの発言など)の進捗と、このリポジトリの進捗状況にはズレがあります。
このリポジトリについての説明や Vapor Mode の近況については別記事でまとめる予定があるのでお楽しみに！ ↩



"
https://qiita.com/si-sugaya/items/e5d3165d5a2c56d40d68,"['Qiita', '初心者', 'ポエム', '設計']","
はじめに
みなさんは設計書を書く際に気を付けていることはありますか？
今回は私が最近設計書レビューをしてて感じる事について記載していこうと思います。

その設計書ただのメモ書きになってませんか？
何故この記事を書こうと思ったのかですが、
「その設計書はあなたがコーディングをするためだけのものじゃない」
ということをお伝えしたいためになります。
では、具体的に何を意識するのか。

コーダーが何をどのように実装したいのか理解しやすい内容とする。

設計を担当したものを自分が作るとは限りません。


保守・運用を行う際にどのような造りになっているのか分かりやすい内容とする。

開発を行ったシステムを同じ人がそのまま保守・運用を行っていくとは限りません。



要するに、他人が見るドキュメントだということを意識してほしいということになります。


自分が開発を行うためだけのものではなく、
チームとして開発～保守・運用を行うドキュメントであることを意識する。


次の項では具体的に何を書くべきかについて記載していきます。
（具体的にと言いつつ抽象的な気もしなくもない・・・）

基本設計では何を書くのか
基本設計では要件定義をもとにシステムとして何を実現したいのかを記載していきましょう。
大まかにまとめると下記三点を意識できればよいと思います。

正常系の動作
バリデーション
異常系の動作

この中で特に書かれてないと感じるのは異常系の動作です。
以下の三点は最低限基本設計で検討しておけるとよいと思います。

エラーログの出力内容
ユーザにどのように通知するか

画面に表示するメッセージの出力方法や、バッチであればメールを送信する等


リカバリをどのように行うか



エラーが起きた際にどのように対応するかをしっかり検討しておく。



詳細設計では何を書くのか
詳細設計では各モジュールの処理内容や関連性を明確にしましょう。
その上で書かれてないなと感じる事は下記三点です。

共通化
どのようなライブラリを使用する想定なのか
モジュール同士の関連性

詳細設計において設計者から感じることは
「コーディングする際に考えればいいや」
ということです。
自分がコーディングするのであればそれでも何とかなるかもしれません。
しかし、その記載レベルだとほぼ必ずと言っていいほどコーダーから質問が飛んできます。
（質問がない場合は全然違うものができたりします・・・。）
せっかく詳細設計書を書いたにもかかわらずそこでコミュニケーションに時間を取られるのはもったいないですよね。
コーダーが迷子にならないように各モジュールで実装するべき内容を明確に記載しましょう。


誰がコーディングしてもコーディング内容に大きな相違が起きないレベルで記載する。



余談
設計について記載してきましたが、コーディングの際のコメントも同様になります。
美しいソース（保守・改修・運用しやすいソース）というのはコメントも含めて完成するものだと思います。
得てして開発は時間が足りなくなりがちで、蔑ろになってしまう部分ではありますが、
そこで書いたあなたのコメントで救われる命があります（笑）

まとめ
初めてこの様な記事を書かせていただきましたが、
「設計書に限らず各ドキュメントは他人が見ることを想定した記述レベルで書く」
ということがお伝えできていれば幸いです。
自分が何を表現したいか分からないドキュメントであれば他人は絶対に分かりません（苦笑）
どこまで書けばいいのか最初は難しいと思いますが、その際は周りの諸先輩方を頼りながら分かりやすいドキュメントを作成していきましょう。
"
https://qiita.com/vicie/items/2c37edc7042a39b3dad6,"['TypeScript', 'LineNotify', 'GitHubActions', 'HealthPlanetAPI']","これは、ディップ Advent Calendar 2023の記事になります。

経緯
私は趣味で筋トレやボディメイクに取り組んでいます。その一環として毎朝、タニタの体重計で体重を測っています。
タニタの体重計は、Bluetoothを介して、測定したデータを専用アプリ「Health Planet」に送信してくれる機能があります。この機能を使って、毎日の測定結果をデジタルデータとして記録しています。



人間の体重は、食事量や水分量、睡眠時間などで常に変動しているため、ボディメイクでは、1日ごとの体重の変化よりも、週単位、月単位での中長期的な平均の推移を見てあげることが重要です。しかし、毎日の体重を記録し、その平均を手計算するのはとても面倒くさい作業です。
そこで、タニタの「Health Planet API」の存在を知りました。このAPIを利用して、自動でデータの取得し、平均値や増減の算出を行い、毎日必ず見るLINEに通知を送れば、自身のボディメイクにかなり役に立つと思い、実装してみようと考えました。

Health Planet API とは？
「Health Planet API」は、日時や取得項目などを指定して、測定日時や体重、体脂肪率などを取得できるAPIです。
詳しくは、Health Planet API 仕様書をご覧ください。




今回作ったもの
直近7日間の平均体重と先週分（8~14日前）の平均体重との増減を、毎朝8時にLINE通知するシンプルなものです。


実装
今回は、大まかに

Health Planet APIから測定データ取得
取得した測定データから平均値や増減の算出
LINE Notifyを使って通知
GitHub Actionsのscheduleイベントから毎朝8時に通知処理を実行

という流れで実装しました。
実装のコードは掻い摘んで載せるので、全貌はGitHubのリポジトリをご覧ください。




1. Health Planet API から体重データを取得

その前に）Health Planet APIのユーザー認証
Health Planet APIは、誰でもすぐ叩けるというわけではなく、ユーザー認証を行う必要があります。
ユーザー認証、データ取得までの過程は以下の記事でまとめられていたので、こちらをご覧ください。




直近2週間分の体重データを取得
次にデータを取得します。
言語はTypeScriptを使いました。（言語はなんでもいいと思います）
今回、実行環境はブラウザではなくNode.jsを想定しているため、ブラウザのみでサポートされているJavaScript標準のfetchメソッドではなく、axios で、データフェッチを行いました。
また、日時の処理は「date-fns」を使用しています。
/** 体重データを取得 */
export const fetchWeight = async () => {
  /** Health Planet API URL */
  const url = ""https://www.healthplanet.jp/status/innerscan.json"";

  /** 現在のJST時間を取得 */
  const jstNow = utcToZonedTime(new Date(), ""Asia/Tokyo"");

  /** 14日前（YYYYMMDD000000） */
  const date14DaysAgo = format(subDays(jstNow, 14), ""yyyyMMdd"") + ""000000"";
  /** 1日前（YYYYMMDD235959） */
  const date1DayAgo = format(subDays(jstNow, 1), ""yyyyMMdd"") + ""235959"";

  const params = {
    access_token: process.env.HEALTH_PLANET_ACCESS_TOKEN,
    date: ""1"",
    from: date14DaysAgo,
    to: date1DayAgo,
    tag: ""6021"",
  };

  try {
    const response = await axios.get(url, { params });
    if (response.status !== 200) {
      throw new Error(""Network response was not ok"");
    }
    return response.data;
  } catch (error) {
    console.error(""Error fetching data from Health Planet API"", error);
    throw error;
  }
};

すると、以下のようなデータが返ってきます。（平気で晒す）
{
  birth_date: '19991027',
  data: [
    {
      date: '202312090750', // 2023年12月9日7時50分
      keydata: '74.10', // 74.10kg
      model: '01000099', // 測定機器名（01000099は、RD-903）
      tag: '6021' // 体重
    },
    {
      date: '202312080728',
      keydata: '73.90',
      model: '01000099',
      tag: '6021'
    },
    // 略
    {
      date: '202311270816', // 2週間前まで取得
      keydata: '74.00',
      model: '01000099',
      tag: '6021'
    }
  ],
  height: '173',
  sex: 'male'
}

こちらのデータをもとに、平均値や増減の算出を行っていきます。

2. 取得した測定データから平均値と増減の算出
date-fnsのisWithinIntervalメソッドを使い、取得した2週間分のデータから、今週分の体重（1~7日前の体重）と先週分の体重（8~14日前の体重）で分けました。
そこから、今週分の体重の平均と先週分との体重の増減を算出しました。
calcWeightAverageDiff関数では、以下のような返り値が返ってきます。
/** 今週分(1~7日前)の体重の平均と先週分との体重の増減 */
const { currentWeekAverageWeight, weeklyWeightDiff } = calcWeightAverageDiff(fetchData);
console.log(currentWeekAverageWeight, weeklyWeightDiff);

>> 73.63000000000001 -0.15

これは直近1週間の平均体重が73.63kg、先週の体重平均と比較して-0.15kgだったことを表しています。
ここで算出した値を、LINE通知させていきます。

3. LINE Notifyでの通知
「LINE Notify」は、文字通りLINEでメッセージを通知するためのLINE公式サービスです。




LINE Notify トークンの発行
LINE Notifyでも、アクセストークンを発行する必要があります。
トークンの発行方法は以下の記事にまとめられていましたので、こちらをご覧ください。




LINE通知する関数を作成
まず、今週の体重 XX.Xkg (±〇〇.〇kg)の形になるように通知するメッセージを整形します。
/** 通知メッセージの整形 */
export const formatMessage = (currentWeekAverageWeight: number, weeklyWeightDiff: string) => {
  /** 通知メッセージ */
  const message = `\n今週の体重 ${currentWeekAverageWeight.toFixed(2)}kg(${weeklyWeightDiff}kg)`;

  return message;
};

整形した通知メッセージを引数に渡して、以下のようにsendLineNotification関数でLINE通知を行います。
export const sendLineNotification = async (message: string) => {
  const url = ""https://notify-api.line.me/api/notify"";
  const token = process.env.LINE_NOTIFY_TOKEN;

  const headers = {
    ""Content-Type"": ""application/x-www-form-urlencoded"",
    Authorization: `Bearer ${token}`,
  };

  try {
    const response = await axios.post(url, `message=${encodeURIComponent(message)}`, { headers });
    if (response.status !== 200) {
      throw new Error(""Network response was not ok"");
    }
  } catch (error) {
    console.log(""Error sending LINE notification"", error);
    throw error;
  }
};

次のステップで、体重取得、計算処理、LINE通知の一連の処理を定期実行させていきます。

4. GitHub Actionsで毎朝8時に定期実行させる
当初、cronジョブを設定したり、Macの「Automator」を使用して、定期実行させていました。ただこれらの方法だと、JavaScriptを実行するコンピューターがスリープ状態のときは、実行できないという懸念点がありました。
そこで、GitHub Actionsのscheduleイベントで毎朝8時に定期実行させようと考えました。

アクセストークンをSecretsに追加
コードでは、Health Planet API と LINE Notify のトークンを環境変数にしているので、GitHub Actionsでも環境変数を使用するために、リポジトリのセキュリティ設定に値を追加する必要があります。
以下の手順で追加

タブ「Settings」を選択
サイドバー「Secrets and variables」から「Actions」を選択
「New repository secret」で追加

以下のようになればOKです。


ワークフローの作成
協定世界時（UTC）で23時（日本標準時で翌朝の8時）に定期実行させるようにワークフローを作成して、Pushします。
name: Daily Weight Notification

on:
  schedule:
    - cron: ""0 23 * * *"" # UTCで毎日23時（JSTで翌日の8時）
  workflow_dispatch:

jobs:
  build-and-run:
    runs-on: ubuntu-latest

    env: # 環境変数を設定
      LINE_NOTIFY_TOKEN: ${{ secrets.LINE_NOTIFY_TOKEN }}
      HEALTH_PLANET_ACCESS_TOKEN: ${{ secrets.HEALTH_PLANET_ACCESS_TOKEN }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ""20""

      - name: Install dependencies
        run: npm install

      - name: Compile TypeScript
        run: npm run build # tscを実行するように、事前にbuildコマンドを用意

      - name: Run script
        run: node dist/index.js

そうすると、リポジトリのActionsタブから定期実行されていることが確認できました。


感想
感想としては、「便利すぎる」という点に尽きます。
実際に今年の夏、人生で初めて本格的な減量を行いましたが、12~3キロほど、計画的に減量させることができました。もちろん減量期だけではなく、増量期にも使っています。
今回の開発で、日常の「こういうことできたらいいけど、現状ちょっと足りないな...」というかゆい機能を自分でプラスできるのは、開発してて楽しいなと感じました。
あと、ドヤ顔で晒せるぐらいムキムキになりたい.....。

補足
使用している体重計
innerscanDUAL RD-903



"
